"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3299],{5252:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/action-execution/manipulation-interaction","title":"Manipulation and Interaction Systems","description":"Overview","source":"@site/docs/module-4-vision-language-action/action-execution/manipulation-interaction.md","sourceDirName":"module-4-vision-language-action/action-execution","slug":"/module-4-vision-language-action/action-execution/manipulation-interaction","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/action-execution/manipulation-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/action-execution/manipulation-interaction.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"LLM Integration for Cognitive Planning","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/llm-integration/llm-cognitive-planning"},"next":{"title":"Vision-Action Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/action-execution/vision-action-integration"}}');var a=t(4848),s=t(8453);const i={sidebar_position:5},r="Manipulation and Interaction Systems",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Robotic Manipulation Architecture",id:"robotic-manipulation-architecture",level:2},{value:"Manipulation Framework Design",id:"manipulation-framework-design",level:3},{value:"Grasp Planning and Execution",id:"grasp-planning-and-execution",level:3},{value:"Human-Robot Interaction Systems",id:"human-robot-interaction-systems",level:2},{value:"Interaction Protocol Design",id:"interaction-protocol-design",level:3},{value:"Multimodal Interaction Interface",id:"multimodal-interaction-interface",level:3},{value:"Safety and Validation Systems",id:"safety-and-validation-systems",level:2},{value:"Manipulation Safety Validator",id:"manipulation-safety-validator",level:3},{value:"Integration with VLA System",id:"integration-with-vla-system",level:2},{value:"Main Integration Node",id:"main-integration-node",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"1. Set Up the Manipulation Package",id:"1-set-up-the-manipulation-package",level:3},{value:"2. Install Dependencies",id:"2-install-dependencies",level:3},{value:"3. Configure the System",id:"3-configure-the-system",level:3},{value:"4. Testing the System",id:"4-testing-the-system",level:3},{value:"Best Practices and Considerations",id:"best-practices-and-considerations",level:2},{value:"1. Safety First Approach",id:"1-safety-first-approach",level:3},{value:"2. Grasp Planning Optimization",id:"2-grasp-planning-optimization",level:3},{value:"3. Human-Robot Interaction",id:"3-human-robot-interaction",level:3},{value:"4. Performance Optimization",id:"4-performance-optimization",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Tips",id:"debugging-tips",level:3}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"manipulation-and-interaction-systems",children:"Manipulation and Interaction Systems"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Manipulation and interaction systems form the physical execution layer of Vision-Language-Action (VLA) systems for humanoid robots. These systems translate cognitive plans and perceptual understanding into precise physical actions, enabling humanoid robots to manipulate objects, interact with humans, and perform complex tasks in real-world environments."}),"\n",(0,a.jsx)(e.p,{children:"This module covers the implementation of robotic manipulation capabilities, human-robot interaction protocols, and safe interaction systems that work in conjunction with voice recognition, LLM planning, and multimodal perception to create natural and effective human-robot collaboration."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement robotic manipulation systems for humanoid robots"}),"\n",(0,a.jsx)(e.li,{children:"Design safe and intuitive human-robot interaction protocols"}),"\n",(0,a.jsx)(e.li,{children:"Create object manipulation pipelines with perception integration"}),"\n",(0,a.jsx)(e.li,{children:"Implement grasp planning and execution for various object types"}),"\n",(0,a.jsx)(e.li,{children:"Develop multimodal interaction systems combining voice, vision, and touch"}),"\n",(0,a.jsx)(e.li,{children:"Design safe interaction validation and monitoring systems"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(e.p,{children:"Before implementing manipulation and interaction systems, ensure you have:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Completed Modules 1-3 (ROS 2, Digital Twin, AI-Robot Brain)"}),"\n",(0,a.jsx)(e.li,{children:"Voice recognition and LLM integration systems from previous modules"}),"\n",(0,a.jsx)(e.li,{children:"Multimodal perception integration for object detection and scene understanding"}),"\n",(0,a.jsx)(e.li,{children:"Basic understanding of robotic manipulation concepts (kinematics, grasping)"}),"\n",(0,a.jsx)(e.li,{children:"Experience with ROS 2 action and service interfaces"}),"\n",(0,a.jsx)(e.li,{children:"Familiarity with MoveIt! for motion planning"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"robotic-manipulation-architecture",children:"Robotic Manipulation Architecture"}),"\n",(0,a.jsx)(e.h3,{id:"manipulation-framework-design",children:"Manipulation Framework Design"}),"\n",(0,a.jsx)(e.p,{children:"Design a comprehensive manipulation framework for humanoid robots:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom moveit_msgs.msg import CollisionObject\nfrom moveit_msgs.srv import GetPositionIK, GetPositionFK\nfrom shape_msgs.msg import SolidPrimitive\nfrom std_msgs.msg import Header\nimport numpy as np\nimport tf2_ros\nfrom typing import Dict, List, Tuple, Optional\nfrom enum import Enum\n\nclass ManipulationState(Enum):\n    IDLE = "idle"\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    SAFETY_STOP = "safety_stop"\n\nclass GraspType(Enum):\n    PINCH = "pinch"\n    POWER = "power"\n    PRECISION = "precision"\n    LATERAL = "lateral"\n    SUCTION = "suction"  # For robots with suction grippers\n\nclass ManipulationController(Node):\n    """Main controller for humanoid manipulation systems"""\n\n    def __init__(self):\n        super().__init__(\'manipulation_controller\')\n\n        # Initialize callback group for concurrent callbacks\n        self.callback_group = ReentrantCallbackGroup()\n\n        # MoveIt! action clients\n        self.move_group_client = ActionClient(\n            self, MoveGroup, \'move_group\',\n            callback_group=self.callback_group)\n        self.pick_client = ActionClient(\n            self, PickUp, \'pickup\',\n            callback_group=self.callback_group)\n        self.place_client = ActionClient(\n            self, Place, \'place\',\n            callback_group=self.callback_group)\n\n        # Service clients\n        self.get_ik_client = self.create_client(\n            GetPositionIK, \'compute_ik\',\n            callback_group=self.callback_group)\n        self.get_fk_client = self.create_client(\n            GetPositionFK, \'compute_fk\',\n            callback_group=self.callback_group)\n\n        # TF2 buffer for coordinate transformations\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # Publishers and subscribers\n        self.manipulation_status_pub = self.create_publisher(\n            String, \'manipulation_status\', 10)\n        self.collision_object_pub = self.create_publisher(\n            PlanningScene, \'planning_scene\', 10)\n        self.interaction_request_sub = self.create_subscription(\n            String, \'interaction_request\', self.interaction_callback, 10)\n\n        # Internal state\n        self.current_state = ManipulationState.IDLE\n        self.robot_group = "humanoid_arm"  # Default group name\n        self.end_effector_link = "humanoid_hand"  # Default end effector\n        self.manipulation_history = []\n\n        # Grasp planning parameters\n        self.grasp_database = self._initialize_grasp_database()\n        self.approach_distance = 0.1  # meters\n        self.lift_distance = 0.05     # meters\n\n        self.get_logger().info("Manipulation Controller initialized")\n\n    def _initialize_grasp_database(self) -> Dict:\n        """Initialize database of known grasps for common objects"""\n        return {\n            \'cup\': {\n                \'grasp_types\': [GraspType.POWER, GraspType.PINCH],\n                \'approach_direction\': [0, -1, 0],  # Approach from side\n                \'grasp_pose_offset\': [0.05, 0, 0],  # Offset for cup handle\n                \'gripper_width\': 0.08\n            },\n            \'bottle\': {\n                \'grasp_types\': [GraspType.POWER],\n                \'approach_direction\': [0, 0, -1],  # Approach from top\n                \'grasp_pose_offset\': [0, 0, 0.1],  # Grasp at neck\n                \'gripper_width\': 0.06\n            },\n            \'book\': {\n                \'grasp_types\': [GraspType.PINCH, GraspType.PRECISION],\n                \'approach_direction\': [0, -1, 0],  # Approach from spine\n                \'grasp_pose_offset\': [0, 0, 0.02],  # Thickness offset\n                \'gripper_width\': 0.04\n            },\n            \'box\': {\n                \'grasp_types\': [GraspType.POWER],\n                \'approach_direction\': [0, 0, -1],  # Approach from top\n                \'grasp_pose_offset\': [0, 0, 0.05],  # Half height offset\n                \'gripper_width\': 0.12\n            }\n        }\n\n    def move_to_pose(self, target_pose: Pose, group_name: str = None) -> bool:\n        """Move robot to specified pose"""\n        if group_name is None:\n            group_name = self.robot_group\n\n        # Create MoveGroup goal\n        goal = MoveGroup.Goal()\n        goal.request.group_name = group_name\n        goal.request.num_planning_attempts = 5\n        goal.request.allowed_planning_time = 10.0\n\n        # Set target pose constraint\n        pose_constraint = Constraints()\n        position_constraint = PositionConstraint()\n        position_constraint.link_name = self.end_effector_link\n        position_constraint.header.frame_id = "base_link"\n        position_constraint.position = target_pose.position\n\n        # Set position tolerance\n        position_constraint.constraint_region.primitives.append(\n            SolidPrimitive(type=SolidPrimitive.SPHERE, dimensions=[0.01]))\n        position_constraint.weight = 1.0\n\n        # Set orientation constraint\n        orientation_constraint = OrientationConstraint()\n        orientation_constraint.link_name = self.end_effector_link\n        orientation_constraint.header.frame_id = "base_link"\n        orientation_constraint.orientation = target_pose.orientation\n        orientation_constraint.absolute_x_axis_tolerance = 0.1\n        orientation_constraint.absolute_y_axis_tolerance = 0.1\n        orientation_constraint.absolute_z_axis_tolerance = 0.1\n        orientation_constraint.weight = 1.0\n\n        pose_constraint.position_constraints.append(position_constraint)\n        pose_constraint.orientation_constraints.append(orientation_constraint)\n\n        goal.request.goal_constraints.append(pose_constraint)\n\n        # Send goal and wait for result\n        self.move_group_client.wait_for_server()\n        future = self.move_group_client.send_goal_async(goal)\n\n        # Wait for result with timeout\n        rclpy.spin_until_future_complete(self, future, timeout_sec=15.0)\n\n        if future.result() is not None:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                result_future = goal_handle.get_result_async()\n                rclpy.spin_until_future_complete(self, result_future, timeout_sec=15.0)\n\n                if result_future.result() is not None:\n                    result = result_future.result().result\n                    return result.error_code.val == MoveItErrorCodes.SUCCESS\n                else:\n                    return False\n            else:\n                return False\n        else:\n            return False\n\n    def pick_object(self, object_name: str, grasp_pose: Pose = None) -> bool:\n        """Pick up an object using pre-planned grasps or provided pose"""\n        if grasp_pose is None:\n            # Plan grasp based on object type\n            grasp_pose = self._plan_grasp_for_object(object_name)\n            if grasp_pose is None:\n                self.get_logger().error(f"Could not plan grasp for object: {object_name}")\n                return False\n\n        # Create pick goal\n        goal = PickUp.Goal()\n        goal.target_name = object_name\n        goal.group_name = self.robot_group\n        goal.end_effector = self.end_effector_link\n        goal.allow_gripper_support_collision = True\n\n        # Add pre-grasp and grasp poses\n        pre_grasp_pose = self._calculate_pre_grasp_pose(grasp_pose)\n        goal.pre_grasp_approach.direction.vector.z = -1.0  # Approach from above\n        goal.pre_grasp_approach.min_distance = 0.05\n        goal.pre_grasp_approach.desired_distance = 0.1\n\n        goal.grasp_pose = grasp_pose\n        goal.grasp_grasp.direction.vector.z = 1.0  # Grasp direction\n        goal.grasp_grasp.min_distance = 0.02\n        goal.grasp_grasp.desired_distance = 0.05\n\n        # Send pick goal\n        self.pick_client.wait_for_server()\n        future = self.pick_client.send_goal_async(goal)\n\n        rclpy.spin_until_future_complete(self, future, timeout_sec=30.0)\n\n        if future.result() is not None:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                result_future = goal_handle.get_result_async()\n                rclpy.spin_until_future_complete(self, result_future, timeout_sec=30.0)\n\n                if result_future.result() is not None:\n                    result = result_future.result().result\n                    return result.error_code.val == MoveItErrorCodes.SUCCESS\n                else:\n                    return False\n            else:\n                return False\n        else:\n            return False\n\n    def place_object(self, object_name: str, place_pose: Pose) -> bool:\n        """Place object at specified location"""\n        goal = Place.Goal()\n        goal.group_name = self.robot_group\n        goal.attached_object_name = object_name\n        goal.place_locations = [place_pose]\n\n        # Configure approach and lift\n        goal.place_location.pre_place_approach.direction.vector.z = -1.0\n        goal.place_location.pre_place_approach.min_distance = 0.05\n        goal.place_location.pre_place_approach.desired_distance = 0.1\n\n        goal.place_location.post_place_retreat.direction.vector.x = -1.0\n        goal.place_location.post_place_retreat.min_distance = 0.05\n        goal.place_location.post_place_retreat.desired_distance = 0.1\n\n        # Send place goal\n        self.place_client.wait_for_server()\n        future = self.place_client.send_goal_async(goal)\n\n        rclpy.spin_until_future_complete(self, future, timeout_sec=30.0)\n\n        if future.result() is not None:\n            goal_handle = future.result()\n            if goal_handle.accepted:\n                result_future = goal_handle.get_result_async()\n                rclpy.spin_until_future_complete(self, result_future, timeout_sec=30.0)\n\n                if result_future.result() is not None:\n                    result = result_future.result().result\n                    return result.error_code.val == MoveItErrorCodes.SUCCESS\n                else:\n                    return False\n            else:\n                return False\n        else:\n            return False\n\n    def _plan_grasp_for_object(self, object_name: str) -> Optional[Pose]:\n        """Plan grasp pose for known object type"""\n        # Get object information from perception system\n        object_info = self._get_object_info(object_name)\n        if not object_info:\n            return None\n\n        object_type = object_info.get(\'type\', \'unknown\')\n        object_pose = object_info.get(\'pose\')\n\n        if object_type in self.grasp_database:\n            grasp_config = self.grasp_database[object_type]\n\n            # Calculate grasp pose based on object type and pose\n            grasp_pose = Pose()\n            grasp_pose.position = object_pose.position\n\n            # Apply offset based on object type\n            offset = grasp_config[\'grasp_pose_offset\']\n            grasp_pose.position.x += offset[0]\n            grasp_pose.position.y += offset[1]\n            grasp_pose.position.z += offset[2]\n\n            # Set appropriate orientation based on approach direction\n            approach_dir = grasp_config[\'approach_direction\']\n            grasp_pose.orientation = self._calculate_grasp_orientation(\n                approach_dir, object_pose.orientation)\n\n            return grasp_pose\n        else:\n            # Use generic grasp planning for unknown objects\n            return self._plan_generic_grasp(object_pose)\n\n    def _calculate_pre_grasp_pose(self, grasp_pose: Pose) -> Pose:\n        """Calculate pre-grasp pose by moving away from grasp pose"""\n        pre_grasp = Pose()\n        pre_grasp.position = grasp_pose.position\n\n        # Move back by approach distance along approach direction\n        # Assuming approach is from above (z-axis)\n        pre_grasp.position.z += self.approach_distance\n        pre_grasp.orientation = grasp_pose.orientation\n\n        return pre_grasp\n\n    def _calculate_grasp_orientation(self, approach_dir: List[float],\n                                   object_orientation: Quaternion) -> Quaternion:\n        """Calculate appropriate grasp orientation based on approach direction"""\n        # This is a simplified orientation calculation\n        # In practice, you\'d use more sophisticated orientation planning\n\n        # For now, return a default orientation\n        # In a real system, you\'d calculate orientation based on:\n        # - Approach direction\n        # - Object orientation\n        # - Grasp type requirements\n        return Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"grasp-planning-and-execution",children:"Grasp Planning and Execution"}),"\n",(0,a.jsx)(e.p,{children:"Implement advanced grasp planning for various object types:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import open3d as o3d\nfrom scipy.spatial.transform import Rotation as R\nimport tf_transformations\n\nclass AdvancedGraspPlanner:\n    """Advanced grasp planning using point cloud data"""\n\n    def __init__(self):\n        self.gripper_width_limits = (0.02, 0.15)  # meters\n        self.min_grasp_quality = 0.7\n        self.max_grasps_to_generate = 20\n\n    def plan_grasps_from_point_cloud(self, point_cloud: np.ndarray,\n                                   object_centroid: np.ndarray) -> List[Dict]:\n        """Plan grasps from point cloud data"""\n        # Convert numpy array to Open3D point cloud\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(point_cloud)\n\n        # Downsample for faster processing\n        pcd_downsampled = pcd.voxel_down_sample(voxel_size=0.01)\n\n        # Estimate normals\n        pcd_downsampled.estimate_normals(\n            search_param=o3d.geometry.KDTreeSearchParamHybrid(\n                radius=0.02, max_nn=30))\n\n        # Generate grasp candidates\n        grasp_candidates = self._generate_grasp_candidates(pcd_downsampled, object_centroid)\n\n        # Evaluate grasp quality\n        valid_grasps = []\n        for grasp in grasp_candidates:\n            quality = self._evaluate_grasp_quality(grasp, pcd_downsampled)\n            if quality >= self.min_grasp_quality:\n                grasp[\'quality\'] = quality\n                valid_grasps.append(grasp)\n\n        # Sort by quality\n        valid_grasps.sort(key=lambda x: x[\'quality\'], reverse=True)\n\n        return valid_grasps[:self.max_grasps_to_generate]\n\n    def _generate_grasp_candidates(self, pcd: o3d.geometry.PointCloud,\n                                 object_centroid: np.ndarray) -> List[Dict]:\n        """Generate potential grasp candidates"""\n        points = np.asarray(pcd.points)\n        normals = np.asarray(pcd.normals)\n\n        grasp_candidates = []\n\n        for i in range(len(points)):\n            point = points[i]\n            normal = normals[i]\n\n            # Skip points too far from centroid\n            if np.linalg.norm(point - object_centroid) > 0.3:  # 30cm radius\n                continue\n\n            # Generate grasp poses around the surface normal\n            for angle_offset in [0, 45, 90, 135]:  # Different orientations\n                grasp_pose = self._create_grasp_pose(point, normal, angle_offset)\n\n                grasp_candidates.append({\n                    \'pose\': grasp_pose,\n                    \'contact_point\': point,\n                    \'surface_normal\': normal\n                })\n\n        return grasp_candidates\n\n    def _create_grasp_pose(self, contact_point: np.ndarray,\n                         surface_normal: np.ndarray,\n                         angle_offset: float) -> Pose:\n        """Create grasp pose from contact point and surface normal"""\n        pose = Pose()\n\n        # Set position\n        pose.position.x = float(contact_point[0])\n        pose.position.y = float(contact_point[1])\n        pose.position.z = float(contact_point[2])\n\n        # Calculate orientation based on surface normal and angle offset\n        # Create a rotation matrix from the approach direction\n        approach_dir = -surface_normal  # Grasp from opposite of surface normal\n\n        # Create orthogonal vectors for complete orientation\n        if abs(approach_dir[2]) < 0.9:\n            # Use z-axis as reference if approach is not vertical\n            ortho1 = np.cross(approach_dir, [0, 0, 1])\n        else:\n            # Use x-axis as reference if approach is nearly vertical\n            ortho1 = np.cross(approach_dir, [1, 0, 0])\n\n        ortho1 = ortho1 / np.linalg.norm(ortho1)\n        ortho2 = np.cross(approach_dir, ortho1)\n\n        # Create rotation matrix\n        rotation_matrix = np.column_stack([ortho1, ortho2, approach_dir])\n\n        # Add angle offset around approach direction\n        rotation_obj = R.from_rotvec(angle_offset * np.pi / 180.0 * approach_dir)\n        rotation_matrix = rotation_matrix @ rotation_obj.as_matrix()\n\n        # Convert to quaternion\n        quat = R.from_matrix(rotation_matrix).as_quat()\n        pose.orientation.x = float(quat[0])\n        pose.orientation.y = float(quat[1])\n        pose.orientation.z = float(quat[2])\n        pose.orientation.w = float(quat[3])\n\n        return pose\n\n    def _evaluate_grasp_quality(self, grasp: Dict, pcd: o3d.geometry.PointCloud) -> float:\n        """Evaluate grasp quality based on geometric criteria"""\n        contact_point = grasp[\'contact_point\']\n        approach_dir = -grasp[\'surface_normal\']  # Opposite of surface normal\n\n        # Check if grasp is stable (not too steep)\n        stability_score = self._evaluate_stability(approach_dir)\n\n        # Check if grasp is accessible (not blocked by other geometry)\n        accessibility_score = self._evaluate_accessibility(\n            contact_point, approach_dir, pcd)\n\n        # Combine scores\n        quality = 0.6 * stability_score + 0.4 * accessibility_score\n\n        return min(quality, 1.0)\n\n    def _evaluate_stability(self, approach_dir: np.ndarray) -> float:\n        """Evaluate grasp stability based on approach direction"""\n        # Prefer grasps that are not too steep (z-component should be significant)\n        vertical_alignment = abs(approach_dir[2])\n\n        # Prefer upward-facing grasps for stability\n        if approach_dir[2] > 0:\n            return vertical_alignment\n        else:\n            # Downward grasps are less stable\n            return vertical_alignment * 0.7\n\n    def _evaluate_accessibility(self, contact_point: np.ndarray,\n                              approach_dir: np.ndarray,\n                              pcd: o3d.geometry.PointCloud) -> float:\n        """Evaluate if grasp approach is accessible"""\n        points = np.asarray(pcd.points)\n\n        # Check for obstacles along approach direction\n        approach_vector = approach_dir * 0.05  # 5cm approach distance\n\n        # Create approach path\n        approach_start = contact_point - approach_vector\n        approach_end = contact_point + approach_vector * 2\n\n        # Check for points in approach path\n        path_points = points[\n            (np.linalg.norm(points - approach_start, axis=1) < 0.02) |\n            (np.linalg.norm(points - approach_end, axis=1) < 0.02)\n        ]\n\n        # If no obstacles in path, it\'s accessible\n        accessibility = 1.0 if len(path_points) == 0 else 0.3\n\n        return accessibility\n'})}),"\n",(0,a.jsx)(e.h2,{id:"human-robot-interaction-systems",children:"Human-Robot Interaction Systems"}),"\n",(0,a.jsx)(e.h3,{id:"interaction-protocol-design",children:"Interaction Protocol Design"}),"\n",(0,a.jsx)(e.p,{children:"Implement safe and intuitive human-robot interaction protocols:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class HumanRobotInteractionController(Node):\n    """Controller for safe human-robot interaction"""\n\n    def __init__(self):\n        super().__init__(\'human_robot_interaction_controller\')\n\n        # Publishers and subscribers\n        self.interaction_status_pub = self.create_publisher(\n            String, \'interaction_status\', 10)\n        self.human_detection_sub = self.create_subscription(\n            Detection2DArray, \'human_detections\', self.human_detected_callback, 10)\n        self.voice_command_sub = self.create_subscription(\n            String, \'voice_command\', self.voice_command_callback, 10)\n        self.interaction_request_pub = self.create_publisher(\n            String, \'interaction_request\', 10)\n\n        # Safety and proximity monitoring\n        self.human_proximity_threshold = 1.0  # meters\n        self.safety_zones = {}  # Track humans and their positions\n        self.interaction_mode = "autonomous"  # or "collaborative", "supervised"\n\n        # Interaction state management\n        self.current_interaction = None\n        self.interaction_queue = []\n        self.interaction_history = []\n\n        # Timer for safety monitoring\n        self.safety_timer = self.create_timer(0.1, self._monitor_safety)\n\n        self.get_logger().info("Human-Robot Interaction Controller initialized")\n\n    def human_detected_callback(self, msg: Detection2DArray):\n        """Process human detections for interaction management"""\n        current_humans = {}\n\n        for detection in msg.detections:\n            # Check if this is a human detection\n            if detection.results:\n                best_result = max(detection.results,\n                                key=lambda x: x.hypothesis.score)\n\n                if best_result.hypothesis.class_id == "person":\n                    human_id = f"person_{len(self.safety_zones)}"\n                    position = self._convert_bbox_to_position(detection.bbox)\n\n                    current_humans[human_id] = {\n                        \'position\': position,\n                        \'confidence\': best_result.hypothesis.score,\n                        \'timestamp\': self.get_clock().now().to_msg()\n                    }\n\n        # Update safety zones\n        self.safety_zones = current_humans\n\n        # Check for potential interaction opportunities\n        if self.interaction_mode == "collaborative":\n            self._check_interaction_opportunities()\n\n    def voice_command_callback(self, msg: String):\n        """Process voice commands that may involve human interaction"""\n        command = msg.data.lower()\n\n        # Check for interaction-specific commands\n        interaction_keywords = [\'help\', \'assist\', \'work with\', \'collaborate\', \'follow\']\n\n        for keyword in interaction_keywords:\n            if keyword in command:\n                self._initiate_interaction(command)\n                break\n\n    def _initiate_interaction(self, command: str):\n        """Initiate human-robot interaction based on command"""\n        if not self.safety_zones:\n            self.get_logger().warn("No humans detected for interaction")\n            return\n\n        # Find closest human for interaction\n        closest_human = self._find_closest_human()\n        if closest_human:\n            interaction_request = {\n                \'type\': \'initiate_interaction\',\n                \'target_human\': closest_human[0],\n                \'command\': command,\n                \'timestamp\': self.get_clock().now().to_msg()\n            }\n\n            request_msg = String()\n            request_msg.data = json.dumps(interaction_request)\n            self.interaction_request_pub.publish(request_msg)\n\n            # Update interaction state\n            self.current_interaction = interaction_request\n            self.interaction_history.append(interaction_request)\n\n    def _find_closest_human(self) -> Optional[Tuple[str, Dict]]:\n        """Find the closest human to the robot"""\n        if not self.safety_zones:\n            return None\n\n        robot_position = self._get_robot_position()\n        if robot_position is None:\n            return None\n\n        closest_human = None\n        min_distance = float(\'inf\')\n\n        for human_id, human_info in self.safety_zones.items():\n            human_pos = human_info[\'position\']\n            distance = np.sqrt(\n                (human_pos.x - robot_position.x)**2 +\n                (human_pos.y - robot_position.y)**2\n            )\n\n            if distance < min_distance:\n                min_distance = distance\n                closest_human = (human_id, human_info)\n\n        return closest_human\n\n    def _check_interaction_opportunities(self):\n        """Check for opportunities to initiate interaction"""\n        if self.current_interaction is not None:\n            # Interaction already in progress\n            return\n\n        # Check if any human is within interaction distance\n        robot_position = self._get_robot_position()\n        if robot_position is None:\n            return\n\n        for human_id, human_info in self.safety_zones.items():\n            human_pos = human_info[\'position\']\n            distance = np.sqrt(\n                (human_pos.x - robot_position.x)**2 +\n                (human_pos.y - robot_position.y)**2\n            )\n\n            if distance < 2.0:  # Within 2 meters\n                # Potential interaction opportunity\n                self._request_interaction_permission(human_id)\n\n    def _request_interaction_permission(self, human_id: str):\n        """Request permission to interact with a human"""\n        # This would typically involve visual/audible signals\n        # For now, we\'ll simulate permission granted\n        interaction_request = {\n            \'type\': \'request_permission\',\n            \'target_human\': human_id,\n            \'request_type\': \'approach\',\n            \'timestamp\': self.get_clock().now().to_msg()\n        }\n\n        request_msg = String()\n        request_msg.data = json.dumps(interaction_request)\n        self.interaction_request_pub.publish(request_msg)\n\n    def _monitor_safety(self):\n        """Monitor safety during human-robot interaction"""\n        if not self.safety_zones:\n            return\n\n        robot_position = self._get_robot_position()\n        if robot_position is None:\n            return\n\n        for human_id, human_info in self.safety_zones.items():\n            human_pos = human_info[\'position\']\n            distance = np.sqrt(\n                (human_pos.x - robot_position.x)**2 +\n                (human_pos.y - robot_position.y)**2\n            )\n\n            if distance < self.human_proximity_threshold:\n                # Human is too close, trigger safety response\n                safety_alert = {\n                    \'type\': \'safety_violation\',\n                    \'human_id\': human_id,\n                    \'distance\': distance,\n                    \'timestamp\': self.get_clock().now().to_msg()\n                }\n\n                # Publish safety alert\n                alert_msg = String()\n                alert_msg.data = json.dumps(safety_alert)\n                self.interaction_status_pub.publish(alert_msg)\n\n                # Trigger safety response based on interaction mode\n                self._handle_safety_violation(safety_alert)\n\n    def _handle_safety_violation(self, alert: Dict):\n        """Handle safety violation during interaction"""\n        if self.interaction_mode == "autonomous":\n            # Stop all motion\n            self._emergency_stop()\n        elif self.interaction_mode == "collaborative":\n            # Pause manipulation, maintain safe distance\n            self._pause_manipulation()\n        elif self.interaction_mode == "supervised":\n            # Alert supervisor and wait for instructions\n            self._alert_supervisor()\n\n    def _convert_bbox_to_position(self, bbox: BoundingBox2D) -> Point:\n        """Convert 2D bounding box to 3D position estimate"""\n        # This is a simplified conversion\n        # In practice, you\'d use depth information\n        return Point(\n            x=float(bbox.center.x),\n            y=float(bbox.center.y),\n            z=0.0  # Assume ground level for now\n        )\n\n    def _get_robot_position(self) -> Optional[Point]:\n        """Get current robot position from localization"""\n        # This would interface with the localization system\n        # For now, returning a placeholder\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                \'map\', \'base_link\', rclpy.time.Time())\n            return Point(\n                x=transform.transform.translation.x,\n                y=transform.transform.translation.y,\n                z=transform.transform.translation.z\n            )\n        except:\n            return None\n\n    def _emergency_stop(self):\n        """Emergency stop for safety violations"""\n        self.get_logger().warn("Emergency stop triggered due to safety violation")\n        # Implementation would send stop commands to all controllers\n\n    def _pause_manipulation(self):\n        """Pause manipulation while maintaining safety"""\n        self.get_logger().info("Pausing manipulation due to safety concern")\n        # Implementation would pause current manipulation task\n\n    def _alert_supervisor(self):\n        """Alert supervisor for supervised mode"""\n        self.get_logger().info("Alerting supervisor for safety decision")\n        # Implementation would notify human supervisor\n'})}),"\n",(0,a.jsx)(e.h3,{id:"multimodal-interaction-interface",children:"Multimodal Interaction Interface"}),"\n",(0,a.jsx)(e.p,{children:"Create a multimodal interaction interface that combines voice, vision, and touch:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class MultimodalInteractionInterface(Node):\n    \"\"\"Interface for multimodal human-robot interaction\"\"\"\n\n    def __init__(self):\n        super().__init__('multimodal_interaction_interface')\n\n        # Publishers and subscribers\n        self.interaction_response_pub = self.create_publisher(\n            String, 'interaction_response', 10)\n        self.audio_response_pub = self.create_publisher(\n            String, 'audio_response', 10)\n        self.visual_feedback_pub = self.create_publisher(\n            MarkerArray, 'visual_feedback', 10)\n\n        # Subscriptions from various modalities\n        self.voice_command_sub = self.create_subscription(\n            String, 'voice_command', self.voice_callback, 10)\n        self.gesture_sub = self.create_subscription(\n            Gesture, 'hand_gestures', self.gesture_callback, 10)\n        self.touch_sub = self.create_subscription(\n            TouchEvent, 'touch_events', self.touch_callback, 10)\n        self.vision_command_sub = self.create_subscription(\n            String, 'vision_commands', self.vision_callback, 10)\n\n        # Interaction state\n        self.interaction_context = {\n            'current_user': None,\n            'interaction_history': [],\n            'attention_objects': [],\n            'pending_requests': []\n        }\n\n        # Response generation\n        self.response_generator = ResponseGenerator()\n\n        self.get_logger().info(\"Multimodal Interaction Interface initialized\")\n\n    def voice_callback(self, msg: String):\n        \"\"\"Handle voice commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f\"Processing voice command: {command}\")\n\n        # Parse voice command and generate response\n        response = self._process_voice_command(command)\n        self._publish_response(response, 'voice')\n\n    def gesture_callback(self, msg: Gesture):\n        \"\"\"Handle gesture commands\"\"\"\n        self.get_logger().info(f\"Processing gesture: {msg.type}\")\n\n        # Process gesture and generate response\n        response = self._process_gesture(msg)\n        self._publish_response(response, 'gesture')\n\n    def touch_callback(self, msg: TouchEvent):\n        \"\"\"Handle touch events\"\"\"\n        self.get_logger().info(f\"Processing touch event: {msg.location}\")\n\n        # Process touch event and generate response\n        response = self._process_touch_event(msg)\n        self._publish_response(response, 'touch')\n\n    def vision_callback(self, msg: String):\n        \"\"\"Handle vision-based commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f\"Processing vision command: {command}\")\n\n        # Process vision command and generate response\n        response = self._process_vision_command(command)\n        self._publish_response(response, 'vision')\n\n    def _process_voice_command(self, command: str) -> Dict:\n        \"\"\"Process voice command and generate response\"\"\"\n        # Simple command parsing - in practice, use NLP\n        command_lower = command.lower()\n\n        if 'hello' in command_lower or 'hi' in command_lower:\n            return {\n                'type': 'greeting',\n                'response': 'Hello! How can I assist you today?',\n                'action': 'wave'\n            }\n        elif 'help' in command_lower:\n            return {\n                'type': 'assistance_request',\n                'response': 'I can help you with navigation, object manipulation, or information lookup.',\n                'action': 'point_to_self'\n            }\n        elif 'pick' in command_lower or 'grasp' in command_lower:\n            # Extract object from command\n            object_name = self._extract_object_from_command(command)\n            return {\n                'type': 'manipulation_request',\n                'response': f'I will pick up the {object_name} for you.',\n                'action': 'manipulation',\n                'object': object_name\n            }\n        else:\n            return {\n                'type': 'unknown_command',\n                'response': 'I did not understand that command. Could you please repeat?',\n                'action': 'confused'\n            }\n\n    def _process_gesture(self, gesture: Gesture) -> Dict:\n        \"\"\"Process gesture and generate response\"\"\"\n        gesture_type = gesture.type.lower()\n\n        if gesture_type == 'pointing':\n            return {\n                'type': 'object_identification',\n                'response': 'I see you are pointing at something. What would you like me to do with it?',\n                'action': 'look_at_pointed_location'\n            }\n        elif gesture_type == 'beckoning':\n            return {\n                'type': 'approach_request',\n                'response': 'I am coming to you now.',\n                'action': 'approach_user'\n            }\n        elif gesture_type == 'stop':\n            return {\n                'type': 'stop_request',\n                'response': 'I will stop my current action.',\n                'action': 'stop_motion'\n            }\n        else:\n            return {\n                'type': 'unknown_gesture',\n                'response': 'I did not recognize that gesture.',\n                'action': 'idle'\n            }\n\n    def _process_touch_event(self, touch_event: TouchEvent) -> Dict:\n        \"\"\"Process touch event and generate response\"\"\"\n        location = touch_event.location\n\n        if 'head' in location.lower():\n            return {\n                'type': 'head_touch',\n                'response': 'Hello! I feel your touch on my head.',\n                'action': 'happy_animation'\n            }\n        elif 'hand' in location.lower():\n            return {\n                'type': 'hand_touch',\n                'response': 'Nice to meet you! I feel your handshake.',\n                'action': 'greeting_response'\n            }\n        else:\n            return {\n                'type': 'touch_response',\n                'response': 'I feel your touch.',\n                'action': 'acknowledge_touch'\n            }\n\n    def _process_vision_command(self, command: str) -> Dict:\n        \"\"\"Process vision-based command\"\"\"\n        # Vision commands typically come from object detection or scene analysis\n        if 'red cup' in command.lower():\n            return {\n                'type': 'object_identification',\n                'response': 'I see a red cup. Would you like me to pick it up?',\n                'action': 'point_to_object',\n                'object': 'red cup'\n            }\n        elif 'person' in command.lower():\n            return {\n                'type': 'person_detection',\n                'response': 'I see a person. How can I assist?',\n                'action': 'greeting_pose'\n            }\n        else:\n            return {\n                'type': 'scene_analysis',\n                'response': f'I see {command}. How can I help?',\n                'action': 'curious_pose'\n            }\n\n    def _extract_object_from_command(self, command: str) -> str:\n        \"\"\"Extract object name from command\"\"\"\n        # Simple extraction - in practice, use NLP\n        words = command.lower().split()\n        common_objects = ['cup', 'bottle', 'book', 'box', 'apple', 'banana', 'phone']\n\n        for word in words:\n            if word in common_objects:\n                return word\n\n        # If no common object found, return the noun after 'the' or 'a'\n        for i, word in enumerate(words):\n            if word in ['the', 'a', 'an'] and i + 1 < len(words):\n                return words[i + 1]\n\n        return 'object'  # Default if no object found\n\n    def _publish_response(self, response: Dict, modality: str):\n        \"\"\"Publish response through appropriate modalities\"\"\"\n        # Publish to main interaction response\n        response_msg = String()\n        response_msg.data = json.dumps(response)\n        self.interaction_response_pub.publish(response_msg)\n\n        # Publish audio response\n        if 'response' in response:\n            audio_msg = String()\n            audio_msg.data = response['response']\n            self.audio_response_pub.publish(audio_msg)\n\n        # Publish visual feedback if needed\n        if response.get('action') == 'point_to_object':\n            self._publish_pointing_marker(response.get('object'))\n        elif response.get('action') == 'look_at_pointed_location':\n            self._publish_attention_marker()\n\n    def _publish_pointing_marker(self, object_name: str):\n        \"\"\"Publish visual marker for pointing action\"\"\"\n        marker = Marker()\n        marker.header.frame_id = \"map\"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = \"interaction\"\n        marker.id = 0\n        marker.type = Marker.ARROW\n        marker.action = Marker.ADD\n\n        # Set arrow properties\n        marker.scale.x = 0.5  # shaft diameter\n        marker.scale.y = 0.1  # head diameter\n        marker.scale.z = 0.1  # head length\n        marker.color.a = 1.0\n        marker.color.r = 1.0\n        marker.color.g = 1.0\n        marker.color.b = 0.0  # Yellow arrow\n\n        # Position and orientation would be set based on object location\n        marker.pose.position.x = 1.0\n        marker.pose.position.y = 1.0\n        marker.pose.position.z = 0.0\n        marker.pose.orientation.w = 1.0\n\n        marker.text = f\"Pointing to {object_name}\"\n\n        marker_array = MarkerArray()\n        marker_array.markers.append(marker)\n        self.visual_feedback_pub.publish(marker_array)\n\n    def _publish_attention_marker(self):\n        \"\"\"Publish visual marker for attention focus\"\"\"\n        marker = Marker()\n        marker.header.frame_id = \"map\"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = \"interaction\"\n        marker.id = 1\n        marker.type = Marker.SPHERE\n        marker.action = Marker.ADD\n\n        marker.scale.x = 0.2\n        marker.scale.y = 0.2\n        marker.scale.z = 0.2\n        marker.color.a = 0.7\n        marker.color.r = 0.0\n        marker.color.g = 1.0\n        marker.color.b = 0.0  # Green sphere\n\n        marker.pose.position.x = 1.5\n        marker.pose.position.y = 1.5\n        marker.pose.position.z = 0.5\n        marker.pose.orientation.w = 1.0\n\n        marker.text = \"Attention Focus\"\n\n        marker_array = MarkerArray()\n        marker_array.markers.append(marker)\n        self.visual_feedback_pub.publish(marker_array)\n\nclass ResponseGenerator:\n    \"\"\"Generate appropriate responses for different interaction scenarios\"\"\"\n\n    def __init__(self):\n        self.response_templates = {\n            'greeting': [\n                \"Hello! How can I assist you today?\",\n                \"Hi there! What can I do for you?\",\n                \"Greetings! I'm ready to help.\"\n            ],\n            'acknowledgment': [\n                \"I understand.\",\n                \"Got it.\",\n                \"I'll take care of that for you.\"\n            ],\n            'confusion': [\n                \"I didn't quite understand that. Could you please repeat?\",\n                \"I'm not sure what you mean. Can you clarify?\",\n                \"I didn't catch that. Could you say it again?\"\n            ],\n            'success': [\n                \"Task completed successfully!\",\n                \"I've finished what you asked.\",\n                \"All done!\"\n            ],\n            'failure': [\n                \"I couldn't complete that task. Is there something else I can help with?\",\n                \"I encountered an issue. Would you like me to try again?\",\n                \"I'm sorry, I couldn't do that.\"\n            ]\n        }\n\n    def generate_response(self, response_type: str, context: str = \"\") -> str:\n        \"\"\"Generate response based on type and context\"\"\"\n        import random\n\n        if response_type in self.response_templates:\n            responses = self.response_templates[response_type]\n            return random.choice(responses)\n        else:\n            return \"I'm not sure how to respond to that.\"\n"})}),"\n",(0,a.jsx)(e.h2,{id:"safety-and-validation-systems",children:"Safety and Validation Systems"}),"\n",(0,a.jsx)(e.h3,{id:"manipulation-safety-validator",children:"Manipulation Safety Validator"}),"\n",(0,a.jsx)(e.p,{children:"Implement comprehensive safety validation for manipulation tasks:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class ManipulationSafetyValidator:\n    \"\"\"Validate manipulation tasks for safety compliance\"\"\"\n\n    def __init__(self):\n        self.safety_rules = self._load_safety_rules()\n        self.robot_limits = self._load_robot_limits()\n        self.environment_constraints = {}\n\n    def _load_safety_rules(self) -> Dict:\n        \"\"\"Load safety rules for manipulation\"\"\"\n        return {\n            'human_safety': {\n                'minimum_distance': 0.5,  # meters\n                'collision_threshold': 0.1  # meters\n            },\n            'robot_safety': {\n                'joint_limits': True,\n                'force_limits': True,\n                'velocity_limits': True\n            },\n            'object_safety': {\n                'fragile_objects': ['glass', 'ceramic', 'electronics'],\n                'hazardous_objects': ['sharp', 'hot', 'chemical']\n            },\n            'environment_safety': {\n                'restricted_zones': [],\n                'obstacle_avoidance': True\n            }\n        }\n\n    def _load_robot_limits(self) -> Dict:\n        \"\"\"Load robot-specific limits\"\"\"\n        return {\n            'max_payload': 2.0,  # kg\n            'max_velocity': 1.0,  # m/s\n            'max_force': 50.0,    # N\n            'workspace_limits': {\n                'min_x': -1.0, 'max_x': 1.0,\n                'min_y': -1.0, 'max_y': 1.0,\n                'min_z': 0.1, 'max_z': 2.0\n            }\n        }\n\n    def validate_manipulation_task(self, task: Dict, environment_state: Dict) -> Dict:\n        \"\"\"Validate a manipulation task for safety compliance\"\"\"\n        validation_results = {\n            'task_safe': True,\n            'issues': [],\n            'recommendations': [],\n            'risk_level': 'low'\n        }\n\n        # Check human safety\n        human_safety_ok, human_issues = self._check_human_safety(task, environment_state)\n        if not human_safety_ok:\n            validation_results['task_safe'] = False\n            validation_results['issues'].extend(human_issues)\n\n        # Check robot limits\n        robot_limits_ok, robot_issues = self._check_robot_limits(task)\n        if not robot_limits_ok:\n            validation_results['task_safe'] = False\n            validation_results['issues'].extend(robot_issues)\n\n        # Check object safety\n        object_safety_ok, object_issues = self._check_object_safety(task)\n        if not object_safety_ok:\n            validation_results['task_safe'] = False\n            validation_results['issues'].extend(object_issues)\n\n        # Check environment constraints\n        env_ok, env_issues = self._check_environment_constraints(task, environment_state)\n        if not env_ok:\n            validation_results['task_safe'] = False\n            validation_results['issues'].extend(env_issues)\n\n        # Calculate risk level\n        validation_results['risk_level'] = self._calculate_risk_level(\n            validation_results['issues'])\n\n        # Generate recommendations for unsafe tasks\n        if not validation_results['task_safe']:\n            validation_results['recommendations'] = self._generate_recommendations(\n                validation_results['issues'])\n\n        return validation_results\n\n    def _check_human_safety(self, task: Dict, environment_state: Dict) -> Tuple[bool, List[str]]:\n        \"\"\"Check if manipulation task is safe regarding humans\"\"\"\n        issues = []\n\n        humans = environment_state.get('humans', [])\n        task_target = task.get('target_position', {})\n\n        for human in humans:\n            human_pos = human.get('position', {})\n            distance = self._calculate_distance(task_target, human_pos)\n\n            if distance < self.safety_rules['human_safety']['minimum_distance']:\n                issues.append(f\"Task target too close to human at position {human_pos}\")\n\n        return len(issues) == 0, issues\n\n    def _check_robot_limits(self, task: Dict) -> Tuple[bool, List[str]]:\n        \"\"\"Check if task respects robot physical limits\"\"\"\n        issues = []\n\n        # Check payload\n        object_weight = task.get('object_weight', 0)\n        if object_weight > self.robot_limits['max_payload']:\n            issues.append(f\"Object weight {object_weight}kg exceeds maximum payload {self.robot_limits['max_payload']}kg\")\n\n        # Check workspace limits\n        target_pos = task.get('target_position', {})\n        if target_pos:\n            if (target_pos.get('x', 0) < self.robot_limits['workspace_limits']['min_x'] or\n                target_pos.get('x', 0) > self.robot_limits['workspace_limits']['max_x'] or\n                target_pos.get('y', 0) < self.robot_limits['workspace_limits']['min_y'] or\n                target_pos.get('y', 0) > self.robot_limits['workspace_limits']['max_y'] or\n                target_pos.get('z', 0) < self.robot_limits['workspace_limits']['min_z'] or\n                target_pos.get('z', 0) > self.robot_limits['workspace_limits']['max_z']):\n                issues.append(\"Target position outside robot workspace limits\")\n\n        return len(issues) == 0, issues\n\n    def _check_object_safety(self, task: Dict) -> Tuple[bool, List[str]]:\n        \"\"\"Check if object manipulation is safe\"\"\"\n        issues = []\n\n        object_type = task.get('object_type', '').lower()\n        object_name = task.get('object_name', '').lower()\n\n        # Check if object is fragile\n        for fragile in self.safety_rules['object_safety']['fragile_objects']:\n            if fragile in object_type or fragile in object_name:\n                issues.append(f\"Handling fragile object '{object_name}' requires extra care\")\n\n        # Check if object is hazardous\n        for hazardous in self.safety_rules['object_safety']['hazardous_objects']:\n            if hazardous in object_type or hazardous in object_name:\n                issues.append(f\"Object '{object_name}' may be hazardous to handle\")\n\n        return len(issues) == 0, issues\n\n    def _check_environment_constraints(self, task: Dict, environment_state: Dict) -> Tuple[bool, List[str]]:\n        \"\"\"Check environment-specific constraints\"\"\"\n        issues = []\n\n        # Check restricted zones\n        target_pos = task.get('target_position', {})\n        for zone in self.safety_rules['environment_safety']['restricted_zones']:\n            if self._point_in_zone(target_pos, zone):\n                issues.append(f\"Target position in restricted zone: {zone['name']}\")\n\n        # Check obstacles\n        obstacles = environment_state.get('obstacles', [])\n        approach_path = self._calculate_approach_path(task)\n\n        for obstacle in obstacles:\n            if self._path_intersects_obstacle(approach_path, obstacle):\n                issues.append(f\"Approach path intersects with obstacle at {obstacle['position']}\")\n\n        return len(issues) == 0, issues\n\n    def _calculate_distance(self, pos1: Dict, pos2: Dict) -> float:\n        \"\"\"Calculate distance between two positions\"\"\"\n        dx = pos1.get('x', 0) - pos2.get('x', 0)\n        dy = pos1.get('y', 0) - pos2.get('y', 0)\n        dz = pos1.get('z', 0) - pos2.get('z', 0)\n        return np.sqrt(dx*dx + dy*dy + dz*dz)\n\n    def _calculate_approach_path(self, task: Dict) -> List[Dict]:\n        \"\"\"Calculate approach path for manipulation task\"\"\"\n        # Simplified path calculation\n        # In practice, this would use full motion planning\n        target = task.get('target_position', {})\n        approach_distance = 0.1  # 10cm approach\n\n        # Calculate approach point (10cm before target)\n        approach_point = {\n            'x': target.get('x', 0) - approach_distance,\n            'y': target.get('y', 0),\n            'z': target.get('z', 0)\n        }\n\n        return [approach_point, target]\n\n    def _point_in_zone(self, point: Dict, zone: Dict) -> bool:\n        \"\"\"Check if point is within a zone\"\"\"\n        zone_center = zone.get('center', {})\n        zone_radius = zone.get('radius', 0)\n\n        distance = self._calculate_distance(point, zone_center)\n        return distance <= zone_radius\n\n    def _path_intersects_obstacle(self, path: List[Dict], obstacle: Dict) -> bool:\n        \"\"\"Check if path intersects with obstacle\"\"\"\n        obstacle_pos = obstacle.get('position', {})\n        obstacle_radius = obstacle.get('radius', 0.1)\n\n        for point in path:\n            distance = self._calculate_distance(point, obstacle_pos)\n            if distance <= obstacle_radius:\n                return True\n        return False\n\n    def _calculate_risk_level(self, issues: List[str]) -> str:\n        \"\"\"Calculate risk level based on issues\"\"\"\n        if not issues:\n            return 'low'\n        elif len(issues) <= 2:\n            return 'medium'\n        else:\n            return 'high'\n\n    def _generate_recommendations(self, issues: List[str]) -> List[str]:\n        \"\"\"Generate safety recommendations based on issues\"\"\"\n        recommendations = []\n\n        for issue in issues:\n            if 'human' in issue.lower():\n                recommendations.append(\"Maintain safe distance from humans during manipulation\")\n            elif 'payload' in issue.lower():\n                recommendations.append(\"Use appropriate gripping force for object weight\")\n            elif 'workspace' in issue.lower():\n                recommendations.append(\"Verify target position is within robot reach\")\n            elif 'fragile' in issue.lower():\n                recommendations.append(\"Use gentle manipulation for fragile objects\")\n            elif 'hazardous' in issue.lower():\n                recommendations.append(\"Consider safety equipment when handling hazardous objects\")\n\n        return recommendations\n"})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-vla-system",children:"Integration with VLA System"}),"\n",(0,a.jsx)(e.h3,{id:"main-integration-node",children:"Main Integration Node"}),"\n",(0,a.jsx)(e.p,{children:"Create the main integration node that connects all manipulation and interaction components:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class ManipulationInteractionNode(Node):\n    \"\"\"Main integration node for manipulation and interaction systems\"\"\"\n\n    def __init__(self):\n        super().__init__('manipulation_interaction_node')\n\n        # Initialize components\n        self.manipulation_controller = ManipulationController()\n        self.interaction_controller = HumanRobotInteractionController()\n        self.multimodal_interface = MultimodalInteractionInterface()\n        self.safety_validator = ManipulationSafetyValidator()\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, 'manipulation_status', 10)\n        self.action_request_pub = self.create_publisher(\n            String, 'action_requests', 10)\n\n        # Subscriptions\n        self.task_sub = self.create_subscription(\n            String, 'planned_tasks', self.task_callback, 10)\n        self.environment_sub = self.create_subscription(\n            String, 'integrated_perception', self.environment_callback, 10)\n\n        # Internal state\n        self.current_task = None\n        self.environment_state = {}\n        self.manipulation_queue = []\n\n        # Timer for task execution\n        self.execution_timer = self.create_timer(0.1, self._execute_tasks)\n\n        self.get_logger().info(\"Manipulation and Interaction Node initialized\")\n\n    def task_callback(self, msg: String):\n        \"\"\"Handle incoming manipulation tasks\"\"\"\n        try:\n            task_data = json.loads(msg.data)\n\n            # Validate task for safety\n            validation = self.safety_validator.validate_manipulation_task(\n                task_data, self.environment_state)\n\n            if validation['task_safe']:\n                # Add safe task to execution queue\n                self.manipulation_queue.append(task_data)\n                self.get_logger().info(f\"Added safe task to queue: {task_data.get('type', 'unknown')}\")\n            else:\n                self.get_logger().warn(f\"Unsafe task rejected: {validation['issues']}\")\n                # Notify system of unsafe task\n                self._notify_unsafe_task(task_data, validation)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Invalid task data received\")\n\n    def environment_callback(self, msg: String):\n        \"\"\"Update environment state\"\"\"\n        try:\n            self.environment_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().warn(\"Invalid environment data\")\n\n    def _execute_tasks(self):\n        \"\"\"Execute tasks in the queue\"\"\"\n        if self.manipulation_queue and self.current_task is None:\n            # Start next task\n            self.current_task = self.manipulation_queue.pop(0)\n            self._execute_current_task()\n\n    def _execute_current_task(self):\n        \"\"\"Execute the current manipulation task\"\"\"\n        if not self.current_task:\n            return\n\n        task_type = self.current_task.get('type', 'unknown')\n\n        try:\n            if task_type == 'pick_object':\n                success = self._execute_pick_task(self.current_task)\n            elif task_type == 'place_object':\n                success = self._execute_place_task(self.current_task)\n            elif task_type == 'move_to_pose':\n                success = self._execute_move_task(self.current_task)\n            elif task_type == 'interact_with_human':\n                success = self._execute_interaction_task(self.current_task)\n            else:\n                self.get_logger().warn(f\"Unknown task type: {task_type}\")\n                success = False\n\n            if success:\n                self.get_logger().info(f\"Task completed successfully: {task_type}\")\n            else:\n                self.get_logger().error(f\"Task failed: {task_type}\")\n\n        except Exception as e:\n            self.get_logger().error(f\"Error executing task {task_type}: {e}\")\n            success = False\n\n        finally:\n            self.current_task = None\n\n    def _execute_pick_task(self, task: Dict) -> bool:\n        \"\"\"Execute object pick task\"\"\"\n        object_name = task.get('object_name')\n        object_pose = task.get('object_pose')\n\n        if object_pose:\n            # Use provided pose\n            pose = Pose()\n            pose.position.x = object_pose.get('x', 0)\n            pose.position.y = object_pose.get('y', 0)\n            pose.position.z = object_pose.get('z', 0)\n            # Set orientation appropriately\n            pose.orientation.w = 1.0\n        else:\n            # Plan grasp based on object name\n            pose = None\n\n        return self.manipulation_controller.pick_object(object_name, pose)\n\n    def _execute_place_task(self, task: Dict) -> bool:\n        \"\"\"Execute object place task\"\"\"\n        object_name = task.get('object_name')\n        place_pose = task.get('place_pose')\n\n        if place_pose:\n            pose = Pose()\n            pose.position.x = place_pose.get('x', 0)\n            pose.position.y = place_pose.get('y', 0)\n            pose.position.z = place_pose.get('z', 0)\n            pose.orientation.w = 1.0\n\n            return self.manipulation_controller.place_object(object_name, pose)\n\n        return False\n\n    def _execute_move_task(self, task: Dict) -> bool:\n        \"\"\"Execute move to pose task\"\"\"\n        target_pose = task.get('target_pose')\n\n        if target_pose:\n            pose = Pose()\n            pose.position.x = target_pose.get('x', 0)\n            pose.position.y = target_pose.get('y', 0)\n            pose.position.z = target_pose.get('z', 0)\n            pose.orientation.w = 1.0\n\n            return self.manipulation_controller.move_to_pose(pose)\n\n        return False\n\n    def _execute_interaction_task(self, task: Dict) -> bool:\n        \"\"\"Execute human interaction task\"\"\"\n        interaction_type = task.get('interaction_type')\n        target_human = task.get('target_human')\n\n        # Interaction tasks are handled by the interaction controller\n        # This is a simplified implementation\n        if interaction_type == 'greet':\n            response = self.multimodal_interface.response_generator.generate_response('greeting')\n            self.get_logger().info(f\"Greeting response: {response}\")\n            return True\n        elif interaction_type == 'assist':\n            response = self.multimodal_interface.response_generator.generate_response('acknowledgment')\n            self.get_logger().info(f\"Assistance response: {response}\")\n            return True\n\n        return False\n\n    def _notify_unsafe_task(self, task: Dict, validation: Dict):\n        \"\"\"Notify system about unsafe task\"\"\"\n        notification = {\n            'type': 'unsafe_task_rejected',\n            'task': task,\n            'validation_issues': validation['issues'],\n            'recommendations': validation['recommendations'],\n            'timestamp': self.get_clock().now().to_msg()\n        }\n\n        notification_msg = String()\n        notification_msg.data = json.dumps(notification)\n        self.status_pub.publish(notification_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ManipulationInteractionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,a.jsx)(e.h3,{id:"1-set-up-the-manipulation-package",children:"1. Set Up the Manipulation Package"}),"\n",(0,a.jsx)(e.p,{children:"Create the ROS 2 package for manipulation and interaction:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Create package\ncd ~/ros2_ws/src\nros2 pkg create --build-type ament_python manipulation_interaction\ncd manipulation_interaction\nmkdir -p manipulation_interaction/config manipulation_interaction/launch\n"})}),"\n",(0,a.jsx)(e.h3,{id:"2-install-dependencies",children:"2. Install Dependencies"}),"\n",(0,a.jsx)(e.p,{children:"Create requirements file:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Create requirements.txt\ncat > manipulation_interaction/requirements.txt << EOF\ntorch>=2.0.0\nopen3d>=0.17.0\nscipy>=1.10.0\nnumpy>=1.21.0\ntf-transformations>=1.0.0\nmoveit-ros>=2.0.0\nopencv-python>=4.8.0\nEOF\n"})}),"\n",(0,a.jsx)(e.h3,{id:"3-configure-the-system",children:"3. Configure the System"}),"\n",(0,a.jsx)(e.p,{children:"Create a launch file for the manipulation and interaction system:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:"\x3c!-- manipulation_interaction/launch/manipulation_interaction.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config_dir = os.path.join(\n        get_package_share_directory('manipulation_interaction'), 'config')\n\n    return LaunchDescription([\n        Node(\n            package='manipulation_interaction',\n            executable='manipulation_interaction_node',\n            name='manipulation_interaction_node',\n            parameters=[],\n            output='screen'\n        ),\n        Node(\n            package='manipulation_interaction',\n            executable='manipulation_controller',\n            name='manipulation_controller',\n            output='screen'\n        ),\n        Node(\n            package='manipulation_interaction',\n            executable='human_robot_interaction_controller',\n            name='human_robot_interaction_controller',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(e.h3,{id:"4-testing-the-system",children:"4. Testing the System"}),"\n",(0,a.jsx)(e.p,{children:"Create a test script to verify manipulation and interaction:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# test_manipulation_interaction.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport time\n\nclass ManipulationTestClient(Node):\n    def __init__(self):\n        super().__init__('manipulation_test_client')\n\n        # Publishers for testing\n        self.task_pub = self.create_publisher(\n            String, 'planned_tasks', 10)\n        self.voice_pub = self.create_publisher(\n            String, 'voice_command', 10)\n        self.perception_pub = self.create_publisher(\n            String, 'integrated_perception', 10)\n\n        # Subscription to status\n        self.status_sub = self.create_subscription(\n            String, 'manipulation_status',\n            self.status_callback, 10)\n\n        self.timer = self.create_timer(3.0, self.send_test_commands)\n        self.command_count = 0\n\n    def send_test_commands(self):\n        \"\"\"Send test commands to manipulation system\"\"\"\n        test_tasks = [\n            {\n                'type': 'pick_object',\n                'object_name': 'red_cup',\n                'object_pose': {'x': 1.0, 'y': 0.5, 'z': 0.2}\n            },\n            {\n                'type': 'place_object',\n                'object_name': 'red_cup',\n                'place_pose': {'x': 0.5, 'y': 1.0, 'z': 0.2}\n            },\n            {\n                'type': 'interact_with_human',\n                'interaction_type': 'greet',\n                'target_human': 'person_1'\n            }\n        ]\n\n        if self.command_count < len(test_tasks):\n            task_msg = String()\n            task_msg.data = json.dumps(test_tasks[self.command_count])\n            self.task_pub.publish(task_msg)\n\n            self.get_logger().info(f\"Sent test task: {test_tasks[self.command_count]['type']}\")\n            self.command_count += 1\n\n    def status_callback(self, msg: String):\n        \"\"\"Handle status updates\"\"\"\n        self.get_logger().info(f\"Received status: {msg.data[:100]}...\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    test_client = ManipulationTestClient()\n\n    try:\n        rclpy.spin(test_client)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        test_client.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices-and-considerations",children:"Best Practices and Considerations"}),"\n",(0,a.jsx)(e.h3,{id:"1-safety-first-approach",children:"1. Safety First Approach"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Always validate manipulation tasks before execution"}),"\n",(0,a.jsx)(e.li,{children:"Implement multiple safety layers (hardware and software)"}),"\n",(0,a.jsx)(e.li,{children:"Maintain emergency stop capabilities"}),"\n",(0,a.jsx)(e.li,{children:"Monitor robot and environment continuously"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-grasp-planning-optimization",children:"2. Grasp Planning Optimization"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use object-specific grasp strategies"}),"\n",(0,a.jsx)(e.li,{children:"Consider object properties (weight, fragility, shape)"}),"\n",(0,a.jsx)(e.li,{children:"Plan approach and retreat trajectories carefully"}),"\n",(0,a.jsx)(e.li,{children:"Validate grasp stability before execution"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-human-robot-interaction",children:"3. Human-Robot Interaction"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design intuitive interaction protocols"}),"\n",(0,a.jsx)(e.li,{children:"Provide clear feedback during interactions"}),"\n",(0,a.jsx)(e.li,{children:"Respect personal space and comfort zones"}),"\n",(0,a.jsx)(e.li,{children:"Implement graceful failure handling"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"4-performance-optimization",children:"4. Performance Optimization"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use efficient collision checking algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Implement grasp learning for improved performance"}),"\n",(0,a.jsx)(e.li,{children:"Optimize motion planning for real-time execution"}),"\n",(0,a.jsx)(e.li,{children:"Cache frequently used grasp configurations"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(e.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Grasp Planning Failures"}),": Verify object detection accuracy and point cloud quality"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Violations"}),": Check safety rule configurations and sensor calibrations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Motion Planning Failures"}),": Validate robot kinematic models and joint limits"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Interaction Timing"}),": Adjust interaction protocols for natural communication"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Enable detailed logging for each manipulation component"}),"\n",(0,a.jsx)(e.li,{children:"Monitor robot state and joint positions during execution"}),"\n",(0,a.jsx)(e.li,{children:"Use RViz for visualizing planned trajectories and grasps"}),"\n",(0,a.jsx)(e.li,{children:"Test individual components before system integration"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"This manipulation and interaction systems module provides the physical execution capabilities for Vision-Language-Action systems, enabling humanoid robots to perform complex tasks while maintaining safe and natural interactions with humans and the environment."})]})}function _(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>r});var o=t(6540);const a={},s=o.createContext(a);function i(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);