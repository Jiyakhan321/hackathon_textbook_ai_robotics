"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7781],{2871:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vision-language-action/voice-processing/voice-to-action-whisper","title":"Voice-to-Action Using OpenAI Whisper","description":"Overview","source":"@site/docs/module-4-vision-language-action/voice-processing/voice-to-action-whisper.md","sourceDirName":"module-4-vision-language-action/voice-processing","slug":"/module-4-vision-language-action/voice-processing/voice-to-action-whisper","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/voice-processing/voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/voice-processing/voice-to-action-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Voice Recognition and Processing for Humanoid Robots","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/voice-processing/voice-recognition"},"next":{"title":"LLM Integration for Action Planning","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/llm-integration/"}}');var o=i(4848),t=i(8453);const a={sidebar_position:2},r="Voice-to-Action Using OpenAI Whisper",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Speech Recognition Pipeline for Humanoid Robots",id:"speech-recognition-pipeline-for-humanoid-robots",level:2},{value:"1. Audio Input and Preprocessing",id:"1-audio-input-and-preprocessing",level:3},{value:"2. Whisper Integration with ROS 2",id:"2-whisper-integration-with-ros-2",level:3},{value:"3. Voice Command Validation and Safety",id:"3-voice-command-validation-and-safety",level:3},{value:"Real-time Voice Processing Pipeline",id:"real-time-voice-processing-pipeline",level:2},{value:"1. Complete Voice Processing System",id:"1-complete-voice-processing-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Efficient Whisper Processing",id:"1-efficient-whisper-processing",level:3},{value:"Launch Files and Configuration",id:"launch-files-and-configuration",level:2},{value:"1. Complete Voice System Launch",id:"1-complete-voice-system-launch",level:3},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action-using-openai-whisper",children:"Voice-to-Action Using OpenAI Whisper"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Speech recognition is a critical component of natural human-robot interaction. This section covers implementing voice-to-action systems using OpenAI Whisper, focusing on real-time speech recognition and integration with ROS 2 for humanoid robot control. Whisper provides robust, multilingual speech recognition capabilities that are essential for creating intuitive voice interfaces."}),"\n",(0,o.jsx)(n.h2,{id:"speech-recognition-pipeline-for-humanoid-robots",children:"Speech Recognition Pipeline for Humanoid Robots"}),"\n",(0,o.jsx)(n.h3,{id:"1-audio-input-and-preprocessing",children:"1. Audio Input and Preprocessing"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots require specialized audio processing to handle the challenges of real-world environments:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# audio_input_handler.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import AudioData\nimport pyaudio\nimport numpy as np\nimport webrtcvad\nfrom collections import deque\nimport threading\nimport time\n\nclass AudioInputHandler(Node):\n    """\n    Handle audio input for humanoid robot voice recognition\n    """\n    def __init__(self):\n        super().__init__(\'audio_input_handler\')\n\n        # Audio configuration\n        self.sample_rate = 16000  # Whisper works best at 16kHz\n        self.chunk_size = 1024\n        self.channels = 1\n        self.format = pyaudio.paInt16\n\n        # Voice Activity Detection\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(2)  # Aggressive mode for robotics\n\n        # Audio buffers\n        self.audio_buffer = deque(maxlen=30)  # 30 chunks for 1.9s of audio\n        self.speech_segments = []\n        self.is_listening = False\n\n        # Publishers\n        self.audio_pub = self.create_publisher(AudioData, \'/audio_input\', 10)\n        self.speech_detected_pub = self.create_publisher(Bool, \'/speech_detected\', 10)\n        self.voice_activity_pub = self.create_publisher(Bool, \'/voice_activity\', 10)\n\n        # Initialize PyAudio\n        self.audio_interface = pyaudio.PyAudio()\n\n        # Start audio capture thread\n        self.audio_thread = threading.Thread(target=self.capture_audio)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n\n        # Create timer for voice activity detection\n        self.vad_timer = self.create_timer(0.1, self.check_voice_activity)\n\n        self.get_logger().info(\'Audio Input Handler initialized\')\n\n    def capture_audio(self):\n        """\n        Continuously capture audio from microphone\n        """\n        stream = self.audio_interface.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        self.get_logger().info(\'Audio capture started\')\n\n        while rclpy.ok():\n            try:\n                # Read audio chunk\n                audio_chunk = stream.read(self.chunk_size, exception_on_overflow=False)\n\n                # Convert to numpy array for processing\n                audio_array = np.frombuffer(audio_chunk, dtype=np.int16)\n\n                # Add to buffer\n                self.audio_buffer.append(audio_array)\n\n                # Check for voice activity\n                if self.is_voice_active(audio_chunk):\n                    self.speech_segments.append(audio_chunk)\n                    self.is_listening = True\n\n                    # If we have accumulated enough speech, publish it\n                    if len(self.speech_segments) > 50:  # ~3.2 seconds of speech\n                        self.publish_speech_segment()\n                        self.speech_segments = []\n                else:\n                    if self.is_listening and len(self.speech_segments) > 0:\n                        # End of speech detected\n                        self.publish_speech_segment()\n                        self.speech_segments = []\n                        self.is_listening = False\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in audio capture: {e}\')\n                time.sleep(0.1)\n\n        stream.stop_stream()\n        stream.close()\n\n    def is_voice_active(self, audio_chunk):\n        """\n        Check if voice is active in audio chunk using VAD\n        """\n        try:\n            # VAD requires 10, 20, or 30ms frames\n            frame_duration = 20  # ms\n            frame_size = int(self.sample_rate * frame_duration / 1000)\n\n            # Split chunk into VAD-compatible frames\n            for i in range(0, len(audio_chunk), frame_size):\n                frame = audio_chunk[i:i+frame_size]\n                if len(frame) == frame_size:\n                    # VAD expects 16-bit PCM data\n                    try:\n                        is_speech = self.vad.is_speech(frame, self.sample_rate)\n                        if is_speech:\n                            return True\n                    except:\n                        continue\n            return False\n        except:\n            return False\n\n    def publish_speech_segment(self):\n        """\n        Publish accumulated speech segment\n        """\n        if not self.speech_segments:\n            return\n\n        # Combine all speech segments\n        combined_audio = b\'\'.join(self.speech_segments)\n\n        # Create and publish audio message\n        audio_msg = AudioData()\n        audio_msg.data = combined_audio\n        self.audio_pub.publish(audio_msg)\n\n        # Publish speech detected flag\n        speech_msg = Bool()\n        speech_msg.data = True\n        self.speech_detected_pub.publish(speech_msg)\n\n        self.get_logger().info(f\'Published speech segment: {len(combined_audio)} bytes\')\n\n    def check_voice_activity(self):\n        """\n        Periodically check for voice activity\n        """\n        voice_active = self.is_listening\n        activity_msg = Bool()\n        activity_msg.data = voice_active\n        self.voice_activity_pub.publish(activity_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    audio_handler = AudioInputHandler()\n\n    try:\n        rclpy.spin(audio_handler)\n    except KeyboardInterrupt:\n        audio_handler.get_logger().info(\'Shutting down audio input handler\')\n    finally:\n        audio_handler.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-whisper-integration-with-ros-2",children:"2. Whisper Integration with ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"Implementing Whisper for real-time speech recognition:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# whisper_integration.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nimport io\nimport wave\nimport tempfile\nimport os\nfrom threading import Lock\n\nclass WhisperROSIntegration(Node):\n    \"\"\"\n    Integrate OpenAI Whisper with ROS 2 for speech recognition\n    \"\"\"\n    def __init__(self):\n        super().__init__('whisper_integration')\n\n        # Initialize Whisper model\n        self.get_logger().info('Loading Whisper model...')\n        self.model = whisper.load_model(\"base\")  # Use \"small\" or \"medium\" for better accuracy\n        self.get_logger().info('Whisper model loaded')\n\n        # Create subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            '/audio_input',\n            self.audio_callback,\n            10\n        )\n\n        # Create publishers\n        self.transcription_pub = self.create_publisher(String, '/transcription', 10)\n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\n        self.processing_status_pub = self.create_publisher(Bool, '/whisper_processing', 10)\n\n        # Processing state\n        self.processing_lock = Lock()\n        self.is_processing = False\n\n        # Command keywords for humanoid robots\n        self.command_keywords = [\n            'move', 'go', 'walk', 'turn', 'stop', 'start',\n            'pick', 'place', 'grab', 'release', 'take',\n            'find', 'look', 'see', 'show', 'bring',\n            'hello', 'hi', 'help', 'please', 'thank you'\n        ]\n\n        self.get_logger().info('Whisper Integration initialized')\n\n    def audio_callback(self, msg):\n        \"\"\"\n        Process incoming audio data with Whisper\n        \"\"\"\n        if self.is_processing:\n            self.get_logger().warn('Whisper is busy, dropping audio packet')\n            return\n\n        # Process audio in separate thread to avoid blocking\n        processing_thread = threading.Thread(\n            target=self.process_audio_with_whisper,\n            args=(msg,)\n        )\n        processing_thread.start()\n\n    def process_audio_with_whisper(self, audio_msg):\n        \"\"\"\n        Process audio data using Whisper model\n        \"\"\"\n        with self.processing_lock:\n            self.is_processing = True\n\n        # Publish processing status\n        status_msg = Bool()\n        status_msg.data = True\n        self.processing_status_pub.publish(status_msg)\n\n        try:\n            # Convert audio data to numpy array\n            audio_array = np.frombuffer(audio_msg.data, dtype=np.int16)\n\n            # Normalize audio to float32\n            audio_float = audio_array.astype(np.float32) / 32768.0\n\n            # Process with Whisper\n            result = self.model.transcribe(\n                audio_float,\n                language='en',\n                fp16=torch.cuda.is_available()  # Use fp16 if CUDA available\n            )\n\n            transcription = result['text'].strip()\n\n            if transcription:\n                self.get_logger().info(f'Transcribed: \"{transcription}\"')\n\n                # Publish transcription\n                trans_msg = String()\n                trans_msg.data = transcription\n                self.transcription_pub.publish(trans_msg)\n\n                # Check if this is a command\n                if self.is_command(transcription):\n                    cmd_msg = String()\n                    cmd_msg.data = transcription\n                    self.command_pub.publish(cmd_msg)\n                    self.get_logger().info(f'Command detected: \"{transcription}\"')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in Whisper processing: {e}')\n\n        finally:\n            # Reset processing state\n            with self.processing_lock:\n                self.is_processing = False\n\n            # Publish processing status\n            status_msg = Bool()\n            status_msg.data = False\n            self.processing_status_pub.publish(status_msg)\n\n    def is_command(self, transcription):\n        \"\"\"\n        Check if transcription contains robot commands\n        \"\"\"\n        transcription_lower = transcription.lower()\n\n        # Check for command keywords\n        for keyword in self.command_keywords:\n            if keyword in transcription_lower:\n                return True\n\n        # Check for action verbs\n        action_verbs = ['move', 'go', 'walk', 'turn', 'stop', 'start', 'pick', 'place']\n        for verb in action_verbs:\n            if verb in transcription_lower:\n                return True\n\n        return False\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperROSIntegration()\n\n    try:\n        rclpy.spin(whisper_node)\n    except KeyboardInterrupt:\n        whisper_node.get_logger().info('Shutting down Whisper integration')\n    finally:\n        whisper_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"3-voice-command-validation-and-safety",children:"3. Voice Command Validation and Safety"}),"\n",(0,o.jsx)(n.p,{children:"Implementing safety checks for voice commands:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# voice_command_validator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import PoseStamped\nimport json\nimport re\nfrom enum import Enum\n\nclass SafetyLevel(Enum):\n    SAFE = 0\n    WARNING = 1\n    DANGEROUS = 2\n\nclass VoiceCommandValidator(Node):\n    \"\"\"\n    Validate voice commands for safety and correctness\n    \"\"\"\n    def __init__(self):\n        super().__init__('voice_command_validator')\n\n        # Create subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice_command',\n            self.command_callback,\n            10\n        )\n\n        # Create publishers\n        self.validated_command_pub = self.create_publisher(String, '/validated_command', 10)\n        self.safety_status_pub = self.create_publisher(String, '/command_safety_status', 10)\n        self.request_clarification_pub = self.create_publisher(String, '/request_clarification', 10)\n\n        # Safety configuration\n        self.dangerous_commands = [\n            'shutdown', 'power off', 'emergency stop', 'danger', 'harm', 'damage'\n        ]\n\n        self.dangerous_objects = [\n            'fire', 'hot', 'sharp', 'dangerous', 'poison', 'toxic'\n        ]\n\n        # Movement constraints\n        self.max_distance = 10.0  # meters\n        self.max_height = 2.0    # meters\n\n        self.get_logger().info('Voice Command Validator initialized')\n\n    def command_callback(self, msg):\n        \"\"\"\n        Process incoming voice command with validation\n        \"\"\"\n        command = msg.data.strip()\n\n        self.get_logger().info(f'Validating command: \"{command}\"')\n\n        # Analyze command for safety\n        safety_level, safety_issues = self.analyze_safety(command)\n\n        if safety_level == SafetyLevel.DANGEROUS:\n            self.get_logger().error(f'Dangerous command blocked: {command}')\n            self.publish_safety_status(f'DANGEROUS: {\"; \".join(safety_issues)}')\n            return\n        elif safety_level == SafetyLevel.WARNING:\n            self.get_logger().warn(f'Warning for command: {command} - {\"; \".join(safety_issues)}')\n            self.request_clarification(command, safety_issues)\n        else:\n            self.get_logger().info(f'Command validated: {command}')\n            self.publish_safety_status('SAFE')\n            self.validated_command_pub.publish(msg)\n\n    def analyze_safety(self, command):\n        \"\"\"\n        Analyze command for safety issues\n        \"\"\"\n        safety_issues = []\n\n        # Check for dangerous keywords\n        for dangerous_cmd in self.dangerous_commands:\n            if dangerous_cmd.lower() in command.lower():\n                safety_issues.append(f'Dangerous keyword: {dangerous_cmd}')\n\n        # Check for dangerous objects\n        for dangerous_obj in self.dangerous_objects:\n            if dangerous_obj.lower() in command.lower():\n                safety_issues.append(f'Dangerous object: {dangerous_obj}')\n\n        # Check for movement commands that exceed limits\n        distance = self.extract_distance(command)\n        if distance and distance > self.max_distance:\n            safety_issues.append(f'Distance too far: {distance}m > {self.max_distance}m')\n\n        height = self.extract_height(command)\n        if height and height > self.max_height:\n            safety_issues.append(f'Height too high: {height}m > {self.max_height}m')\n\n        # Check for ambiguous commands\n        if self.is_ambiguous(command):\n            safety_issues.append('Command is ambiguous')\n\n        if safety_issues:\n            if any('DANGEROUS' in issue for issue in safety_issues):\n                return SafetyLevel.DANGEROUS, safety_issues\n            else:\n                return SafetyLevel.WARNING, safety_issues\n\n        return SafetyLevel.SAFE, []\n\n    def extract_distance(self, command):\n        \"\"\"\n        Extract distance from command using regex\n        \"\"\"\n        # Look for patterns like \"go 5 meters\", \"move 3 feet\", etc.\n        distance_patterns = [\n            r'(\\d+(?:\\.\\d+)?)\\s*meters?',  # meters\n            r'(\\d+(?:\\.\\d+)?)\\s*feet',     # feet\n            r'(\\d+(?:\\.\\d+)?)\\s*steps?',   # steps\n        ]\n\n        for pattern in distance_patterns:\n            match = re.search(pattern, command, re.IGNORECASE)\n            if match:\n                distance = float(match.group(1))\n                # Convert feet to meters if needed\n                if 'feet' in match.group(0).lower():\n                    distance *= 0.3048\n                return distance\n\n        return None\n\n    def extract_height(self, command):\n        \"\"\"\n        Extract height from command\n        \"\"\"\n        height_patterns = [\n            r'(\\d+(?:\\.\\d+)?)\\s*meters?\\s*(?:high|tall|up)',\n            r'(\\d+(?:\\.\\d+)?)\\s*feet\\s*(?:high|tall|up)',\n        ]\n\n        for pattern in height_patterns:\n            match = re.search(pattern, command, re.IGNORECASE)\n            if match:\n                height = float(match.group(1))\n                if 'feet' in match.group(0).lower():\n                    height *= 0.3048\n                return height\n\n        return None\n\n    def is_ambiguous(self, command):\n        \"\"\"\n        Check if command is ambiguous\n        \"\"\"\n        ambiguous_indicators = [\n            'that', 'there', 'it', 'thing', 'object', 'stuff',\n            'somewhere', 'anywhere', 'here', 'there'\n        ]\n\n        command_lower = command.lower()\n        for indicator in ambiguous_indicators:\n            if indicator in command_lower:\n                # Check if there's specific context\n                if not any(specific in command_lower for specific in ['kitchen', 'table', 'chair', 'door', 'window']):\n                    return True\n\n        return False\n\n    def request_clarification(self, command, issues):\n        \"\"\"\n        Request clarification for ambiguous or potentially unsafe commands\n        \"\"\"\n        clarification_msg = String()\n        clarification_msg.data = f'Command \"{command}\" needs clarification: {\"; \".join(issues)}'\n        self.request_clarification_pub.publish(clarification_msg)\n\n    def publish_safety_status(self, status):\n        \"\"\"\n        Publish safety status\n        \"\"\"\n        status_msg = String()\n        status_msg.data = status\n        self.safety_status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = VoiceCommandValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        validator.get_logger().info('Shutting down voice command validator')\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-voice-processing-pipeline",children:"Real-time Voice Processing Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"1-complete-voice-processing-system",children:"1. Complete Voice Processing System"}),"\n",(0,o.jsx)(n.p,{children:"Creating a complete voice processing pipeline:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# voice_processing_pipeline.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import AudioData\nfrom geometry_msgs.msg import PoseStamped\nimport threading\nimport queue\nimport time\nfrom collections import deque\n\nclass VoiceProcessingPipeline(Node):\n    """\n    Complete voice processing pipeline for humanoid robots\n    """\n    def __init__(self):\n        super().__init__(\'voice_processing_pipeline\')\n\n        # Initialize components\n        self.command_queue = queue.Queue(maxsize=10)\n        self.response_queue = queue.Queue(maxsize=10)\n        self.command_history = deque(maxlen=50)\n\n        # Create subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'/audio_input\',\n            self.audio_callback,\n            10\n        )\n\n        self.transcription_sub = self.create_subscription(\n            String,\n            \'/transcription\',\n            self.transcription_callback,\n            10\n        )\n\n        self.validated_command_sub = self.create_subscription(\n            String,\n            \'/validated_command\',\n            self.validated_command_callback,\n            10\n        )\n\n        # Create publishers\n        self.response_pub = self.create_publisher(String, \'/robot_response\', 10)\n        self.status_pub = self.create_publisher(String, \'/voice_pipeline_status\', 10)\n\n        # Pipeline state\n        self.is_active = True\n        self.pipeline_status = "IDLE"\n\n        # Start processing threads\n        self.command_processing_thread = threading.Thread(\n            target=self.process_commands\n        )\n        self.command_processing_thread.daemon = True\n        self.command_processing_thread.start()\n\n        self.get_logger().info(\'Voice Processing Pipeline initialized\')\n\n    def audio_callback(self, msg):\n        """\n        Handle incoming audio data\n        """\n        self.update_status("RECEIVING_AUDIO")\n        self.get_logger().debug(\'Audio received\')\n\n    def transcription_callback(self, msg):\n        """\n        Handle incoming transcriptions\n        """\n        self.update_status("PROCESSING_TRANSCRIPTION")\n        self.get_logger().info(f\'Transcription received: {msg.data}\')\n\n        # Add to command queue for processing\n        try:\n            self.command_queue.put_nowait(msg.data)\n        except queue.Full:\n            self.get_logger().warn(\'Command queue full, dropping transcription\')\n\n    def validated_command_callback(self, msg):\n        """\n        Handle validated commands\n        """\n        self.update_status("EXECUTING_COMMAND")\n        self.get_logger().info(f\'Executing validated command: {msg.data}\')\n\n        # Add to execution queue\n        try:\n            self.command_queue.put_nowait(f"EXECUTE: {msg.data}")\n        except queue.Full:\n            self.get_logger().warn(\'Execution queue full\')\n\n    def process_commands(self):\n        """\n        Process commands from queue\n        """\n        while self.is_active:\n            try:\n                command = self.command_queue.get(timeout=1.0)\n\n                if command.startswith("EXECUTE: "):\n                    actual_command = command[9:]  # Remove "EXECUTE: " prefix\n                    self.execute_command(actual_command)\n                else:\n                    self.process_command(command)\n\n                self.command_queue.task_done()\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Error processing command: {e}\')\n\n    def process_command(self, command):\n        """\n        Process a voice command\n        """\n        self.get_logger().info(f\'Processing command: {command}\')\n\n        # Add to history\n        self.command_history.append({\n            \'command\': command,\n            \'timestamp\': time.time()\n        })\n\n        # Generate response\n        response = self.generate_response(command)\n\n        # Publish response\n        response_msg = String()\n        response_msg.data = response\n        self.response_pub.publish(response_msg)\n\n        self.get_logger().info(f\'Response: {response}\')\n\n    def execute_command(self, command):\n        """\n        Execute a validated command\n        """\n        self.get_logger().info(f\'Executing command: {command}\')\n\n        # This is where you would integrate with the robot\'s action system\n        # For now, just log the execution\n        self.update_status(f"EXECUTING: {command}")\n\n        # Simulate command execution\n        time.sleep(0.5)  # Simulate execution time\n\n        # Publish completion\n        response_msg = String()\n        response_msg.data = f"Command \'{command}\' executed successfully"\n        self.response_pub.publish(response_msg)\n\n    def generate_response(self, command):\n        """\n        Generate appropriate response for a command\n        """\n        command_lower = command.lower()\n\n        if any(greeting in command_lower for greeting in [\'hello\', \'hi\', \'hey\']):\n            return "Hello! How can I help you today?"\n        elif any(help_request in command_lower for help_request in [\'help\', \'what can you do\']):\n            return "I can navigate, pick up objects, and respond to voice commands. What would you like me to do?"\n        elif any(thanks in command_lower for thanks in [\'thank you\', \'thanks\']):\n            return "You\'re welcome! Is there anything else I can help with?"\n        else:\n            return f"I understand you said: \'{command}\'. How should I respond to this?"\n\n    def update_status(self, status):\n        """\n        Update and publish pipeline status\n        """\n        self.pipeline_status = status\n        status_msg = String()\n        status_msg.data = status\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    pipeline = VoiceProcessingPipeline()\n\n    try:\n        rclpy.spin(pipeline)\n    except KeyboardInterrupt:\n        pipeline.get_logger().info(\'Shutting down voice processing pipeline\')\n        pipeline.is_active = False\n    finally:\n        pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"1-efficient-whisper-processing",children:"1. Efficient Whisper Processing"}),"\n",(0,o.jsx)(n.p,{children:"Optimizing Whisper for real-time performance:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# efficient_whisper_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nfrom threading import Lock, Thread\nimport time\nfrom collections import deque\n\nclass EfficientWhisperProcessor(Node):\n    \"\"\"\n    Efficient Whisper processor optimized for real-time humanoid applications\n    \"\"\"\n    def __init__(self):\n        super().__init__('efficient_whisper_processor')\n\n        # Initialize Whisper model with optimizations\n        self.get_logger().info('Loading optimized Whisper model...')\n\n        # Use GPU if available\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = whisper.load_model(\"base\").to(device)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n        self.get_logger().info(f'Whisper model loaded on {device}')\n\n        # Create subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            '/audio_input',\n            self.audio_callback,\n            10\n        )\n\n        # Create publishers\n        self.transcription_pub = self.create_publisher(String, '/transcription', 10)\n        self.performance_pub = self.create_publisher(String, '/whisper_performance', 10)\n\n        # Processing optimization\n        self.processing_lock = Lock()\n        self.is_processing = False\n        self.processing_times = deque(maxlen=100)  # Track last 100 processing times\n\n        # Audio processing parameters\n        self.sample_rate = 16000\n        self.max_audio_duration = 10.0  # Maximum 10 seconds for processing\n\n        # Create timer for performance monitoring\n        self.performance_timer = self.create_timer(5.0, self.report_performance)\n\n        self.get_logger().info('Efficient Whisper Processor initialized')\n\n    def audio_callback(self, msg):\n        \"\"\"\n        Process incoming audio with optimized Whisper\n        \"\"\"\n        if self.is_processing:\n            self.get_logger().warn('Whisper is busy, dropping audio')\n            return\n\n        # Start processing in background thread\n        processing_thread = Thread(\n            target=self.process_audio_optimized,\n            args=(msg,)\n        )\n        processing_thread.daemon = True\n        processing_thread.start()\n\n    def process_audio_optimized(self, audio_msg):\n        \"\"\"\n        Optimized audio processing with Whisper\n        \"\"\"\n        start_time = time.time()\n\n        with self.processing_lock:\n            if self.is_processing:\n                return  # Another thread already started processing\n            self.is_processing = True\n\n        try:\n            # Convert audio data to numpy array\n            audio_array = np.frombuffer(audio_msg.data, dtype=np.int16)\n\n            # Normalize and convert to float\n            audio_float = audio_array.astype(np.float32) / 32768.0\n\n            # Limit audio duration to prevent long processing times\n            max_samples = int(self.max_audio_duration * self.sample_rate)\n            if len(audio_float) > max_samples:\n                self.get_logger().info(f'Truncating audio from {len(audio_float)/self.sample_rate:.2f}s to {self.max_audio_duration}s')\n                audio_float = audio_float[:max_samples]\n\n            # Process with Whisper using optimized settings\n            with torch.no_grad():  # Disable gradient computation for inference\n                result = self.model.transcribe(\n                    audio_float,\n                    language='en',\n                    task='transcribe',\n                    temperature=0.0,  # Use greedy decoding for speed\n                    best_of=1,        # Only use the best result\n                    fp16=torch.cuda.is_available()\n                )\n\n            transcription = result['text'].strip()\n\n            if transcription:\n                self.get_logger().info(f'Transcribed: \"{transcription}\"')\n\n                # Publish transcription\n                trans_msg = String()\n                trans_msg.data = transcription\n                self.transcription_pub.publish(trans_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in optimized Whisper processing: {e}')\n\n        finally:\n            # Calculate and store processing time\n            processing_time = time.time() - start_time\n            self.processing_times.append(processing_time)\n\n            # Reset processing state\n            with self.processing_lock:\n                self.is_processing = False\n\n    def report_performance(self):\n        \"\"\"\n        Report performance metrics\n        \"\"\"\n        if self.processing_times:\n            avg_time = sum(self.processing_times) / len(self.processing_times)\n            max_time = max(self.processing_times)\n            min_time = min(self.processing_times)\n\n            perf_msg = String()\n            perf_msg.data = f'Whisper Performance - Avg: {avg_time:.3f}s, Min: {min_time:.3f}s, Max: {max_time:.3f}s, Count: {len(self.processing_times)}'\n            self.performance_pub.publish(perf_msg)\n\n            self.get_logger().info(f'Performance: Avg {avg_time:.3f}s, Current load: {len(self.processing_times)}/100')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = EfficientWhisperProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info('Shutting down efficient Whisper processor')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"launch-files-and-configuration",children:"Launch Files and Configuration"}),"\n",(0,o.jsx)(n.h3,{id:"1-complete-voice-system-launch",children:"1. Complete Voice System Launch"}),"\n",(0,o.jsx)(n.p,{children:"Creating launch files for the complete voice system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# launch/voice_recognition_system.launch.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    whisper_model = LaunchConfiguration('whisper_model')\n\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation time'\n    )\n\n    declare_whisper_model = DeclareLaunchArgument(\n        'whisper_model',\n        default_value='base',\n        description='Whisper model size (tiny, base, small, medium, large)'\n    )\n\n    # Audio input handler\n    audio_input_handler = Node(\n        package='humanoid_voice_system',\n        executable='audio_input_handler',\n        name='audio_input_handler',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'sample_rate': 16000,\n            'chunk_size': 1024,\n            'channels': 1\n        }],\n        remappings=[\n            ('/audio_input', '/audio_input'),\n            ('/speech_detected', '/speech_detected'),\n            ('/voice_activity', '/voice_activity')\n        ]\n    )\n\n    # Whisper integration\n    whisper_integration = Node(\n        package='humanoid_voice_system',\n        executable='whisper_integration',\n        name='whisper_integration',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'whisper_model': whisper_model\n        }],\n        remappings=[\n            ('/audio_input', '/audio_input'),\n            ('/transcription', '/transcription'),\n            ('/voice_command', '/voice_command'),\n            ('/whisper_processing', '/whisper_processing')\n        ]\n    )\n\n    # Voice command validator\n    voice_validator = Node(\n        package='humanoid_voice_system',\n        executable='voice_command_validator',\n        name='voice_command_validator',\n        parameters=[{\n            'use_sim_time': use_sim_time\n        }],\n        remappings=[\n            ('/voice_command', '/voice_command'),\n            ('/validated_command', '/validated_command'),\n            ('/command_safety_status', '/command_safety_status'),\n            ('/request_clarification', '/request_clarification')\n        ]\n    )\n\n    # Voice processing pipeline\n    voice_pipeline = Node(\n        package='humanoid_voice_system',\n        executable='voice_processing_pipeline',\n        name='voice_processing_pipeline',\n        parameters=[{\n            'use_sim_time': use_sim_time\n        }],\n        remappings=[\n            ('/audio_input', '/audio_input'),\n            ('/transcription', '/transcription'),\n            ('/validated_command', '/validated_command'),\n            ('/robot_response', '/robot_response'),\n            ('/voice_pipeline_status', '/voice_pipeline_status')\n        ]\n    )\n\n    # Efficient Whisper processor (alternative to basic integration)\n    efficient_whisper = Node(\n        package='humanoid_voice_system',\n        executable='efficient_whisper_processor',\n        name='efficient_whisper_processor',\n        parameters=[{\n            'use_sim_time': use_sim_time\n        }],\n        remappings=[\n            ('/audio_input', '/audio_input'),\n            ('/transcription', '/transcription'),\n            ('/whisper_performance', '/whisper_performance')\n        ]\n    )\n\n    return LaunchDescription([\n        declare_use_sim_time,\n        declare_whisper_model,\n\n        audio_input_handler,\n        whisper_integration,\n        voice_validator,\n        voice_pipeline,\n        efficient_whisper\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"With the voice recognition and Whisper integration properly implemented, you're ready to move on to implementing cognitive planning using Large Language Models (LLMs) to convert natural language commands into robotic actions. The next section will cover connecting LLMs to ROS 2 systems for intelligent action planning and task decomposition."})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var s=i(6540);const o={},t=s.createContext(o);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);