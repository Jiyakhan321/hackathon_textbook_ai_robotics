"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7176],{8e3:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/module-4-project","title":"Module 4 Project: Vision-Language-Action Humanoid Robot","description":"Project Overview","source":"@site/docs/module-4-vision-language-action/module-4-project.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/module-4-project","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/module-4-project","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/module-4-project.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Multimodal Perception Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/practical-exercises/multimodal-perception"},"next":{"title":"Specification Quality Checklist: Physical AI and Humanoid Robotics Textbook","permalink":"/hackathon_textbook_ai_robotics/docs/specs/physical-ai-humanoid-textbook/checklists/requirements"}}');var i=t(4848),a=t(8453);const s={sidebar_position:5},r="Module 4 Project: Vision-Language-Action Humanoid Robot",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Component Interaction Flow",id:"component-interaction-flow",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Voice Command Processing",id:"step-1-voice-command-processing",level:3},{value:"Step 2: LLM Cognitive Planning",id:"step-2-llm-cognitive-planning",level:3},{value:"Step 3: Navigation System",id:"step-3-navigation-system",level:3},{value:"Step 4: Object Manipulation System",id:"step-4-object-manipulation-system",level:3},{value:"Step 5: System Integration and Behavior Manager",id:"step-5-system-integration-and-behavior-manager",level:3},{value:"Step 6: Configuration Files",id:"step-6-configuration-files",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Tests",id:"unit-tests",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:3},{value:"Deployment and Operation",id:"deployment-and-operation",level:2},{value:"System Setup",id:"system-setup",level:3},{value:"Launching the System",id:"launching-the-system",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Tips",id:"debugging-tips",level:3},{value:"Summary",id:"summary",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"module-4-project-vision-language-action-humanoid-robot",children:"Module 4 Project: Vision-Language-Action Humanoid Robot"})}),"\n",(0,i.jsx)(e.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(e.p,{children:"In this capstone project, we'll build a complete Vision-Language-Action system for a humanoid robot that can understand voice commands, plan actions using LLMs, navigate to targets, and manipulate objects. This integrates all components from Modules 1-4 into a cohesive autonomous system."}),"\n",(0,i.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    HUMANOID ROBOT SYSTEM                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   VOICE INPUT   \u2502  \u2502   VISION        \u2502  \u2502   ACTION        \u2502  \u2502\n\u2502  \u2502   PROCESSING    \u2502  \u2502   PROCESSING    \u2502  \u2502   EXECUTION     \u2502  \u2502\n\u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502\n\u2502  \u2502 \u2022 Speech-to-    \u2502  \u2502 \u2022 RGB-D Camera  \u2502  \u2502 \u2022 Navigation    \u2502  \u2502\n\u2502  \u2502   Text          \u2502  \u2502 \u2022 Object Detec. \u2502  \u2502 \u2022 Manipulation  \u2502  \u2502\n\u2502  \u2502 \u2022 Noise Filter  \u2502  \u2502 \u2022 3D Tracking   \u2502  \u2502 \u2022 Grasping      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502            \u2502                  \u2502                      \u2502          \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                               \u2502                                  \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502            \u2502   LLM PLANNER   \u2502 \u25c4\u2500\u2500\u25ba  COGNITIVE CONTROL      \u2502  \u2502\n\u2502            \u2502                 \u2502 \u2502  \u2502   (Behavior Manager)     \u2502  \u2502\n\u2502            \u2502 \u2022 Task Planning \u2502 \u2502  \u2502 \u2022 State Machine          \u2502  \u2502\n\u2502            \u2502 \u2022 Action Seq.   \u2502 \u2502  \u2502 \u2022 Failure Recovery       \u2502  \u2502\n\u2502            \u2502 \u2022 Context       \u2502 \u2502  \u2502 \u2022 Safety Monitoring      \u2502  \u2502\n\u2502            \u2502   Reasoning     \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                                \u2502\n\u2502                      \u2502         \u2502                                \u2502\n\u2502                      \u25bc         \u25bc                                \u2502\n\u2502            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502            \u2502              ROS 2 MIDDLEWARE                   \u2502  \u2502\n\u2502            \u2502  \u2022 Topics, Services, Actions                    \u2502  \u2502\n\u2502            \u2502  \u2022 Distributed Computing Framework            \u2502  \u2502\n\u2502            \u2502  \u2022 Hardware Abstraction Layer                 \u2502  \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(e.h3,{id:"component-interaction-flow",children:"Component Interaction Flow"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"VOICE COMMAND \u2192 [STT] \u2192 TEXT \u2192 [LLM] \u2192 ACTION PLAN \u2192 [NAVIGATION] \u2192 [MANIPULATION]\n      \u2191                                              \u2193              \u2193\n[ERROR HANDLING] \u2190 [FEEDBACK] \u2190 [PERCEPTION] \u2190 [MONITORING] \u2190 [EXECUTION]\n"})}),"\n",(0,i.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(e.h3,{id:"step-1-voice-command-processing",children:"Step 1: Voice Command Processing"}),"\n",(0,i.jsx)(e.p,{children:"First, we'll implement the voice command processing pipeline that converts speech to actionable commands:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import speech_recognition as sr\nimport asyncio\nimport json\nfrom typing import Dict, List, Optional\n\nclass VoiceCommandProcessor:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Configure recognizer\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Command vocabulary\n        self.command_patterns = [\n            r"pick up the (\\w+)",\n            r"go to the (\\w+)",\n            r"bring me the (\\w+)",\n            r"move the (\\w+) to the (\\w+)",\n            r"find the (\\w+)",\n        ]\n\n    async def listen_for_command(self) -> Optional[str]:\n        """Listen for voice commands and return recognized text."""\n        try:\n            with self.microphone as source:\n                print("Listening for command...")\n                audio = self.recognizer.listen(source, timeout=5.0, phrase_time_limit=5.0)\n\n            # Recognize speech\n            text = self.recognizer.recognize_google(audio)\n            print(f"Recognized: {text}")\n            return text\n\n        except sr.WaitTimeoutError:\n            print("No speech detected within timeout")\n            return None\n        except sr.UnknownValueError:\n            print("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            print(f"Error with speech recognition service: {e}")\n            return None\n\n    def parse_command(self, text: str) -> Dict:\n        """Parse voice command into structured action."""\n        import re\n\n        text_lower = text.lower()\n\n        # Handle pickup command\n        pickup_match = re.search(r"pick up the (\\w+)", text_lower)\n        if pickup_match:\n            return {\n                "action": "pickup",\n                "object": pickup_match.group(1),\n                "location": None\n            }\n\n        # Handle navigation command\n        go_to_match = re.search(r"go to the (\\w+)", text_lower)\n        if go_to_match:\n            return {\n                "action": "navigate",\n                "object": None,\n                "location": go_to_match.group(1)\n            }\n\n        # Handle bring command\n        bring_match = re.search(r"bring me the (\\w+)", text_lower)\n        if bring_match:\n            return {\n                "action": "bring",\n                "object": bring_match.group(1),\n                "location": "user"\n            }\n\n        # Handle move command\n        move_match = re.search(r"move the (\\w+) to the (\\w+)", text_lower)\n        if move_match:\n            return {\n                "action": "move",\n                "object": move_match.group(1),\n                "location": move_match.group(2)\n            }\n\n        # Default unknown command\n        return {\n            "action": "unknown",\n            "object": None,\n            "location": None,\n            "raw_text": text\n        }\n\nclass VoiceControlNode:\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from std_msgs.msg import String\n\n        self.node = Node(\'voice_control_node\')\n        self.command_pub = self.node.create_publisher(String, \'/voice_commands\', 10)\n\n        # Timer for continuous listening\n        self.timer = self.node.create_timer(1.0, self.check_voice_input)\n        self.processor = VoiceCommandProcessor()\n\n    async def check_voice_input(self):\n        """Continuously check for voice input."""\n        command_text = await self.processor.listen_for_command()\n\n        if command_text:\n            parsed_command = self.processor.parse_command(command_text)\n\n            # Publish command\n            cmd_msg = String()\n            cmd_msg.data = json.dumps(parsed_command)\n            self.command_pub.publish(cmd_msg)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-2-llm-cognitive-planning",children:"Step 2: LLM Cognitive Planning"}),"\n",(0,i.jsx)(e.p,{children:"Next, we'll implement the LLM-based cognitive planning system that interprets commands and generates action sequences:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\nimport json\nfrom typing import Dict, List, Any\nimport asyncio\n\nclass LLMParticipant:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n        self.system_prompt = """\n        You are an AI planning system for a humanoid robot. Your role is to interpret high-level commands\n        and break them down into executable action sequences. Consider:\n\n        1. Object locations and accessibility\n        2. Navigation requirements\n        3. Manipulation prerequisites\n        4. Safety constraints\n        5. Environmental context\n\n        Respond with JSON containing the action sequence.\n        """\n\n    async def plan_action_sequence(self, command: Dict, environment_state: Dict) -> List[Dict]:\n        """Generate action sequence from command and environment state."""\n\n        user_prompt = f"""\n        Command: {command}\n        Environment: {environment_state}\n\n        Generate a step-by-step action plan as JSON array with elements like:\n        {{\n            "step": 1,\n            "action": "navigation|perception|manipulation",\n            "description": "...",\n            "parameters": {{}},\n            "success_criteria": "..."\n        }}\n        """\n\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": user_prompt}\n                ],\n                temperature=0.1,\n                max_tokens=1000\n            )\n\n            # Parse the response\n            content = response.choices[0].message.content.strip()\n\n            # Extract JSON from response (handle potential markdown formatting)\n            if content.startswith("```json"):\n                content = content[7:-3]  # Remove ```json and ```\n            elif content.startswith("```"):\n                content = content[3:-3]   # Remove ``` and ```\n\n            plan = json.loads(content)\n            return plan\n\n        except Exception as e:\n            print(f"Error in LLM planning: {e}")\n            # Fallback plan\n            return self.generate_fallback_plan(command)\n\n    def generate_fallback_plan(self, command: Dict) -> List[Dict]:\n        """Generate a basic fallback plan if LLM fails."""\n        if command.get(\'action\') == \'pickup\':\n            return [\n                {\n                    "step": 1,\n                    "action": "navigation",\n                    "description": "Navigate to object location",\n                    "parameters": {"target_object": command.get(\'object\')},\n                    "success_criteria": "Robot at object location"\n                },\n                {\n                    "step": 2,\n                    "action": "perception",\n                    "description": "Locate specific object",\n                    "parameters": {"object_name": command.get(\'object\')},\n                    "success_criteria": "Object detected and localized"\n                },\n                {\n                    "step": 3,\n                    "action": "manipulation",\n                    "description": "Grasp the object",\n                    "parameters": {"object_id": command.get(\'object\')},\n                    "success_criteria": "Object successfully grasped"\n                }\n            ]\n\n        return []\n\nclass CognitivePlannerNode:\n    def __init__(self, llm_api_key: str):\n        import rclpy\n        from rclpy.node import Node\n        from std_msgs.msg import String\n        import threading\n\n        self.node = Node(\'cognitive_planner_node\')\n\n        # Publishers and subscribers\n        self.command_sub = self.node.create_subscription(\n            String, \'/voice_commands\', self.command_callback, 10\n        )\n        self.action_plan_pub = self.node.create_publisher(\n            String, \'/action_plan\', 10\n        )\n        self.environment_sub = self.node.create_subscription(\n            String, \'/environment_state\', self.environment_callback, 10\n        )\n\n        # Initialize LLM planner\n        self.llm_planner = LLMParticipant(llm_api_key)\n        self.current_environment = {}\n\n        # Thread for async LLM calls\n        self.planning_thread = None\n\n    def command_callback(self, msg):\n        """Handle incoming voice commands."""\n        try:\n            command = json.loads(msg.data)\n\n            # Start planning in background thread\n            self.planning_thread = threading.Thread(\n                target=self.async_plan_wrapper,\n                args=(command,)\n            )\n            self.planning_thread.start()\n\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in command: {msg.data}")\n\n    def environment_callback(self, msg):\n        """Update environment state."""\n        try:\n            self.current_environment = json.loads(msg.data)\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in environment: {msg.data}")\n\n    def async_plan_wrapper(self, command):\n        """Wrapper for async planning."""\n        import asyncio\n\n        async def plan():\n            plan = await self.llm_planner.plan_action_sequence(\n                command, self.current_environment\n            )\n\n            # Publish action plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan)\n            self.action_plan_pub.publish(plan_msg)\n\n        # Run async planning\n        asyncio.run(plan())\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-3-navigation-system",children:"Step 3: Navigation System"}),"\n",(0,i.jsx)(e.p,{children:"Now we'll implement the navigation system that moves the robot to target locations:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom typing import Tuple, List, Dict\nimport math\n\nclass NavigationSystem:\n    def __init__(self):\n        # Robot properties\n        self.robot_radius = 0.3  # meters\n        self.max_speed = 0.5     # m/s\n        self.rotation_speed = 0.5  # rad/s\n\n        # Map and localization\n        self.map_resolution = 0.05  # meters per pixel\n        self.local_map = None\n        self.global_map = None\n        self.robot_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n\n    def update_robot_pose(self, pose: np.ndarray):\n        """Update current robot pose."""\n        self.robot_pose = pose\n\n    def find_path(self, start: Tuple[float, float], goal: Tuple[float, float]) -> List[Tuple[float, float]]:\n        """Find path from start to goal using A* algorithm."""\n        import heapq\n\n        def heuristic(pos1, pos2):\n            return math.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)\n\n        def get_neighbors(pos):\n            neighbors = []\n            for dx in [-1, 0, 1]:\n                for dy in [-1, 0, 1]:\n                    if dx == 0 and dy == 0:\n                        continue\n                    new_pos = (pos[0] + dx, pos[1] + dy)\n                    if self.is_valid_position(new_pos):\n                        neighbors.append(new_pos)\n            return neighbors\n\n        def is_valid_position(pos):\n            # Check map bounds and obstacles\n            if (pos[0] < 0 or pos[1] < 0 or\n                pos[0] >= self.local_map.shape[0] or pos[1] >= self.local_map.shape[1]):\n                return False\n\n            # Check if cell is free (assuming 0 = free, 100 = obstacle)\n            return self.local_map[pos[0], pos[1]] < 50\n\n        # Convert world coordinates to grid coordinates\n        start_grid = self.world_to_grid(start)\n        goal_grid = self.world_to_grid(goal)\n\n        open_set = [(0, start_grid)]\n        came_from = {}\n        g_score = {start_grid: 0}\n        f_score = {start_grid: heuristic(start_grid, goal_grid)}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if current == goal_grid:\n                # Reconstruct path\n                path = []\n                while current in came_from:\n                    path.append(self.grid_to_world(current))\n                    current = came_from[current]\n                path.append(self.grid_to_world(start_grid))\n                return path[::-1]\n\n            for neighbor in get_neighbors(current):\n                tentative_g_score = g_score[current] + heuristic(current, neighbor)\n\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + heuristic(neighbor, goal_grid)\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return []  # No path found\n\n    def world_to_grid(self, pos: Tuple[float, float]) -> Tuple[int, int]:\n        """Convert world coordinates to grid coordinates."""\n        grid_x = int((pos[0] - self.robot_pose[0]) / self.map_resolution)\n        grid_y = int((pos[1] - self.robot_pose[1]) / self.map_resolution)\n        return (grid_x, grid_y)\n\n    def grid_to_world(self, pos: Tuple[int, int]) -> Tuple[float, float]:\n        """Convert grid coordinates to world coordinates."""\n        world_x = pos[0] * self.map_resolution + self.robot_pose[0]\n        world_y = pos[1] * self.map_resolution + self.robot_pose[1]\n        return (world_x, world_y)\n\n    def follow_path(self, path: List[Tuple[float, float]]) -> bool:\n        """Follow the planned path."""\n        for waypoint in path:\n            if not self.navigate_to_waypoint(waypoint):\n                return False  # Navigation failed\n        return True\n\n    def navigate_to_waypoint(self, target: Tuple[float, float]) -> bool:\n        """Navigate to a single waypoint."""\n        target_x, target_y = target\n        current_x, current_y = self.robot_pose[0], self.robot_pose[1]\n\n        # Calculate distance and angle to target\n        distance = math.sqrt((target_x - current_x)**2 + (target_y - current_y)**2)\n\n        if distance < 0.1:  # Close enough\n            return True\n\n        target_angle = math.atan2(target_y - current_y, target_x - current_x)\n        angle_diff = target_angle - self.robot_pose[2]\n\n        # Normalize angle difference\n        while angle_diff > math.pi:\n            angle_diff -= 2 * math.pi\n        while angle_diff < -math.pi:\n            angle_diff += 2 * math.pi\n\n        # Rotate towards target\n        if abs(angle_diff) > 0.1:\n            self.rotate_robot(angle_diff)\n\n        # Move forward\n        self.move_forward(min(distance, 0.1))  # Move in small increments\n\n        return True\n\n    def rotate_robot(self, angle: float):\n        """Rotate robot by specified angle."""\n        # Simulate rotation\n        print(f"Rotating robot by {angle} radians")\n        self.robot_pose[2] += angle\n\n    def move_forward(self, distance: float):\n        """Move robot forward by specified distance."""\n        # Simulate forward movement\n        print(f"Moving forward {distance} meters")\n        self.robot_pose[0] += distance * math.cos(self.robot_pose[2])\n        self.robot_pose[1] += distance * math.sin(self.robot_pose[2])\n\nclass NavigationNode:\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from geometry_msgs.msg import PoseStamped, Twist\n        from nav_msgs.msg import OccupancyGrid\n\n        self.node = Node(\'navigation_node\')\n\n        # Publishers and subscribers\n        self.goal_sub = self.node.create_subscription(\n            PoseStamped, \'/navigation_goal\', self.goal_callback, 10\n        )\n        self.cmd_vel_pub = self.node.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.map_sub = self.node.create_subscription(\n            OccupancyGrid, \'/map\', self.map_callback, 10\n        )\n\n        # Navigation system\n        self.nav_system = NavigationSystem()\n        self.current_goal = None\n\n    def map_callback(self, msg):\n        """Update map data."""\n        import numpy as np\n\n        width = msg.info.width\n        height = msg.info.height\n        data = np.array(msg.data).reshape(height, width)\n        self.nav_system.local_map = data\n\n    def goal_callback(self, msg):\n        """Handle navigation goal."""\n        target_x = msg.pose.position.x\n        target_y = msg.pose.position.y\n\n        # Update robot pose (would come from localization)\n        current_pose = np.array([0.0, 0.0, 0.0])  # Placeholder\n        self.nav_system.update_robot_pose(current_pose)\n\n        # Plan and execute navigation\n        path = self.nav_system.find_path(\n            (current_pose[0], current_pose[1]),\n            (target_x, target_y)\n        )\n\n        if path:\n            success = self.nav_system.follow_path(path)\n            print(f"Navigation {\'successful\' if success else \'failed\'}")\n        else:\n            print("No path found to goal")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-4-object-manipulation-system",children:"Step 4: Object Manipulation System"}),"\n",(0,i.jsx)(e.p,{children:"Now we'll implement the object manipulation system for grasping and manipulating objects:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom typing import Dict, List, Tuple\nimport math\n\nclass ManipulationSystem:\n    def __init__(self):\n        # Robot arm properties\n        self.arm_joints = 6  # Number of joints\n        self.joint_limits = [(-2.96, 2.96)] * 6  # Joint limits in radians\n        self.reach = 1.2  # Maximum reach in meters\n        self.gripper_max_aperture = 0.1  # Maximum gripper opening\n        self.gripper_min_aperture = 0.01  # Minimum gripper opening\n\n        # Current state\n        self.current_joints = np.zeros(self.arm_joints)\n        self.end_effector_pose = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])  # x, y, z, roll, pitch, yaw\n        self.gripper_opening = self.gripper_max_aperture\n\n    def calculate_inverse_kinematics(self, target_pose: np.ndarray) -> np.ndarray:\n        """Calculate joint angles for target end-effector pose."""\n        # Simplified analytical IK for demonstration\n        # In practice, use numerical methods or robot-specific solvers\n\n        target_x, target_y, target_z = target_pose[:3]\n\n        # Calculate distance from base to target\n        dist_xy = math.sqrt(target_x**2 + target_y**2)\n        dist_xyz = math.sqrt(dist_xy**2 + target_z**2)\n\n        if dist_xyz > self.reach:\n            # Target out of reach, return current configuration\n            return self.current_joints.copy()\n\n        # Simplified calculation (real implementation would be more complex)\n        joint_angles = np.zeros(self.arm_joints)\n\n        # Base joint (shoulder pan)\n        joint_angles[0] = math.atan2(target_y, target_x)\n\n        # Shoulder lift\n        shoulder_height = 0.2  # Fixed shoulder height\n        arm_length = self.reach / 2  # Simplified arm model\n\n        joint_angles[1] = math.atan2(target_z - shoulder_height, dist_xy)\n\n        # Elbow joint\n        joint_angles[2] = 0.0  # Simplified\n\n        # Wrist joints\n        joint_angles[3] = target_pose[3]  # Roll\n        joint_angles[4] = target_pose[4]  # Pitch\n        joint_angles[5] = target_pose[5]  # Yaw\n\n        # Validate joint limits\n        for i, angle in enumerate(joint_angles):\n            min_lim, max_lim = self.joint_limits[i]\n            joint_angles[i] = max(min_lim, min(max_lim, angle))\n\n        return joint_angles\n\n    def plan_grasp_pose(self, object_info: Dict) -> np.ndarray:\n        """Plan optimal grasp pose for object."""\n        obj_position = np.array(object_info[\'position\'])\n        obj_dimensions = np.array(object_info[\'dimensions\'])\n\n        # Calculate grasp position (slightly above object center)\n        grasp_x = obj_position[0]\n        grasp_y = obj_position[1]\n        grasp_z = obj_position[2] + obj_dimensions[2] / 2 + 0.05  # 5cm above object\n\n        # Determine grasp orientation based on object shape\n        if obj_dimensions[0] > obj_dimensions[1] and obj_dimensions[0] > obj_dimensions[2]:\n            # Longest dimension is X, grasp along Y axis\n            roll = 0.0\n            pitch = 0.0\n            yaw = math.pi / 2  # 90 degrees\n        elif obj_dimensions[1] > obj_dimensions[2]:\n            # Longest dimension is Y, grasp along X axis\n            roll = 0.0\n            pitch = 0.0\n            yaw = 0.0\n        else:\n            # Longest dimension is Z, grasp from top\n            roll = 0.0\n            pitch = math.pi / 2  # 90 degrees pitch\n            yaw = 0.0\n\n        return np.array([grasp_x, grasp_y, grasp_z, roll, pitch, yaw])\n\n    def execute_grasp(self, object_info: Dict) -> bool:\n        """Execute grasp operation on object."""\n        try:\n            # Plan grasp pose\n            grasp_pose = self.plan_grasp_pose(object_info)\n\n            # Calculate approach pose (above object)\n            approach_pose = grasp_pose.copy()\n            approach_pose[2] += 0.1  # 10cm above object\n\n            # Move to approach position\n            if not self.move_to_pose(approach_pose):\n                return False\n\n            # Move to grasp position\n            if not self.move_to_pose(grasp_pose):\n                return False\n\n            # Close gripper\n            object_width = min(object_info[\'dimensions\'])\n            grip_aperture = max(self.gripper_min_aperture, object_width - 0.01)\n            self.close_gripper(grip_aperture)\n\n            # Verify grasp success (simulated)\n            grasp_successful = self.verify_grasp(object_info)\n\n            if grasp_successful:\n                # Lift object slightly\n                lift_pose = grasp_pose.copy()\n                lift_pose[2] += 0.05  # Lift 5cm\n                self.move_to_pose(lift_pose)\n\n                print(f"Successfully grasped {object_info.get(\'name\', \'object\')}")\n                return True\n            else:\n                print("Grasp failed")\n                return False\n\n        except Exception as e:\n            print(f"Error during grasp execution: {e}")\n            return False\n\n    def move_to_pose(self, target_pose: np.ndarray) -> bool:\n        """Move end effector to target pose."""\n        try:\n            # Calculate required joint angles\n            target_joints = self.calculate_inverse_kinematics(target_pose)\n\n            if not self.validate_joints(target_joints):\n                print("Target pose unreachable due to joint limits")\n                return False\n\n            # Simulate movement (in real system, send commands to controllers)\n            self.current_joints = target_joints\n            self.end_effector_pose = target_pose\n\n            print(f"Moved to pose: [{target_pose[0]:.3f}, {target_pose[1]:.3f}, {target_pose[2]:.3f}]")\n            return True\n\n        except Exception as e:\n            print(f"Error moving to pose: {e}")\n            return False\n\n    def validate_joints(self, joints: np.ndarray) -> bool:\n        """Validate joint angles are within limits."""\n        for i, angle in enumerate(joints):\n            min_lim, max_lim = self.joint_limits[i]\n            if angle < min_lim or angle > max_lim:\n                return False\n        return True\n\n    def close_gripper(self, aperture: float):\n        """Close gripper to specified aperture."""\n        self.gripper_opening = max(self.gripper_min_aperture, min(aperture, self.gripper_max_aperture))\n        print(f"Gripper closed to {self.gripper_opening:.3f}m")\n\n    def verify_grasp(self, object_info: Dict) -> bool:\n        """Verify that grasp was successful."""\n        # Simulated grasp verification\n        # In real system, use force/torque sensors or visual feedback\n        import random\n        return random.random() > 0.2  # 80% success rate for simulation\n\nclass ManipulationNode:\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from geometry_msgs.msg import PoseStamped\n        from std_msgs.msg import String\n\n        self.node = Node(\'manipulation_node\')\n\n        # Publishers and subscribers\n        self.grasp_sub = self.node.create_subscription(\n            String, \'/manipulation_command\', self.grasp_callback, 10\n        )\n        self.object_sub = self.node.create_subscription(\n            String, \'/object_detections\', self.object_callback, 10\n        )\n\n        # Manipulation system\n        self.manipulator = ManipulationSystem()\n        self.known_objects = {}\n\n    def object_callback(self, msg):\n        """Update known objects."""\n        try:\n            objects = json.loads(msg.data)\n            for obj in objects:\n                self.known_objects[obj.get(\'class\')] = obj\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in object detections: {msg.data}")\n\n    def grasp_callback(self, msg):\n        """Handle grasp commands."""\n        try:\n            command = json.loads(msg.data)\n            object_name = command.get(\'object_name\')\n\n            if object_name in self.known_objects:\n                object_info = self.known_objects[object_name]\n                success = self.manipulator.execute_grasp(object_info)\n\n                # Publish result\n                result_msg = String()\n                result_msg.data = json.dumps({\n                    \'success\': success,\n                    \'object\': object_name,\n                    \'action\': \'grasp\'\n                })\n\n                # Would publish to result topic\n                # self.result_pub.publish(result_msg)\n\n            else:\n                print(f"Unknown object: {object_name}")\n\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in grasp command: {msg.data}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-5-system-integration-and-behavior-manager",children:"Step 5: System Integration and Behavior Manager"}),"\n",(0,i.jsx)(e.p,{children:"Finally, we'll create the behavior manager that orchestrates all components:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import asyncio\nimport threading\nfrom typing import Dict, List\nimport time\n\nclass BehaviorManager:\n    def __init__(self):\n        # System state\n        self.current_behavior = "idle"\n        self.action_queue = []\n        self.environment_state = {}\n        self.robot_state = {\n            \'position\': [0, 0, 0],\n            \'orientation\': [0, 0, 0, 1],\n            \'gripper\': \'open\',\n            \'holding_object\': None\n        }\n\n        # Component references (would be connected to actual nodes)\n        self.voice_processor = VoiceCommandProcessor()\n        self.llm_planner = LLMParticipant(api_key="your-api-key")\n        self.navigation_system = NavigationSystem()\n        self.manipulation_system = ManipulationSystem()\n\n        # Safety monitors\n        self.emergency_stop = False\n        self.collision_detected = False\n\n    async def start_system(self):\n        """Start the integrated system."""\n        print("Starting Vision-Language-Action System...")\n\n        # Start main control loop\n        while not self.emergency_stop:\n            await self.control_cycle()\n            await asyncio.sleep(0.1)  # 10Hz control loop\n\n    async def control_cycle(self):\n        """Main control cycle."""\n        try:\n            # Process any pending actions\n            if self.action_queue:\n                current_action = self.action_queue[0]\n\n                # Execute action based on type\n                if current_action[\'action\'] == \'navigation\':\n                    await self.execute_navigation(current_action)\n                elif current_action[\'action\'] == \'manipulation\':\n                    await self.execute_manipulation(current_action)\n                elif current_action[\'action\'] == \'perception\':\n                    await self.execute_perception(current_action)\n\n                # Remove completed action\n                if self.is_action_complete(current_action):\n                    self.action_queue.pop(0)\n\n            # Monitor system health\n            self.monitor_safety()\n\n        except Exception as e:\n            print(f"Error in control cycle: {e}")\n\n    async def execute_navigation(self, action: Dict):\n        """Execute navigation action."""\n        target_location = action[\'parameters\'].get(\'target_object\')\n\n        if target_location:\n            # Find object location from environment state\n            target_pose = self.find_object_location(target_location)\n            if target_pose:\n                # Navigate to object\n                path = self.navigation_system.find_path(\n                    self.robot_state[\'position\'][:2],\n                    target_pose[:2]\n                )\n\n                if path:\n                    success = self.navigation_system.follow_path(path)\n                    if success:\n                        # Update robot position\n                        self.robot_state[\'position\'] = target_pose\n                        print(f"Navigation to {target_location} completed")\n                else:\n                    print(f"Could not navigate to {target_location}: no path found")\n\n    async def execute_manipulation(self, action: Dict):\n        """Execute manipulation action."""\n        object_name = action[\'parameters\'].get(\'object_id\')\n\n        if object_name:\n            # Get object information\n            object_info = self.get_object_info(object_name)\n            if object_info:\n                success = self.manipulation_system.execute_grasp(object_info)\n\n                if success:\n                    self.robot_state[\'holding_object\'] = object_name\n                    self.robot_state[\'gripper\'] = \'closed\'\n                    print(f"Manipulation of {object_name} completed")\n            else:\n                print(f"Object {object_name} not found")\n\n    async def execute_perception(self, action: Dict):\n        """Execute perception action."""\n        object_name = action[\'parameters\'].get(\'object_name\')\n\n        # Update environment state with current perceptions\n        self.update_environment_state()\n\n        if object_name:\n            # Verify object exists and is accessible\n            if object_name in self.environment_state:\n                print(f"Object {object_name} confirmed in environment")\n            else:\n                print(f"Object {object_name} not found in environment")\n\n    def is_action_complete(self, action: Dict) -> bool:\n        """Check if action is complete."""\n        # Simple completion check - in real system, use feedback\n        if action[\'action\'] == \'navigation\':\n            target_pos = action[\'parameters\'].get(\'target_object\')\n            if target_pos:\n                target_pose = self.find_object_location(target_pos)\n                if target_pose:\n                    current_pos = self.robot_state[\'position\']\n                    distance = math.sqrt(sum((a-b)**2 for a, b in\n                                           zip(current_pos[:2], target_pose[:2])))\n                    return distance < 0.3  # Within 30cm\n\n        return False\n\n    def find_object_location(self, object_name: str) -> List[float]:\n        """Find location of object in environment."""\n        # In real system, query perception system\n        if object_name in self.environment_state:\n            return self.environment_state[object_name][\'position\']\n        return None\n\n    def get_object_info(self, object_name: str) -> Dict:\n        """Get detailed information about an object."""\n        if object_name in self.environment_state:\n            return self.environment_state[object_name]\n        return None\n\n    def update_environment_state(self):\n        """Update environment state from perception system."""\n        # In real system, subscribe to perception topics\n        pass\n\n    def monitor_safety(self):\n        """Monitor safety conditions."""\n        # Check for collisions, emergency stops, etc.\n        if self.collision_detected:\n            print("Safety violation detected! Stopping all motion.")\n            self.emergency_stop = True\n\n    async def process_voice_command(self, command_text: str):\n        """Process voice command through full pipeline."""\n        # Parse command\n        parsed_command = self.voice_processor.parse_command(command_text)\n\n        # Update environment state\n        self.update_environment_state()\n\n        # Plan action sequence\n        action_plan = await self.llm_planner.plan_action_sequence(\n            parsed_command, self.environment_state\n        )\n\n        # Queue actions for execution\n        for action in action_plan:\n            self.action_queue.append(action)\n\n        print(f"Queued {len(action_plan)} actions for execution")\n\nclass IntegratedSystemNode:\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from std_msgs.msg import String\n\n        self.node = Node(\'integrated_system_node\')\n\n        # Publishers and subscribers\n        self.voice_cmd_sub = self.node.create_subscription(\n            String, \'/voice_commands\', self.voice_command_callback, 10\n        )\n        self.action_plan_sub = self.node.create_subscription(\n            String, \'/action_plan\', self.action_plan_callback, 10\n        )\n        self.environment_sub = self.node.create_subscription(\n            String, \'/environment_state\', self.environment_callback, 10\n        )\n\n        # Behavior manager\n        self.behavior_manager = BehaviorManager()\n\n        # Start system in background\n        self.system_thread = threading.Thread(\n            target=self.run_system\n        )\n        self.system_thread.daemon = True\n        self.system_thread.start()\n\n    def voice_command_callback(self, msg):\n        """Handle voice commands."""\n        try:\n            command_data = json.loads(msg.data)\n            command_text = command_data.get(\'raw_text\', \'\')\n\n            if command_text:\n                # Process command in async context\n                asyncio.run(\n                    self.behavior_manager.process_voice_command(command_text)\n                )\n\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in voice command: {msg.data}")\n\n    def action_plan_callback(self, msg):\n        """Handle received action plans."""\n        try:\n            action_plan = json.loads(msg.data)\n\n            # Add to behavior manager queue\n            for action in action_plan:\n                self.behavior_manager.action_queue.append(action)\n\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in action plan: {msg.data}")\n\n    def environment_callback(self, msg):\n        """Update environment state."""\n        try:\n            env_state = json.loads(msg.data)\n            self.behavior_manager.environment_state = env_state\n        except json.JSONDecodeError:\n            print(f"Invalid JSON in environment state: {msg.data}")\n\n    def run_system(self):\n        """Run the integrated system."""\n        import asyncio\n        asyncio.run(self.behavior_manager.start_system())\n\ndef main():\n    """Main entry point for the integrated system."""\n    import rclpy\n\n    rclpy.init()\n\n    # Initialize all nodes\n    voice_node = VoiceControlNode()\n    planner_node = CognitivePlannerNode(llm_api_key="your-openai-api-key")\n    nav_node = NavigationNode()\n    manip_node = ManipulationNode()\n    system_node = IntegratedSystemNode()\n\n    # Spin all nodes\n    executor = rclpy.executors.MultiThreadedExecutor()\n    executor.add_node(voice_node.node)\n    executor.add_node(planner_node.node)\n    executor.add_node(nav_node.node)\n    executor.add_node(manip_node.node)\n    executor.add_node(system_node.node)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"step-6-configuration-files",children:"Step 6: Configuration Files"}),"\n",(0,i.jsx)(e.p,{children:"Create the necessary configuration files for the system:"}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"config/vision_action_system.yaml:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'# Vision-Action System Configuration\n\nvoice_processing:\n  sample_rate: 16000\n  chunk_size: 1024\n  threshold: 300\n  timeout: 5.0\n  phrase_time_limit: 5.0\n\nllm_planning:\n  api_endpoint: "https://api.openai.com/v1/chat/completions"\n  model: "gpt-4"\n  temperature: 0.1\n  max_tokens: 1000\n  timeout: 30\n\nnavigation:\n  resolution: 0.05\n  robot_radius: 0.3\n  max_speed: 0.5\n  rotation_speed: 0.5\n  obstacle_threshold: 50\n\nmanipulation:\n  joint_limits:\n    - [-2.96, 2.96]\n    - [-2.96, 2.96]\n    - [-2.96, 2.96]\n    - [-2.96, 2.96]\n    - [-2.96, 2.96]\n    - [-2.96, 2.96]\n  reach: 1.2\n  gripper_max_aperture: 0.1\n  gripper_min_aperture: 0.01\n\nbehavior_manager:\n  control_frequency: 10.0\n  safety_monitoring: true\n  emergency_stop_timeout: 5.0\n'})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"launch/vision_action_system.launch.py:"})}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config_dir = os.path.join(get_package_share_directory('vision_action_system'), 'config')\n\n    return LaunchDescription([\n        Node(\n            package='vision_action_system',\n            executable='voice_control_node',\n            name='voice_control_node',\n            parameters=[os.path.join(config_dir, 'vision_action_system.yaml')],\n            output='screen'\n        ),\n\n        Node(\n            package='vision_action_system',\n            executable='cognitive_planner_node',\n            name='cognitive_planner_node',\n            parameters=[os.path.join(config_dir, 'vision_action_system.yaml')],\n            output='screen'\n        ),\n\n        Node(\n            package='vision_action_system',\n            executable='navigation_node',\n            name='navigation_node',\n            parameters=[os.path.join(config_dir, 'vision_action_system.yaml')],\n            output='screen'\n        ),\n\n        Node(\n            package='vision_action_system',\n            executable='manipulation_node',\n            name='manipulation_node',\n            parameters=[os.path.join(config_dir, 'vision_action_system.yaml')],\n            output='screen'\n        ),\n\n        Node(\n            package='vision_action_system',\n            executable='integrated_system_node',\n            name='integrated_system_node',\n            parameters=[os.path.join(config_dir, 'vision_action_system.yaml')],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,i.jsx)(e.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"unit-tests",children:"Unit Tests"}),"\n",(0,i.jsx)(e.p,{children:"Create comprehensive tests for each component:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import unittest\nimport numpy as np\nfrom unittest.mock import Mock, patch\n\nclass TestVisionActionSystem(unittest.TestCase):\n\n    def setUp(self):\n        """Set up test fixtures."""\n        self.vision_processor = MultiModalFusion()\n        self.llm_planner = LLMParticipant(api_key="test-key")\n        self.nav_system = NavigationSystem()\n        self.manip_system = ManipulationSystem()\n        self.behavior_manager = BehaviorManager()\n\n    def test_object_detection(self):\n        """Test object detection functionality."""\n        # Mock RGB and depth images\n        mock_rgb = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        mock_depth = np.random.rand(480, 640).astype(np.float32)\n\n        # Test detection\n        detections = self.vision_processor.fused_detection(mock_rgb, mock_depth)\n\n        # Assertions\n        self.assertIsInstance(detections, list)\n        # Additional assertions based on expected behavior\n\n    def test_navigation_pathfinding(self):\n        """Test navigation pathfinding."""\n        start = (0.0, 0.0)\n        goal = (5.0, 5.0)\n\n        path = self.nav_system.find_path(start, goal)\n\n        self.assertIsInstance(path, list)\n        self.assertGreater(len(path), 0)\n\n    def test_grasp_planning(self):\n        """Test grasp planning."""\n        object_info = {\n            \'position\': [1.0, 1.0, 0.5],\n            \'dimensions\': [0.1, 0.1, 0.1],\n            \'name\': \'cube\'\n        }\n\n        grasp_pose = self.manip_system.plan_grasp_pose(object_info)\n\n        self.assertEqual(len(grasp_pose), 6)  # x, y, z, roll, pitch, yaw\n\n    @patch(\'openai.ChatCompletion.acreate\')\n    def test_llm_planning(self, mock_create):\n        """Test LLM planning."""\n        mock_response = Mock()\n        mock_response.choices = [Mock()]\n        mock_response.choices[0].message = Mock()\n        mock_response.choices[0].message.content = \'[{"step": 1, "action": "navigation", "description": "Go to object", "parameters": {}, "success_criteria": "At location"}]\'\n\n        mock_create.return_value = mock_response\n\n        command = {"action": "pickup", "object": "red_cube", "location": None}\n        env_state = {"objects": [{"name": "red_cube", "position": [1, 1, 0]}]}\n\n        plan = self.llm_planner.plan_action_sequence(command, env_state)\n\n        self.assertIsInstance(plan, list)\n        self.assertGreater(len(plan), 0)\n\nclass TestIntegration(unittest.TestCase):\n    """Integration tests for the full system."""\n\n    def test_voice_to_action_pipeline(self):\n        """Test complete voice-to-action pipeline."""\n        # This would test the full flow from voice command to action execution\n        pass\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,i.jsx)(e.p,{children:"Monitor system performance with benchmarks:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import time\nimport psutil\nimport threading\nfrom collections import deque\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.cpu_history = deque(maxlen=100)\n        self.memory_history = deque(maxlen=100)\n        self.latency_history = deque(maxlen=100)\n\n        self.monitoring = False\n        self.monitor_thread = None\n\n    def start_monitoring(self):\n        """Start performance monitoring."""\n        self.monitoring = True\n        self.monitor_thread = threading.Thread(target=self._monitor_loop)\n        self.monitor_thread.daemon = True\n        self.monitor_thread.start()\n\n    def _monitor_loop(self):\n        """Monitoring loop."""\n        while self.monitoring:\n            cpu_percent = psutil.cpu_percent(interval=1)\n            memory_percent = psutil.virtual_memory().percent\n\n            self.cpu_history.append(cpu_percent)\n            self.memory_history.append(memory_percent)\n\n    def measure_latency(self, func, *args, **kwargs):\n        """Measure function execution latency."""\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n\n        latency = (end_time - start_time) * 1000  # Convert to milliseconds\n        self.latency_history.append(latency)\n\n        return result, latency\n\n    def get_statistics(self):\n        """Get current performance statistics."""\n        stats = {\n            \'cpu_avg\': sum(self.cpu_history) / len(self.cpu_history) if self.cpu_history else 0,\n            \'memory_avg\': sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0,\n            \'latency_avg\': sum(self.latency_history) / len(self.latency_history) if self.latency_history else 0,\n            \'latency_min\': min(self.latency_history) if self.latency_history else 0,\n            \'latency_max\': max(self.latency_history) if self.latency_history else 0\n        }\n        return stats\n\n# Usage example\nperf_monitor = PerformanceMonitor()\nperf_monitor.start_monitoring()\n\n# Measure specific operations\nresult, latency = perf_monitor.measure_latency(\n    lambda: self.vision_processor.fused_detection(rgb_img, depth_img)\n)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"deployment-and-operation",children:"Deployment and Operation"}),"\n",(0,i.jsx)(e.h3,{id:"system-setup",children:"System Setup"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Hardware Requirements:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"RGB-D camera (Intel RealSense, Kinect, etc.)"}),"\n",(0,i.jsx)(e.li,{children:"Microphone array for voice input"}),"\n",(0,i.jsx)(e.li,{children:"Humanoid robot platform with manipulator arms"}),"\n",(0,i.jsx)(e.li,{children:"Computer with sufficient processing power (GPU recommended)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Software Dependencies:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"ROS 2 (Humble Hawksbill or later)"}),"\n",(0,i.jsx)(e.li,{children:"Python 3.8+"}),"\n",(0,i.jsx)(e.li,{children:"OpenCV"}),"\n",(0,i.jsx)(e.li,{children:"PyTorch/TensorFlow"}),"\n",(0,i.jsx)(e.li,{children:"OpenAI API client"}),"\n",(0,i.jsx)(e.li,{children:"Speech recognition libraries"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Installation:"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Install ROS 2 dependencies\nsudo apt update\nsudo apt install ros-humble-desktop-full\nsource /opt/ros/humble/setup.bash\n\n# Install Python packages\npip install openai opencv-python speechrecognition torch torchvision numpy scipy\n\n# Build the workspace\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,i.jsx)(e.h3,{id:"launching-the-system",children:"Launching the System"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Launch the complete system\nros2 launch vision_action_system vision_action_system.launch.py\n\n# Or launch individual components\nros2 run vision_action_system voice_control_node\nros2 run vision_action_system cognitive_planner_node\n"})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Voice Recognition Problems:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Check microphone permissions and configuration"}),"\n",(0,i.jsx)(e.li,{children:"Adjust ambient noise threshold"}),"\n",(0,i.jsx)(e.li,{children:"Verify internet connectivity for cloud-based STT"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Navigation Failures:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Ensure map is properly loaded"}),"\n",(0,i.jsx)(e.li,{children:"Check localization accuracy"}),"\n",(0,i.jsx)(e.li,{children:"Verify obstacle detection parameters"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Manipulation Failures:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Calibrate camera-robot coordinate transformation"}),"\n",(0,i.jsx)(e.li,{children:"Verify object detection accuracy"}),"\n",(0,i.jsx)(e.li,{children:"Check gripper calibration"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Enable detailed logging for each component"}),"\n",(0,i.jsx)(e.li,{children:"Monitor ROS 2 topics and services"}),"\n",(0,i.jsx)(e.li,{children:"Use visualization tools (RViz) to inspect perception results"}),"\n",(0,i.jsx)(e.li,{children:"Implement graceful error handling and recovery mechanisms"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This Vision-Language-Action system demonstrates the integration of multiple complex technologies to create an intelligent humanoid robot capable of understanding natural language commands, perceiving its environment, and executing meaningful actions. The system follows a modular architecture with clear interfaces between components, making it extensible and maintainable."}),"\n",(0,i.jsx)(e.p,{children:"Key achievements include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Voice command processing with natural language understanding"}),"\n",(0,i.jsx)(e.li,{children:"Multi-modal perception combining RGB and depth information"}),"\n",(0,i.jsx)(e.li,{children:"LLM-powered cognitive planning for complex task decomposition"}),"\n",(0,i.jsx)(e.li,{children:"Safe navigation and manipulation capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Real-time system integration using ROS 2"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The project serves as a foundation for advanced robotics applications and can be extended with additional capabilities such as multi-robot coordination, learning from demonstration, or enhanced safety features."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);