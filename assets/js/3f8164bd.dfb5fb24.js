"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4009],{8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},9345:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vision-language-action/voice-processing/voice-recognition","title":"Voice Recognition and Processing for Humanoid Robots","description":"Overview","source":"@site/docs/module-4-vision-language-action/voice-processing/voice-recognition.md","sourceDirName":"module-4-vision-language-action/voice-processing","slug":"/module-4-vision-language-action/voice-processing/voice-recognition","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/voice-processing/voice-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/voice-processing/voice-recognition.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA) for Humanoid Robots","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/intro"},"next":{"title":"Voice-to-Action Using OpenAI Whisper","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/voice-processing/voice-to-action-whisper"}}');var t=o(4848),a=o(8453);const r={sidebar_position:2},s="Voice Recognition and Processing for Humanoid Robots",d={},c=[{value:"Overview",id:"overview",level:2},{value:"Audio Input and Preprocessing",id:"audio-input-and-preprocessing",level:2},{value:"1. Audio Capture Configuration",id:"1-audio-capture-configuration",level:3},{value:"2. Noise Reduction and Audio Enhancement",id:"2-noise-reduction-and-audio-enhancement",level:3},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:2},{value:"1. Whisper Setup and Configuration",id:"1-whisper-setup-and-configuration",level:3},{value:"2. Custom Voice Command Recognition",id:"2-custom-voice-command-recognition",level:3},{value:"Real-time Processing Optimization",id:"real-time-processing-optimization",level:2},{value:"1. Multi-threaded Audio Processing",id:"1-multi-threaded-audio-processing",level:3},{value:"Voice Command Validation and Safety",id:"voice-command-validation-and-safety",level:2},{value:"1. Safe Voice Command Processing",id:"1-safe-voice-command-processing",level:3},{value:"Launch Files for Voice Recognition System",id:"launch-files-for-voice-recognition-system",level:2},{value:"1. Complete Voice Recognition Launch",id:"1-complete-voice-recognition-launch",level:3},{value:"Performance Optimization and Real-time Considerations",id:"performance-optimization-and-real-time-considerations",level:2},{value:"1. Real-time Audio Processing Constraints",id:"1-real-time-audio-processing-constraints",level:3},{value:"Next Steps",id:"next-steps",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-recognition-and-processing-for-humanoid-robots",children:"Voice Recognition and Processing for Humanoid Robots"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Voice recognition forms the foundation of natural human-robot interaction in VLA systems. This section covers implementing speech-to-text systems optimized for humanoid robot environments, including real-time processing, noise reduction, and context-aware recognition that enables robots to understand and respond to natural language commands."}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots operate in diverse acoustic environments, requiring robust voice recognition systems that can handle background noise, reverberation, and multiple speakers while maintaining real-time performance for natural interaction."}),"\n",(0,t.jsx)(n.h2,{id:"audio-input-and-preprocessing",children:"Audio Input and Preprocessing"}),"\n",(0,t.jsx)(n.h3,{id:"1-audio-capture-configuration",children:"1. Audio Capture Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Setting up audio input for humanoid robots with optimal configuration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# audio_capture.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import AudioData\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\nimport time\n\nclass HumanoidAudioCapture(Node):\n    """\n    Audio capture system optimized for humanoid robot environments\n    """\n    def __init__(self):\n        super().__init__(\'humanoid_audio_capture\')\n\n        # Audio parameters\n        self.rate = 16000  # 16kHz sample rate (good for speech)\n        self.chunk = 1024  # Number of frames per buffer\n        self.channels = 1  # Mono audio\n        self.format = pyaudio.paInt16  # 16-bit audio\n        self.audio_queue = queue.Queue(maxsize=10)\n\n        # Initialize PyAudio\n        self.pyaudio_instance = pyaudio.PyAudio()\n\n        # Publishers\n        self.audio_pub = self.create_publisher(\n            AudioData,\n            \'/audio/raw\',\n            10\n        )\n\n        self.speech_detected_pub = self.create_publisher(\n            Bool,\n            \'/speech_detected\',\n            10\n        )\n\n        # Audio processing thread\n        self.audio_thread = threading.Thread(target=self.audio_capture_loop)\n        self.running = True\n\n        # Voice activity detection parameters\n        self.energy_threshold = 1000  # Adjust based on environment\n        self.silence_threshold = 0.5  # 50% of energy threshold\n        self.silence_duration = 1.0   # 1 second of silence to stop\n\n        # Start audio capture\n        self.audio_thread.start()\n\n        self.get_logger().info(\'Humanoid Audio Capture initialized\')\n\n    def audio_capture_loop(self):\n        """\n        Continuous audio capture loop\n        """\n        # Open audio stream\n        stream = self.pyaudio_instance.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        self.get_logger().info(\'Audio capture started\')\n\n        while self.running:\n            try:\n                # Read audio data\n                data = stream.read(self.chunk, exception_on_overflow=False)\n\n                # Convert to numpy array for processing\n                audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n                # Check for speech activity\n                energy = np.mean(np.abs(audio_array))\n                speech_detected = energy > self.energy_threshold\n\n                # Publish speech detection\n                speech_msg = Bool()\n                speech_msg.data = speech_detected\n                self.speech_detected_pub.publish(speech_msg)\n\n                # Publish audio data if queue not full\n                if speech_detected and not self.audio_queue.full():\n                    self.audio_queue.put(data)\n\n                # Publish raw audio data periodically\n                if not self.audio_queue.empty():\n                    try:\n                        audio_data = self.audio_queue.get_nowait()\n                        audio_msg = AudioData()\n                        audio_msg.data = audio_data\n                        self.audio_pub.publish(audio_msg)\n                    except queue.Empty:\n                        pass\n\n            except Exception as e:\n                self.get_logger().error(f\'Audio capture error: {e}\')\n                time.sleep(0.1)  # Brief pause before retry\n\n        stream.stop_stream()\n        stream.close()\n\n    def destroy_node(self):\n        """\n        Clean shutdown of audio capture\n        """\n        self.running = False\n        if self.audio_thread.is_alive():\n            self.audio_thread.join()\n\n        # Close PyAudio\n        self.pyaudio_instance.terminate()\n\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    audio_capture = HumanoidAudioCapture()\n\n    try:\n        rclpy.spin(audio_capture)\n    except KeyboardInterrupt:\n        audio_capture.get_logger().info(\'Shutting down audio capture\')\n    finally:\n        audio_capture.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-noise-reduction-and-audio-enhancement",children:"2. Noise Reduction and Audio Enhancement"}),"\n",(0,t.jsx)(n.p,{children:"Implementing noise reduction for humanoid environments:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# noise_reduction.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import AudioData\nfrom std_msgs.msg import String\nimport numpy as np\nfrom scipy import signal\nimport webrtcvad\nimport collections\n\nclass AudioNoiseReducer(Node):\n    """\n    Noise reduction system for humanoid audio processing\n    """\n    def __init__(self):\n        super().__init__(\'audio_noise_reducer\')\n\n        # Subscribe to raw audio\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'/audio/raw\',\n            self.audio_callback,\n            10\n        )\n\n        # Publish enhanced audio\n        self.enhanced_audio_pub = self.create_publisher(\n            AudioData,\n            \'/audio/enhanced\',\n            10\n        )\n\n        self.enhanced_text_pub = self.create_publisher(\n            String,\n            \'/audio/enhanced_status\',\n            10\n        )\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Initialize WebRTC VAD (Voice Activity Detection)\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(2)  # Aggressive mode for noisy environments\n\n        # Noise reduction parameters\n        self.noise_floor = 0.01  # Minimum signal level\n        self.snr_threshold = 10  # Signal-to-noise ratio threshold\n\n        # Audio buffer for noise estimation\n        self.noise_buffer = collections.deque(maxlen=100)\n        self.signal_buffer = collections.deque(maxlen=10)\n\n        self.get_logger().info(\'Audio Noise Reducer initialized\')\n\n    def audio_callback(self, msg):\n        """\n        Process incoming audio data\n        """\n        try:\n            # Convert byte data to numpy array\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32)\n            audio_data = audio_data / 32768.0  # Normalize to [-1, 1]\n\n            # Apply noise reduction\n            enhanced_audio = self.reduce_noise(audio_data)\n\n            # Detect voice activity\n            is_speech = self.detect_voice_activity(enhanced_audio)\n\n            if is_speech:\n                # Convert back to int16 for publishing\n                enhanced_int16 = (enhanced_audio * 32768).astype(np.int16)\n                enhanced_bytes = enhanced_int16.tobytes()\n\n                # Publish enhanced audio\n                enhanced_msg = AudioData()\n                enhanced_msg.data = enhanced_bytes\n                self.enhanced_audio_pub.publish(enhanced_msg)\n\n                # Publish status\n                status_msg = String()\n                status_msg.data = f"Enhanced audio with SNR: {self.estimate_snr(enhanced_audio):.2f}dB"\n                self.enhanced_text_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in audio callback: {e}\')\n\n    def reduce_noise(self, audio_data):\n        """\n        Apply noise reduction to audio data\n        """\n        # Simple spectral subtraction noise reduction\n        # More sophisticated methods can be implemented\n\n        # Convert to frequency domain\n        fft_data = np.fft.fft(audio_data)\n        magnitude = np.abs(fft_data)\n        phase = np.angle(fft_data)\n\n        # Estimate noise floor\n        if len(self.noise_buffer) < 50:\n            # Collect initial noise samples\n            self.noise_buffer.extend(magnitude)\n        else:\n            # Update noise estimate\n            current_noise = np.percentile(magnitude, 10)  # 10th percentile as noise estimate\n            self.noise_buffer.append(current_noise)\n\n        # Calculate average noise level\n        if self.noise_buffer:\n            avg_noise = np.mean(list(self.noise_buffer))\n        else:\n            avg_noise = 0.0\n\n        # Apply spectral subtraction\n        enhanced_magnitude = np.maximum(magnitude - avg_noise * 0.7, 0.1 * magnitude)\n\n        # Reconstruct signal\n        enhanced_fft = enhanced_magnitude * np.exp(1j * phase)\n        enhanced_audio = np.real(np.fft.ifft(enhanced_fft))\n\n        # Normalize to prevent clipping\n        max_val = np.max(np.abs(enhanced_audio))\n        if max_val > 0:\n            enhanced_audio = enhanced_audio / max_val * 0.8  # 80% of max to prevent clipping\n\n        return enhanced_audio\n\n    def detect_voice_activity(self, audio_data):\n        """\n        Detect voice activity using WebRTC VAD\n        """\n        # WebRTC VAD works with specific frame sizes and rates\n        # We\'ll do a simplified version here\n        if len(audio_data) < self.frame_size:\n            return False\n\n        # Calculate energy-based voice activity detection\n        energy = np.mean(np.abs(audio_data))\n        threshold = self.estimate_energy_threshold()\n\n        return energy > threshold\n\n    def estimate_energy_threshold(self):\n        """\n        Estimate adaptive energy threshold for VAD\n        """\n        if len(self.signal_buffer) > 0:\n            recent_energies = list(self.signal_buffer)\n            # Use median to be robust to outliers\n            median_energy = np.median(recent_energies)\n            return median_energy * 0.5  # 50% of median as threshold\n        else:\n            return 0.01  # Default threshold\n\n    def estimate_snr(self, audio_data):\n        """\n        Estimate signal-to-noise ratio\n        """\n        signal_power = np.mean(audio_data ** 2)\n        if len(self.noise_buffer) > 0:\n            noise_power = np.mean(list(self.noise_buffer)) ** 2\n            if noise_power > 0:\n                snr = 10 * np.log10(signal_power / noise_power)\n                return snr\n        return 0.0\n\ndef main(args=None):\n    rclpy.init(args=args)\n    noise_reducer = AudioNoiseReducer()\n\n    try:\n        rclpy.spin(noise_reducer)\n    except KeyboardInterrupt:\n        noise_reducer.get_logger().info(\'Shutting down noise reducer\')\n    finally:\n        noise_reducer.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,t.jsx)(n.h3,{id:"1-whisper-setup-and-configuration",children:"1. Whisper Setup and Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Setting up OpenAI Whisper for humanoid robot voice recognition:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# whisper_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import AudioData\nfrom std_msgs.msg import String\nfrom rclpy.qos import QoSProfile\nimport torch\nimport whisper\nimport numpy as np\nimport io\nimport wave\nfrom threading import Thread, Lock\nfrom queue import Queue\nimport time\n\nclass WhisperSpeechRecognizer(Node):\n    """\n    Speech recognition using OpenAI Whisper\n    """\n    def __init__(self):\n        super().__init__(\'whisper_speech_recognizer\')\n\n        # Subscribe to enhanced audio\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'/audio/enhanced\',\n            self.audio_callback,\n            QoSProfile(depth=10)\n        )\n\n        # Publishers\n        self.transcript_pub = self.create_publisher(\n            String,\n            \'/speech_to_text/transcript\',\n            10\n        )\n\n        self.confidence_pub = self.create_publisher(\n            String,\n            \'/speech_to_text/confidence\',\n            10\n        )\n\n        # Audio processing parameters\n        self.sample_rate = 16000\n        self.chunk_duration = 2.0  # Process 2-second chunks\n        self.min_silence_duration = 0.5  # Minimum speech duration to process\n\n        # Audio buffer for accumulating audio chunks\n        self.audio_buffer = []\n        self.buffer_duration = 0.0\n\n        # Whisper model\n        self.model = None\n        self.load_whisper_model()\n\n        # Processing thread and queue\n        self.audio_queue = Queue(maxsize=5)\n        self.processing_thread = Thread(target=self.process_audio_queue)\n        self.processing_thread.start()\n\n        # Processing lock\n        self.processing_lock = Lock()\n\n        self.get_logger().info(\'Whisper Speech Recognizer initialized\')\n\n    def load_whisper_model(self):\n        """\n        Load Whisper model (use smaller model for real-time performance)\n        """\n        try:\n            # Use \'tiny\' or \'base\' model for real-time performance\n            # Use \'small\' or \'medium\' for better accuracy with more compute\n            self.model = whisper.load_model("base", device="cuda" if torch.cuda.is_available() else "cpu")\n            self.get_logger().info(f\'Whisper model loaded on {"GPU" if torch.cuda.is_available() else "CPU"}\')\n        except Exception as e:\n            self.get_logger().error(f\'Failed to load Whisper model: {e}\')\n            # Fallback: try loading on CPU\n            try:\n                self.model = whisper.load_model("base", device="cpu")\n                self.get_logger().info(\'Whisper model loaded on CPU\')\n            except Exception as e2:\n                self.get_logger().error(f\'Failed to load Whisper model on CPU: {e2}\')\n\n    def audio_callback(self, msg):\n        """\n        Process incoming audio data\n        """\n        if self.model is None:\n            return\n\n        try:\n            # Convert byte data to numpy array\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32)\n            audio_data = audio_data / 32768.0  # Normalize\n\n            # Add to buffer\n            self.audio_buffer.extend(audio_data)\n            self.buffer_duration = len(self.audio_buffer) / self.sample_rate\n\n            # Process buffer if it reaches the desired duration\n            if self.buffer_duration >= self.chunk_duration:\n                # Extract chunk and clear buffer\n                chunk_size = int(self.chunk_duration * self.sample_rate)\n                audio_chunk = np.array(self.audio_buffer[:chunk_size])\n                self.audio_buffer = self.audio_buffer[chunk_size:]\n                self.buffer_duration = len(self.audio_buffer) / self.sample_rate\n\n                # Add to processing queue if not full\n                if not self.audio_queue.full():\n                    self.audio_queue.put(audio_chunk)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in audio callback: {e}\')\n\n    def process_audio_queue(self):\n        """\n        Process audio chunks from the queue in a separate thread\n        """\n        while rclpy.ok():\n            try:\n                # Get audio chunk from queue\n                if not self.audio_queue.empty():\n                    audio_chunk = self.audio_queue.get(timeout=0.1)\n\n                    # Process with Whisper\n                    transcript = self.transcribe_audio(audio_chunk)\n\n                    if transcript and transcript.strip():\n                        # Publish transcript\n                        transcript_msg = String()\n                        transcript_msg.data = transcript\n                        self.transcript_pub.publish(transcript_msg)\n\n                        # Publish confidence (simulated - Whisper doesn\'t provide confidence directly)\n                        confidence_msg = String()\n                        confidence_msg.data = f"high"  # Could be calculated based on various factors\n                        self.confidence_pub.publish(confidence_msg)\n\n                        self.get_logger().info(f\'Transcribed: {transcript}\')\n\n                else:\n                    # Small sleep to prevent busy waiting\n                    time.sleep(0.01)\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in processing thread: {e}\')\n                time.sleep(0.1)\n\n    def transcribe_audio(self, audio_chunk):\n        """\n        Transcribe audio chunk using Whisper\n        """\n        if self.model is None:\n            return None\n\n        try:\n            with self.processing_lock:\n                # Transcribe the audio\n                result = self.model.transcribe(\n                    audio_chunk,\n                    language=\'en\',\n                    task=\'transcribe\',\n                    # Add options to improve real-time performance\n                    fp16=torch.cuda.is_available()\n                )\n\n                return result[\'text\'].strip()\n        except Exception as e:\n            self.get_logger().error(f\'Error transcribing audio: {e}\')\n            return None\n\n    def destroy_node(self):\n        """\n        Clean shutdown\n        """\n        if self.processing_thread.is_alive():\n            # Let the thread finish naturally by rclpy.ok() returning False\n            pass\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperSpeechRecognizer()\n\n    try:\n        rclpy.spin(whisper_node)\n    except KeyboardInterrupt:\n        whisper_node.get_logger().info(\'Shutting down Whisper speech recognizer\')\n    finally:\n        whisper_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-custom-voice-command-recognition",children:"2. Custom Voice Command Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Creating a custom voice command recognition system optimized for robotics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# custom_voice_commands.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom builtin_interfaces.msg import Duration\nimport json\nimport re\nfrom typing import Dict, List, Optional\n\nclass CustomVoiceCommandRecognizer(Node):\n    \"\"\"\n    Custom voice command recognition for humanoid robots\n    \"\"\"\n    def __init__(self):\n        super().__init__('custom_voice_command_recognizer')\n\n        # Subscribe to Whisper transcripts\n        self.transcript_sub = self.create_subscription(\n            String,\n            '/speech_to_text/transcript',\n            self.transcript_callback,\n            10\n        )\n\n        # Publishers for recognized commands\n        self.command_pub = self.create_publisher(\n            String,\n            '/voice_command/parsed',\n            10\n        )\n\n        self.navigation_goal_pub = self.create_publisher(\n            PoseStamped,\n            '/goal_pose',\n            10\n        )\n\n        # Voice command patterns and handlers\n        self.command_patterns = {\n            # Navigation commands\n            'move_to': [\n                r'go to (?:the )?(?P<location>\\w+)',\n                r'move to (?:the )?(?P<location>\\w+)',\n                r'go (?:to|toward) (?:the )?(?P<location>\\w+)',\n                r'walk to (?:the )?(?P<location>\\w+)',\n            ],\n            'come_here': [\n                r'come here',\n                r'come to me',\n                r'come over here',\n            ],\n            'stop': [\n                r'stop',\n                r'hold on',\n                r'wait',\n                r'freeze',\n            ],\n            'turn': [\n                r'turn (?:left|right|around)',\n                r'rotate (?:left|right)',\n                r'face (?:left|right)',\n            ],\n\n            # Manipulation commands\n            'pick_up': [\n                r'pick up (?:the )?(?P<object>\\w+)',\n                r'grab (?:the )?(?P<object>\\w+)',\n                r'take (?:the )?(?P<object>\\w+)',\n                r'lift (?:the )?(?P<object>\\w+)',\n            ],\n            'put_down': [\n                r'put down (?:the )?(?P<object>\\w+)',\n                r'drop (?:the )?(?P<object>\\w+)',\n                r'place (?:the )?(?P<object>\\w+)',\n            ],\n\n            # Interaction commands\n            'follow_me': [\n                r'follow me',\n                r'come with me',\n                r'follow',\n            ],\n            'wait_here': [\n                r'wait here',\n                r'stay here',\n                r'hold position',\n            ],\n        }\n\n        # Location mappings (in a real system, these would come from a map)\n        self.location_map = {\n            'kitchen': {'x': 3.0, 'y': 2.0, 'theta': 0.0},\n            'living_room': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'bedroom': {'x': -2.0, 'y': 3.0, 'theta': 1.57},\n            'bathroom': {'x': -1.0, 'y': -2.0, 'theta': 3.14},\n            'office': {'x': 2.0, 'y': -1.0, 'theta': -1.57},\n        }\n\n        # Object locations (simplified - in real system would come from perception)\n        self.object_locations = {\n            'cup': {'x': 3.2, 'y': 2.1, 'z': 0.8},\n            'book': {'x': 0.1, 'y': 0.1, 'z': 0.9},\n            'ball': {'x': -0.5, 'y': 0.5, 'z': 0.1},\n        }\n\n        self.get_logger().info('Custom Voice Command Recognizer initialized')\n\n    def transcript_callback(self, msg):\n        \"\"\"\n        Process incoming transcript and extract commands\n        \"\"\"\n        transcript = msg.data.lower().strip()\n        if not transcript:\n            return\n\n        self.get_logger().info(f'Received transcript: {transcript}')\n\n        # Parse the transcript for commands\n        parsed_commands = self.parse_transcript(transcript)\n\n        if parsed_commands:\n            for command in parsed_commands:\n                self.get_logger().info(f'Parsed command: {command}')\n\n                # Publish command\n                cmd_msg = String()\n                cmd_msg.data = json.dumps(command)\n                self.command_pub.publish(cmd_msg)\n\n                # Execute command if possible\n                self.execute_command(command)\n\n    def parse_transcript(self, transcript: str) -> List[Dict]:\n        \"\"\"\n        Parse transcript to extract commands using regex patterns\n        \"\"\"\n        commands = []\n\n        for command_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, transcript, re.IGNORECASE)\n                if match:\n                    command = {\n                        'type': command_type,\n                        'original_text': transcript,\n                        'matched_pattern': pattern,\n                        'confidence': 0.9,  # High confidence for pattern matches\n                        **match.groupdict()  # Add captured groups as parameters\n                    }\n                    commands.append(command)\n\n        return commands\n\n    def execute_command(self, command: Dict):\n        \"\"\"\n        Execute the parsed command\n        \"\"\"\n        cmd_type = command['type']\n\n        if cmd_type == 'move_to':\n            self.execute_move_to(command)\n        elif cmd_type == 'come_here':\n            self.execute_come_here(command)\n        elif cmd_type == 'stop':\n            self.execute_stop(command)\n        elif cmd_type == 'turn':\n            self.execute_turn(command)\n        elif cmd_type == 'pick_up':\n            self.execute_pick_up(command)\n        elif cmd_type == 'put_down':\n            self.execute_put_down(command)\n        elif cmd_type == 'follow_me':\n            self.execute_follow_me(command)\n        elif cmd_type == 'wait_here':\n            self.execute_wait_here(command)\n        else:\n            self.get_logger().warn(f'Unknown command type: {cmd_type}')\n\n    def execute_move_to(self, command: Dict):\n        \"\"\"\n        Execute move to location command\n        \"\"\"\n        location = command.get('location', '').lower()\n\n        if location in self.location_map:\n            location_data = self.location_map[location]\n\n            goal = PoseStamped()\n            goal.header.stamp = self.get_clock().now().to_msg()\n            goal.header.frame_id = 'map'\n            goal.pose.position.x = location_data['x']\n            goal.pose.position.y = location_data['y']\n            goal.pose.position.z = 0.0  # Ground level\n\n            # Simple orientation based on theta\n            import math\n            goal.pose.orientation.z = math.sin(location_data['theta'] / 2)\n            goal.pose.orientation.w = math.cos(location_data['theta'] / 2)\n\n            self.navigation_goal_pub.publish(goal)\n            self.get_logger().info(f'Moving to {location} at ({location_data[\"x\"]}, {location_data[\"y\"]})')\n        else:\n            self.get_logger().warn(f'Unknown location: {location}')\n            # Could implement location learning or ask for clarification\n\n    def execute_come_here(self, command: Dict):\n        \"\"\"\n        Execute come here command (would need to locate user)\n        \"\"\"\n        # In a real system, this would use person detection/localization\n        # For now, we'll just acknowledge the command\n        self.get_logger().info('Received \"come here\" command - would navigate to user location')\n\n    def execute_stop(self, command: Dict):\n        \"\"\"\n        Execute stop command\n        \"\"\"\n        # Publish stop command to navigation system\n        stop_msg = String()\n        stop_msg.data = 'stop'\n        self.command_pub.publish(stop_msg)\n        self.get_logger().info('Stopping robot')\n\n    def execute_turn(self, command: Dict):\n        \"\"\"\n        Execute turn command\n        \"\"\"\n        # Extract turn direction from original text\n        text = command['original_text']\n        if 'left' in text:\n            direction = 'left'\n        elif 'right' in text:\n            direction = 'right'\n        elif 'around' in text:\n            direction = 'around'\n        else:\n            direction = 'unknown'\n\n        self.get_logger().info(f'Turning {direction}')\n        # Would publish turn command to navigation system\n\n    def execute_pick_up(self, command: Dict):\n        \"\"\"\n        Execute pick up object command\n        \"\"\"\n        obj = command.get('object', '').lower()\n\n        if obj in self.object_locations:\n            obj_pos = self.object_locations[obj]\n            self.get_logger().info(f'Picking up {obj} at ({obj_pos[\"x\"]}, {obj_pos[\"y\"]}, {obj_pos[\"z\"]})')\n            # Would publish pick-up command to manipulation system\n        else:\n            self.get_logger().warn(f'Object not found: {obj}')\n            # Could use perception to locate the object\n\n    def execute_put_down(self, command: Dict):\n        \"\"\"\n        Execute put down object command\n        \"\"\"\n        obj = command.get('object', '').lower()\n        self.get_logger().info(f'Putting down {obj}')\n        # Would publish put-down command to manipulation system\n\n    def execute_follow_me(self, command: Dict):\n        \"\"\"\n        Execute follow me command\n        \"\"\"\n        self.get_logger().info('Following user - would activate follow mode')\n        # Would activate person following behavior\n\n    def execute_wait_here(self, command: Dict):\n        \"\"\"\n        Execute wait here command\n        \"\"\"\n        self.get_logger().info('Waiting at current location')\n        # Would stop navigation and wait\n\ndef main(args=None):\n    rclpy.init(args=args)\n    command_recognizer = CustomVoiceCommandRecognizer()\n\n    try:\n        rclpy.spin(command_recognizer)\n    except KeyboardInterrupt:\n        command_recognizer.get_logger().info('Shutting down voice command recognizer')\n    finally:\n        command_recognizer.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-processing-optimization",children:"Real-time Processing Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"1-multi-threaded-audio-processing",children:"1. Multi-threaded Audio Processing"}),"\n",(0,t.jsx)(n.p,{children:"Implementing efficient multi-threaded audio processing for real-time performance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# multithreaded_audio_processing.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import AudioData\nfrom std_msgs.msg import String\nfrom threading import Thread, Lock, Event\nfrom queue import Queue, Empty\nimport time\nimport numpy as np\n\nclass MultiThreadedAudioProcessor(Node):\n    """\n    Multi-threaded audio processing for real-time VLA systems\n    """\n    def __init__(self):\n        super().__init__(\'multithreaded_audio_processor\')\n\n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'/audio/raw\',\n            self.audio_callback,\n            10\n        )\n\n        # Publishers\n        self.transcript_pub = self.create_publisher(\n            String,\n            \'/vad/transcript\',\n            10\n        )\n\n        # Audio processing queues\n        self.raw_audio_queue = Queue(maxsize=10)\n        self.processed_audio_queue = Queue(maxsize=5)\n\n        # Processing threads\n        self.preprocessing_thread = Thread(target=self.preprocessing_loop)\n        self.speech_recognition_thread = Thread(target=self.speech_recognition_loop)\n        self.postprocessing_thread = Thread(target=self.postprocessing_loop)\n\n        # Control events\n        self.shutdown_event = Event()\n\n        # Processing locks\n        self.audio_lock = Lock()\n\n        # Start threads\n        self.preprocessing_thread.start()\n        self.speech_recognition_thread.start()\n        self.postprocessing_thread.start()\n\n        self.get_logger().info(\'Multi-threaded Audio Processor initialized\')\n\n    def audio_callback(self, msg):\n        """\n        Receive raw audio data\n        """\n        try:\n            if not self.raw_audio_queue.full():\n                self.raw_audio_queue.put(msg, timeout=0.01)\n        except:\n            # Drop frame if queue is full\n            pass\n\n    def preprocessing_loop(self):\n        """\n        Preprocessing thread: noise reduction, VAD, etc.\n        """\n        while not self.shutdown_event.is_set():\n            try:\n                # Get raw audio\n                raw_msg = self.raw_audio_queue.get(timeout=0.1)\n\n                # Convert to numpy\n                audio_data = np.frombuffer(raw_msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n                # Apply preprocessing (noise reduction, normalization, etc.)\n                processed_audio = self.apply_preprocessing(audio_data)\n\n                # Check if audio contains speech\n                if self.is_speech(processed_audio):\n                    # Add to processing queue\n                    processed_item = {\n                        \'data\': processed_audio,\n                        \'timestamp\': raw_msg.header.stamp if hasattr(raw_msg, \'header\') else time.time()\n                    }\n\n                    if not self.processed_audio_queue.full():\n                        self.processed_audio_queue.put(processed_item)\n                    else:\n                        self.get_logger().warn(\'Processed audio queue full, dropping frame\')\n\n            except Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Preprocessing error: {e}\')\n\n    def speech_recognition_loop(self):\n        """\n        Speech recognition thread: ASR processing\n        """\n        # Initialize ASR model (simplified - would use Whisper or similar)\n        asr_model = self.initialize_asr_model()\n\n        while not self.shutdown_event.is_set():\n            try:\n                # Get processed audio\n                processed_item = self.processed_audio_queue.get(timeout=0.1)\n\n                # Perform speech recognition\n                if asr_model:\n                    transcript = self.perform_asr(\n                        asr_model,\n                        processed_item[\'data\']\n                    )\n\n                    if transcript:\n                        # Publish transcript\n                        transcript_msg = String()\n                        transcript_msg.data = transcript\n                        self.transcript_pub.publish(transcript_msg)\n\n            except Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Speech recognition error: {e}\')\n\n    def postprocessing_loop(self):\n        """\n        Postprocessing thread: transcript validation, NLU, etc.\n        """\n        while not self.shutdown_event.is_set():\n            # In a real system, this would handle transcript validation,\n            # natural language understanding, and command generation\n            time.sleep(0.1)  # Placeholder\n\n    def apply_preprocessing(self, audio_data):\n        """\n        Apply audio preprocessing (noise reduction, normalization, etc.)\n        """\n        # Normalize audio\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            audio_data = audio_data / max_val * 0.8  # 80% normalization to prevent clipping\n\n        # Apply simple noise reduction\n        # (In practice, use more sophisticated methods)\n        noise_floor = np.std(audio_data) * 0.1\n        audio_data = np.clip(audio_data, -noise_floor, noise_floor) + audio_data\n\n        return audio_data\n\n    def is_speech(self, audio_data):\n        """\n        Simple voice activity detection\n        """\n        # Calculate energy\n        energy = np.mean(np.abs(audio_data))\n\n        # Define threshold (would be adaptive in real system)\n        threshold = 0.01\n\n        return energy > threshold\n\n    def initialize_asr_model(self):\n        """\n        Initialize ASR model (placeholder)\n        """\n        # In real implementation, load Whisper or similar model\n        return True  # Placeholder\n\n    def perform_asr(self, model, audio_data):\n        """\n        Perform ASR on audio data (placeholder)\n        """\n        # In real implementation, use actual ASR model\n        # This is a simplified simulation\n        if len(audio_data) > 1000:  # Only process substantial audio\n            # Simulate some processing time\n            time.sleep(0.05)\n            # Return placeholder transcript\n            return "simulated transcript from audio"\n        return None\n\n    def destroy_node(self):\n        """\n        Clean shutdown of processing threads\n        """\n        self.shutdown_event.set()\n\n        if self.preprocessing_thread.is_alive():\n            self.preprocessing_thread.join()\n        if self.speech_recognition_thread.is_alive():\n            self.speech_recognition_thread.join()\n        if self.postprocessing_thread.is_alive():\n            self.postprocessing_thread.join()\n\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = MultiThreadedAudioProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info(\'Shutting down multi-threaded audio processor\')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"voice-command-validation-and-safety",children:"Voice Command Validation and Safety"}),"\n",(0,t.jsx)(n.h3,{id:"1-safe-voice-command-processing",children:"1. Safe Voice Command Processing"}),"\n",(0,t.jsx)(n.p,{children:"Implementing safety checks for voice commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# voice_command_safety.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom std_srvs.srv import Trigger\nfrom typing import Dict, Any\nimport json\nimport re\n\nclass VoiceCommandSafetyValidator(Node):\n    \"\"\"\n    Safety validation for voice commands in humanoid robots\n    \"\"\"\n    def __init__(self):\n        super().__init__('voice_command_safety_validator')\n\n        # Subscribe to raw voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/voice_command/parsed',\n            self.command_callback,\n            10\n        )\n\n        # Publishers for validated commands\n        self.validated_command_pub = self.create_publisher(\n            String,\n            '/voice_command/validated',\n            10\n        )\n\n        self.safety_alert_pub = self.create_publisher(\n            String,\n            '/safety/alert',\n            10\n        )\n\n        # Service for emergency stop\n        self.emergency_stop_service = self.create_service(\n            Trigger,\n            '/voice_command/emergency_stop',\n            self.emergency_stop_callback\n        )\n\n        # Safety configuration\n        self.safety_config = {\n            'max_navigation_distance': 10.0,  # meters\n            'forbidden_locations': ['restricted_area', 'danger_zone'],\n            'forbidden_actions': ['jump', 'run_fast', 'harm'],\n            'safe_speed_limits': {\n                'linear': 0.5,  # m/s\n                'angular': 0.5  # rad/s\n            },\n            'timeout_duration': 30.0  # seconds\n        }\n\n        # User authorization (simplified - would use proper auth system)\n        self.authorized_users = ['default_user']\n        self.current_user = 'default_user'\n\n        # Command history for context\n        self.command_history = []\n\n        self.get_logger().info('Voice Command Safety Validator initialized')\n\n    def command_callback(self, msg):\n        \"\"\"\n        Process incoming voice command with safety validation\n        \"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            self.get_logger().info(f'Received command: {command_data}')\n\n            # Validate command\n            is_safe, reason = self.validate_command(command_data)\n\n            if is_safe:\n                # Add to history\n                self.command_history.append({\n                    'command': command_data,\n                    'timestamp': self.get_clock().now().nanoseconds / 1e9\n                })\n\n                # Publish validated command\n                validated_msg = String()\n                validated_msg.data = msg.data\n                self.validated_command_pub.publish(validated_msg)\n\n                self.get_logger().info(f'Command validated and forwarded: {command_data[\"type\"]}')\n            else:\n                # Publish safety alert\n                alert_msg = String()\n                alert_msg.data = f'Safety violation: {reason}'\n                self.safety_alert_pub.publish(alert_msg)\n\n                self.get_logger().warn(f'Safety violation: {reason} for command {command_data}')\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in command message')\n        except Exception as e:\n            self.get_logger().error(f'Error processing command: {e}')\n\n    def validate_command(self, command: Dict[str, Any]) -> tuple[bool, str]:\n        \"\"\"\n        Validate command for safety\n        \"\"\"\n        cmd_type = command.get('type', '').lower()\n\n        # Check user authorization\n        if not self.is_user_authorized():\n            return False, \"User not authorized\"\n\n        # Validate based on command type\n        if cmd_type in ['move_to', 'navigate']:\n            return self.validate_navigation_command(command)\n        elif cmd_type in ['pick_up', 'grasp', 'manipulate']:\n            return self.validate_manipulation_command(command)\n        elif cmd_type in ['turn', 'rotate']:\n            return self.validate_rotation_command(command)\n        elif cmd_type in ['stop', 'wait_here']:\n            return True, \"Command is safe\"  # Stop commands are generally safe\n        else:\n            return self.validate_generic_command(command)\n\n    def validate_navigation_command(self, command: Dict[str, Any]) -> tuple[bool, str]:\n        \"\"\"\n        Validate navigation-related commands\n        \"\"\"\n        # Check if destination is forbidden\n        location = command.get('location', '').lower()\n        if location in self.safety_config['forbidden_locations']:\n            return False, f\"Destination '{location}' is forbidden\"\n\n        # Check distance constraints (would need robot position for real validation)\n        # This is a simplified check - real system would calculate actual distance\n        if 'x' in command and 'y' in command:\n            distance = (command['x']**2 + command['y']**2)**0.5\n            if distance > self.safety_config['max_navigation_distance']:\n                return False, f\"Destination too far: {distance:.2f}m > {self.safety_config['max_navigation_distance']}m\"\n\n        return True, \"Navigation command is safe\"\n\n    def validate_manipulation_command(self, command: Dict[str, Any]) -> tuple[bool, str]:\n        \"\"\"\n        Validate manipulation-related commands\n        \"\"\"\n        # Check if object is forbidden\n        obj = command.get('object', '').lower()\n        if obj in self.safety_config['forbidden_actions']:\n            return False, f\"Object '{obj}' manipulation is restricted\"\n\n        # Check if action is potentially dangerous\n        action = command.get('type', '').lower()\n        if any(dangerous in action for dangerous in ['harm', 'damage', 'break']):\n            return False, \"Manipulation command may cause harm\"\n\n        return True, \"Manipulation command is safe\"\n\n    def validate_rotation_command(self, command: Dict[str, Any]) -> tuple[bool, str]:\n        \"\"\"\n        Validate rotation-related commands\n        \"\"\"\n        # Check for excessive rotation\n        text = command.get('original_text', '').lower()\n\n        if 'spin' in text or 'fast' in text:\n            return False, \"Rotation command too aggressive\"\n\n        return True, \"Rotation command is safe\"\n\n    def validate_generic_command(self, command: Dict[str, Any]) -> tuple[bool, str]:\n        \"\"\"\n        Validate generic commands\n        \"\"\"\n        # Check for forbidden words in original text\n        original_text = command.get('original_text', '').lower()\n\n        for forbidden in self.safety_config['forbidden_actions']:\n            if forbidden in original_text:\n                return False, f\"Command contains forbidden action: {forbidden}\"\n\n        return True, \"Command is safe\"\n\n    def is_user_authorized(self) -> bool:\n        \"\"\"\n        Check if current user is authorized\n        \"\"\"\n        # In a real system, this would use proper authentication\n        # For now, we'll assume the default user is authorized\n        return self.current_user in self.authorized_users\n\n    def emergency_stop_callback(self, request, response):\n        \"\"\"\n        Handle emergency stop request\n        \"\"\"\n        self.get_logger().warn('EMERGENCY STOP ACTIVATED')\n\n        # Publish safety alert\n        alert_msg = String()\n        alert_msg.data = 'EMERGENCY STOP - All motion stopped'\n        self.safety_alert_pub.publish(alert_msg)\n\n        # In a real system, this would stop all robot motion\n        response.success = True\n        response.message = 'Emergency stop executed'\n        return response\n\n    def check_command_history_safety(self) -> tuple[bool, str]:\n        \"\"\"\n        Check command history for safety violations\n        \"\"\"\n        current_time = self.get_clock().now().nanoseconds / 1e9\n\n        # Check for command flooding (too many commands in short time)\n        recent_commands = [\n            cmd for cmd in self.command_history\n            if current_time - cmd['timestamp'] < 2.0  # Last 2 seconds\n        ]\n\n        if len(recent_commands) > 5:  # More than 5 commands in 2 seconds\n            return False, \"Command flooding detected\"\n\n        return True, \"Command history is safe\"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    safety_validator = VoiceCommandSafetyValidator()\n\n    try:\n        rclpy.spin(safety_validator)\n    except KeyboardInterrupt:\n        safety_validator.get_logger().info('Shutting down safety validator')\n    finally:\n        safety_validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"launch-files-for-voice-recognition-system",children:"Launch Files for Voice Recognition System"}),"\n",(0,t.jsx)(n.h3,{id:"1-complete-voice-recognition-launch",children:"1. Complete Voice Recognition Launch"}),"\n",(0,t.jsx)(n.p,{children:"Creating launch files for the complete voice recognition system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# launch/voice_recognition_system.launch.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, GroupAction, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node, SetParameter\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    robot_name = LaunchConfiguration('robot_name')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation time'\n    )\n\n    declare_robot_name = DeclareLaunchArgument(\n        'robot_name',\n        default_value='humanoid_robot',\n        description='Name of the robot'\n    )\n\n    # Audio capture node\n    audio_capture = Node(\n        package='humanoid_vla',\n        executable='audio_capture',\n        name='audio_capture',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Audio noise reducer\n    noise_reducer = Node(\n        package='humanoid_vla',\n        executable='audio_noise_reducer',\n        name='audio_noise_reducer',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Whisper speech recognizer\n    whisper_recognizer = Node(\n        package='humanoid_vla',\n        executable='whisper_speech_recognizer',\n        name='whisper_speech_recognizer',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Custom voice command recognizer\n    voice_command_recognizer = Node(\n        package='humanoid_vla',\n        executable='custom_voice_command_recognizer',\n        name='voice_command_recognizer',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Voice command safety validator\n    safety_validator = Node(\n        package='humanoid_vla',\n        executable='voice_command_safety_validator',\n        name='voice_command_safety_validator',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Multi-threaded audio processor\n    multithreaded_processor = Node(\n        package='humanoid_vla',\n        executable='multithreaded_audio_processor',\n        name='multithreaded_audio_processor',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Group all audio processing nodes\n    audio_processing_group = GroupAction(\n        actions=[\n            SetParameter('use_sim_time', use_sim_time),\n            audio_capture,\n            noise_reducer,\n            whisper_recognizer,\n            voice_command_recognizer,\n            safety_validator,\n            multithreaded_processor\n        ]\n    )\n\n    return LaunchDescription([\n        declare_use_sim_time,\n        declare_robot_name,\n\n        audio_processing_group,\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-and-real-time-considerations",children:"Performance Optimization and Real-time Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"1-real-time-audio-processing-constraints",children:"1. Real-time Audio Processing Constraints"}),"\n",(0,t.jsx)(n.p,{children:"Optimizing for real-time performance in voice recognition:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# real_time_constraints.md\n\n## Real-time Audio Processing Constraints for Humanoid Robots\n\n### 1. Processing Latency Requirements\n\n#### Audio Pipeline Latency Budget\n- **Audio Capture**: <5ms\n- **Preprocessing**: <10ms\n- **Voice Activity Detection**: <5ms\n- **Speech Recognition**: <100ms (for real-time interaction)\n- **Command Processing**: <20ms\n- **Total Pipeline**: <140ms (aim for <100ms)\n\n#### Critical Path Optimization\n- Use lock-free queues between processing stages\n- Implement audio ring buffers for continuous processing\n- Optimize FFT sizes for real-time performance (512-1024 samples)\n- Use fixed-point arithmetic where possible for embedded systems\n\n### 2. Computational Resource Management\n\n#### CPU Utilization\n- **Target**: <70% CPU usage for audio processing\n- **Strategy**: Use multi-threading for parallel processing\n- **Monitoring**: Implement CPU usage feedback control\n\n#### Memory Management\n- **Audio Buffers**: Pre-allocate all audio processing buffers\n- **Real-time Allocation**: Avoid dynamic allocation during processing\n- **Memory Pool**: Implement memory pools for audio frames\n\n#### GPU Acceleration\n- **Whisper Model**: Use GPU for speech recognition when available\n- **Batch Processing**: Process multiple audio chunks in parallel\n- **Precision**: Use FP16 for reduced memory usage and faster processing\n\n### 3. Power Consumption Optimization\n\n#### Adaptive Processing\n- **Low Power Mode**: Reduce processing rate when no speech detected\n- **Dynamic Frequency Scaling**: Adjust CPU/GPU frequency based on workload\n- **Sleep States**: Enter low-power states during silence periods\n\n### 4. Robustness and Error Handling\n\n#### Failure Modes\n- **Microphone Failure**: Fallback to alternative audio sources\n- **Network Disconnection**: Local processing capabilities\n- **Model Corruption**: Model integrity checking and reloading\n\n#### Error Recovery\n- **Watchdog Timers**: Monitor processing pipeline health\n- **Graceful Degradation**: Fallback to simpler processing when resources limited\n- **State Recovery**: Preserve command context during failures\n"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"With the voice recognition and processing system fully implemented, you're ready to move on to integrating large language models (LLMs) for cognitive planning. The next section will cover connecting LLMs to your robotic system, implementing prompt engineering for robotics tasks, and creating cognitive planning pipelines that convert natural language commands into executable robotic actions."}),"\n",(0,t.jsx)(n.p,{children:"The voice recognition system you've built provides the foundation for natural human-robot interaction that will be essential when implementing the higher-level cognitive functions in the following sections."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);