"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5468],{896:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-3-ai-robot-brain/practical-exercises/photorealistic-simulation","title":"Photorealistic Simulation and Synthetic Data Generation","description":"Overview","source":"@site/docs/module-3-ai-robot-brain/practical-exercises/photorealistic-simulation.md","sourceDirName":"module-3-ai-robot-brain/practical-exercises","slug":"/module-3-ai-robot-brain/practical-exercises/photorealistic-simulation","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/practical-exercises/photorealistic-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-3-ai-robot-brain/practical-exercises/photorealistic-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"VSLAM Implementation for Humanoid Robots","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/practical-exercises/vslam-implementation"},"next":{"title":"Module 3 Project: Complete AI-Powered Humanoid Navigation System","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/module-3-project"}}');var a=t(4848),o=t(8453);const r={sidebar_position:3},s="Photorealistic Simulation and Synthetic Data Generation",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Environment Design for Humanoid Robots",id:"environment-design-for-humanoid-robots",level:2},{value:"1. Realistic Indoor Environments",id:"1-realistic-indoor-environments",level:3},{value:"2. Dynamic Environment Features",id:"2-dynamic-environment-features",level:3},{value:"Synthetic Data Generation Pipeline",id:"synthetic-data-generation-pipeline",level:2},{value:"1. Camera Setup for Data Collection",id:"1-camera-setup-for-data-collection",level:3},{value:"2. Domain Randomization Techniques",id:"2-domain-randomization-techniques",level:3},{value:"Performance Optimization for Synthetic Data",id:"performance-optimization-for-synthetic-data",level:2},{value:"1. Efficient Rendering Settings",id:"1-efficient-rendering-settings",level:3},{value:"Quality Assurance for Synthetic Data",id:"quality-assurance-for-synthetic-data",level:2},{value:"1. Data Validation",id:"1-data-validation",level:3},{value:"Next Steps",id:"next-steps",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"photorealistic-simulation-and-synthetic-data-generation",children:"Photorealistic Simulation and Synthetic Data Generation"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Photorealistic simulation is crucial for developing robust AI systems for humanoid robots. NVIDIA Isaac Sim provides advanced rendering capabilities that enable the generation of synthetic data indistinguishable from real-world data. This section covers creating realistic environments, configuring lighting and materials, and implementing synthetic data generation pipelines for humanoid robotics applications."}),"\n",(0,a.jsx)(e.h2,{id:"environment-design-for-humanoid-robots",children:"Environment Design for Humanoid Robots"}),"\n",(0,a.jsx)(e.h3,{id:"1-realistic-indoor-environments",children:"1. Realistic Indoor Environments"}),"\n",(0,a.jsx)(e.p,{children:"Creating indoor environments that closely match real-world spaces where humanoid robots operate:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# indoor_environment.py - Create realistic indoor environments\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import create_prim\nfrom pxr import Gf, Sdf, UsdGeom, UsdLux, UsdShade\nimport numpy as np\n\ndef create_realistic_indoor_environment(world: World):\n    """\n    Create a realistic indoor environment for humanoid robot testing\n    """\n    stage = omni.usd.get_context().get_stage()\n\n    # Create main room structure\n    create_main_room(stage)\n\n    # Add furniture and objects\n    add_furniture(stage)\n\n    # Configure realistic lighting\n    configure_realistic_lighting(stage)\n\n    # Add textured surfaces\n    add_realistic_materials(stage)\n\n    print("Realistic indoor environment created")\n\ndef create_main_room(stage):\n    """\n    Create a main room with realistic dimensions and features\n    """\n    # Room dimensions (10m x 8m x 3m)\n    room_length = 10.0\n    room_width = 8.0\n    room_height = 3.0\n\n    # Floor\n    floor_path = Sdf.Path("/World/Floor")\n    floor_xform = UsdGeom.Xform.Define(stage, floor_path)\n    floor_geom = UsdGeom.Cube.Define(stage, floor_path.AppendChild("Geometry"))\n    floor_geom.CreateSizeAttr(1.0)\n    floor_xform.AddTranslateOp().Set((0, 0, -0.05))\n    floor_xform.AddScaleOp().Set((room_length, room_width, 0.1))\n\n    # Walls\n    wall_thickness = 0.2\n    wall_configs = [\n        {"position": (0, room_width/2, room_height/2), "scale": (room_length, wall_thickness, room_height)},\n        {"position": (0, -room_width/2, room_height/2), "scale": (room_length, wall_thickness, room_height)},\n        {"position": (room_length/2, 0, room_height/2), "scale": (wall_thickness, room_width, room_height)},\n        {"position": (-room_length/2, 0, room_height/2), "scale": (wall_thickness, room_width, room_height)},\n    ]\n\n    for i, config in enumerate(wall_configs):\n        wall_path = Sdf.Path(f"/World/Wall_{i}")\n        wall_xform = UsdGeom.Xform.Define(stage, wall_path)\n        wall_geom = UsdGeom.Cube.Define(stage, wall_path.AppendChild("Geometry"))\n        wall_geom.CreateSizeAttr(1.0)\n        wall_xform.AddTranslateOp().Set(config["position"])\n        wall_xform.AddScaleOp().Set(config["scale"])\n\n    # Ceiling\n    ceiling_path = Sdf.Path("/World/Ceiling")\n    ceiling_xform = UsdGeom.Xform.Define(stage, ceiling_path)\n    ceiling_geom = UsdGeom.Cube.Define(stage, ceiling_path.AppendChild("Geometry"))\n    ceiling_geom.CreateSizeAttr(1.0)\n    ceiling_xform.AddTranslateOp().Set((0, 0, room_height))\n    ceiling_xform.AddScaleOp().Set((room_length, room_width, 0.1))\n\ndef add_furniture(stage):\n    """\n    Add realistic furniture for humanoid interaction\n    """\n    furniture_configs = [\n        # Dining table\n        {\n            "name": "DiningTable",\n            "type": "cube",\n            "position": (2, -2, 0.75),\n            "scale": (1.5, 0.8, 0.8),\n            "color": (0.8, 0.6, 0.4)  # Wood color\n        },\n        # Chair\n        {\n            "name": "Chair",\n            "type": "cube",\n            "position": (2, -3.2, 0.45),\n            "scale": (0.5, 0.5, 0.9),\n            "color": (0.3, 0.3, 0.3)  # Dark color\n        },\n        # Sofa\n        {\n            "name": "Sofa",\n            "type": "cube",\n            "position": (-3, 2, 0.4),\n            "scale": (2.0, 0.8, 0.8),\n            "color": (0.2, 0.2, 0.6)  # Blue color\n        },\n        # Coffee table\n        {\n            "name": "CoffeeTable",\n            "type": "cube",\n            "position": (-1, 0, 0.45),\n            "scale": (0.8, 0.8, 0.9),\n            "color": (0.7, 0.5, 0.3)  # Wood color\n        },\n        # Kitchen counter\n        {\n            "name": "KitchenCounter",\n            "type": "cube",\n            "position": (4, 3, 0.9),\n            "scale": (2.0, 0.6, 0.9),\n            "color": (0.9, 0.9, 0.9)  # White color\n        }\n    ]\n\n    for i, config in enumerate(furniture_configs):\n        furn_path = Sdf.Path(f"/World/Furniture_{config[\'name\']}_{i}")\n        furn_xform = UsdGeom.Xform.Define(stage, furn_path)\n        furn_geom = UsdGeom.Cube.Define(stage, furn_path.AppendChild("Geometry"))\n        furn_geom.CreateSizeAttr(1.0)\n        furn_xform.AddTranslateOp().Set(config["position"])\n        furn_xform.AddScaleOp().Set(config["scale"])\n\ndef configure_realistic_lighting(stage):\n    """\n    Configure realistic indoor lighting with multiple light sources\n    """\n    # Main ceiling light (over dining table area)\n    main_light = UsdLux.DistantLight.Define(stage, Sdf.Path("/World/MainLight"))\n    main_light.CreateIntensityAttr(3000)\n    main_light.CreateColorAttr(Gf.Vec3f(1.0, 0.98, 0.94))  # Warm white\n    main_light.AddRotateXYZOp().Set((60, 0, 0))  # Angle from ceiling\n\n    # Ambient light to fill shadows\n    ambient_light = UsdLux.DomeLight.Define(stage, Sdf.Path("/World/AmbientLight"))\n    ambient_light.CreateIntensityAttr(200)\n    ambient_light.CreateColorAttr(Gf.Vec3f(0.9, 0.95, 1.0))  # Cool white ambient\n\n    # Task lighting (over kitchen counter)\n    task_light = UsdLux.SphereLight.Define(stage, Sdf.Path("/World/TaskLight"))\n    task_light.CreateIntensityAttr(1000)\n    task_light.CreateColorAttr(Gf.Vec3f(1.0, 1.0, 0.98))\n    task_light.AddTranslateOp().Set((4, 3, 2.0))\n\n    # Window light simulation (if needed)\n    window_light = UsdLux.DistantLight.Define(stage, Sdf.Path("/World/WindowLight"))\n    window_light.CreateIntensityAttr(1000)\n    window_light.CreateColorAttr(Gf.Vec3f(0.95, 1.0, 1.0))\n    window_light.AddRotateXYZOp().Set((30, 45, 0))\n\ndef add_realistic_materials(stage):\n    """\n    Add realistic materials to surfaces for better rendering\n    """\n    # Create material paths\n    material_configs = [\n        {"name": "FloorMaterial", "color": (0.7, 0.7, 0.7), "roughness": 0.8},\n        {"name": "WallMaterial", "color": (0.95, 0.95, 0.95), "roughness": 0.9},\n        {"name": "WoodMaterial", "color": (0.8, 0.6, 0.4), "roughness": 0.3},\n        {"name": "MetalMaterial", "color": (0.7, 0.7, 0.8), "roughness": 0.1},\n    ]\n\n    for config in material_configs:\n        material_path = Sdf.Path(f"/World/Materials/{config[\'name\']}")\n        material = UsdShade.Material.Define(stage, material_path)\n\n        # Create PBR shader\n        shader = UsdShade.Shader.Define(stage, material_path.AppendChild("Shader"))\n        shader.CreateIdAttr("OmniPBR")\n\n        # Set material properties\n        shader.CreateInput("diffuse_color", Sdf.ValueTypeNames.Color3f).Set(\n            Gf.Vec3f(*config["color"])\n        )\n        shader.CreateInput("roughness", Sdf.ValueTypeNames.Float).Set(\n            config["roughness"]\n        )\n\n        # Connect shader to material\n        material.CreateSurfaceOutput().ConnectToSource(shader.ConnectableAPI(), "out")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-dynamic-environment-features",children:"2. Dynamic Environment Features"}),"\n",(0,a.jsx)(e.p,{children:"Creating environments that can change for varied training data:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# dynamic_environment.py - Create environments with dynamic elements\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom pxr import Gf, Sdf, UsdGeom\nimport numpy as np\nimport random\n\nclass DynamicEnvironmentManager:\n    """\n    Manager for dynamic elements in simulation environments\n    """\n    def __init__(self, world: World):\n        self.world = world\n        self.stage = omni.usd.get_context().get_stage()\n        self.dynamic_objects = []\n        self.lighting_conditions = []\n\n    def add_dynamic_objects(self):\n        """\n        Add objects that can be moved/changed during simulation\n        """\n        # Random objects that can be placed differently each time\n        object_types = [\n            {"name": "Box", "size": (0.2, 0.2, 0.2), "color": (1.0, 0.0, 0.0)},\n            {"name": "Sphere", "size": (0.15, 0.15, 0.15), "color": (0.0, 1.0, 0.0)},\n            {"name": "Cylinder", "size": (0.1, 0.1, 0.3), "color": (0.0, 0.0, 1.0)},\n        ]\n\n        # Place 5-10 random objects in the environment\n        num_objects = random.randint(5, 10)\n        for i in range(num_objects):\n            obj_config = random.choice(object_types)\n            position = (\n                random.uniform(-4, 4),  # X position\n                random.uniform(-3, 3),  # Y position\n                obj_config["size"][2] / 2 + 0.01  # Z position (on floor)\n            )\n\n            obj_path = Sdf.Path(f"/World/DynamicObject_{i}")\n\n            if obj_config["name"] == "Sphere":\n                geom = UsdGeom.Sphere.Define(self.stage, obj_path)\n                geom.CreateRadiusAttr(obj_config["size"][0])\n            elif obj_config["name"] == "Cylinder":\n                geom = UsdGeom.Cylinder.Define(self.stage, obj_path)\n                geom.CreateRadiusAttr(obj_config["size"][0])\n                geom.CreateHeightAttr(obj_config["size"][2])\n            else:  # Box\n                geom = UsdGeom.Cube.Define(self.stage, obj_path)\n                geom.CreateSizeAttr(1.0)\n                xform = UsdGeom.Xformable(geom.GetPrim())\n                xform.AddScaleOp().Set(obj_config["size"])\n\n            # Position the object\n            xform = UsdGeom.Xformable(geom.GetPrim())\n            xform.AddTranslateOp().Set(position)\n\n            self.dynamic_objects.append({\n                "path": obj_path,\n                "type": obj_config["name"],\n                "original_position": position\n            })\n\n    def change_lighting_conditions(self):\n        """\n        Randomly change lighting conditions for domain randomization\n        """\n        # Get existing lights\n        main_light = self.stage.GetPrimAtPath("/World/MainLight")\n        ambient_light = self.stage.GetPrimAtPath("/World/AmbientLight")\n        task_light = self.stage.GetPrimAtPath("/World/TaskLight")\n\n        if main_light and main_light.IsValid():\n            # Randomize main light intensity and color\n            intensity = random.uniform(2000, 5000)\n            color_var = random.uniform(-0.1, 0.1)\n            color = Gf.Vec3f(\n                max(0.5, min(1.0, 1.0 + color_var)),\n                max(0.5, min(1.0, 0.98 + color_var * 0.5)),\n                max(0.5, min(1.0, 0.94 + color_var * 0.5))\n            )\n\n            main_light.GetAttribute("inputs:intensity").Set(intensity)\n            main_light.GetAttribute("inputs:color").Set(color)\n\n        if ambient_light and ambient_light.IsValid():\n            # Randomize ambient light\n            intensity = random.uniform(100, 500)\n            ambient_light.GetAttribute("inputs:intensity").Set(intensity)\n\n    def randomize_environment(self):\n        """\n        Randomize the environment for synthetic data generation\n        """\n        # Move dynamic objects to new positions\n        for obj in self.dynamic_objects:\n            new_position = (\n                random.uniform(-4, 4),\n                random.uniform(-3, 3),\n                obj["original_position"][2]  # Keep same height\n            )\n\n            geom = UsdGeom.Xformable(self.stage.GetPrimAtPath(obj["path"]))\n            geom.AddTranslateOp().Set(new_position)\n\n        # Change lighting conditions\n        self.change_lighting_conditions()\n\n        print(f"Environment randomized with {len(self.dynamic_objects)} objects moved")\n\n    def reset_environment(self):\n        """\n        Reset environment to original state\n        """\n        for obj in self.dynamic_objects:\n            geom = UsdGeom.Xformable(self.stage.GetPrimAtPath(obj["path"]))\n            geom.AddTranslateOp().Set(obj["original_position"])\n\n        print("Environment reset to original state")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"synthetic-data-generation-pipeline",children:"Synthetic Data Generation Pipeline"}),"\n",(0,a.jsx)(e.h3,{id:"1-camera-setup-for-data-collection",children:"1. Camera Setup for Data Collection"}),"\n",(0,a.jsx)(e.p,{children:"Configuring cameras for synthetic data generation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# camera_setup.py - Set up cameras for synthetic data collection\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_prim\nfrom omni.isaac.sensor import Camera\nfrom pxr import Gf, Sdf\nimport numpy as np\n\ndef setup_data_collection_cameras(world: World):\n    """\n    Set up multiple cameras for comprehensive data collection\n    """\n    # Head-mounted camera (front-facing)\n    head_camera = Camera(\n        prim_path="/World/HumanoidRobot/Head/Camera",\n        frequency=30,  # 30 Hz\n        resolution=(640, 480)\n    )\n\n    # Chest-mounted camera (wide-angle)\n    chest_camera = Camera(\n        prim_path="/World/HumanoidRobot/Chest/Camera",\n        frequency=30,\n        resolution=(800, 600),\n        viewport_position=(0, 0, 0.1),  # Slightly offset\n        clipping_range=(0.1, 10.0)\n    )\n\n    # Eye-level camera (for human interaction)\n    eye_camera = Camera(\n        prim_path="/World/HumanoidRobot/Head/EyeCamera",\n        frequency=60,  # Higher frequency for interaction\n        resolution=(1280, 720)\n    )\n\n    # Add cameras to world\n    world.scene.add(head_camera)\n    world.scene.add(chest_camera)\n    world.scene.add(eye_camera)\n\n    return head_camera, chest_camera, eye_camera\n\ndef capture_synthetic_data(world: World, cameras, data_dir: str, num_samples: int = 1000):\n    """\n    Capture synthetic data from multiple cameras\n    """\n    import os\n    import cv2\n    from datetime import datetime\n\n    # Create data directory\n    os.makedirs(data_dir, exist_ok=True)\n    os.makedirs(f"{data_dir}/rgb", exist_ok=True)\n    os.makedirs(f"{data_dir}/depth", exist_ok=True)\n    os.makedirs(f"{data_dir}/segmentation", exist_ok=True)\n\n    env_manager = DynamicEnvironmentManager(world)\n\n    for i in range(num_samples):\n        # Randomize environment\n        env_manager.randomize_environment()\n\n        # Step the simulation\n        world.step(render=True)\n\n        # Capture data from each camera\n        for j, camera in enumerate(cameras):\n            # Get RGB data\n            rgb_data = camera.get_rgb()\n            if rgb_data is not None:\n                rgb_image = cv2.cvtColor(rgb_data, cv2.COLOR_RGBA2BGR)\n                cv2.imwrite(f"{data_dir}/rgb/sample_{i:04d}_cam_{j}.png", rgb_image)\n\n            # Get depth data\n            depth_data = camera.get_depth()\n            if depth_data is not None:\n                # Normalize depth for visualization\n                depth_normalized = ((depth_data - depth_data.min()) /\n                                  (depth_data.max() - depth_data.min()) * 255).astype(np.uint8)\n                cv2.imwrite(f"{data_dir}/depth/sample_{i:04d}_cam_{j}.png", depth_normalized)\n\n            # Get segmentation data (if available)\n            seg_data = camera.get_semantic_segmentation()\n            if seg_data is not None:\n                cv2.imwrite(f"{data_dir}/segmentation/sample_{i:04d}_cam_{j}.png", seg_data.astype(np.uint8))\n\n        if i % 100 == 0:\n            print(f"Captured {i}/{num_samples} synthetic data samples")\n\n    print(f"Synthetic data collection completed. {num_samples} samples saved to {data_dir}")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-domain-randomization-techniques",children:"2. Domain Randomization Techniques"}),"\n",(0,a.jsx)(e.p,{children:"Implementing domain randomization for robust perception:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# domain_randomization.py - Implement domain randomization for synthetic data\nimport random\nimport numpy as np\nfrom pxr import Gf, Sdf, UsdShade\nimport omni\nfrom omni.isaac.core.utils.prims import get_prim_at_path\n\nclass DomainRandomizer:\n    """\n    Class to implement domain randomization techniques\n    """\n    def __init__(self, stage):\n        self.stage = stage\n        self.materials = []\n        self.lights = []\n\n    def randomize_textures(self):\n        """\n        Randomize textures and materials in the environment\n        """\n        # Randomize floor texture\n        floor_material = self.stage.GetPrimAtPath("/World/Materials/FloorMaterial")\n        if floor_material and floor_material.IsValid():\n            # Randomize color slightly\n            base_color = Gf.Vec3f(\n                random.uniform(0.5, 0.9),\n                random.uniform(0.5, 0.9),\n                random.uniform(0.5, 0.9)\n            )\n            floor_material.GetAttribute("inputs:diffuse_color").Set(base_color)\n\n            # Randomize roughness\n            roughness = random.uniform(0.5, 1.0)\n            floor_material.GetAttribute("inputs:roughness").Set(roughness)\n\n        # Randomize wall texture\n        wall_material = self.stage.GetPrimAtPath("/World/Materials/WallMaterial")\n        if wall_material and wall_material.IsValid():\n            wall_color = Gf.Vec3f(\n                random.uniform(0.8, 1.0),\n                random.uniform(0.8, 1.0),\n                random.uniform(0.8, 1.0)\n            )\n            wall_material.GetAttribute("inputs:diffuse_color").Set(wall_color)\n\n    def randomize_lighting(self):\n        """\n        Randomize lighting conditions\n        """\n        # Get all lights in the scene\n        lights = ["/World/MainLight", "/World/AmbientLight", "/World/TaskLight", "/World/WindowLight"]\n\n        for light_path in lights:\n            light_prim = self.stage.GetPrimAtPath(light_path)\n            if light_prim and light_prim.IsValid():\n                # Randomize intensity\n                base_intensity = light_prim.GetAttribute("inputs:intensity").Get()\n                if base_intensity:\n                    randomized_intensity = base_intensity * random.uniform(0.5, 2.0)\n                    light_prim.GetAttribute("inputs:intensity").Set(randomized_intensity)\n\n                # Randomize color temperature slightly\n                base_color = light_prim.GetAttribute("inputs:color").Get()\n                if base_color:\n                    color_var = random.uniform(-0.1, 0.1)\n                    new_color = Gf.Vec3f(\n                        max(0.1, min(1.0, base_color[0] + color_var)),\n                        max(0.1, min(1.0, base_color[1] + color_var * 0.5)),\n                        max(0.1, min(1.0, base_color[2] + color_var * 0.5))\n                    )\n                    light_prim.GetAttribute("inputs:color").Set(new_color)\n\n    def randomize_object_appearances(self):\n        """\n        Randomize appearances of objects in the scene\n        """\n        # Find all dynamic objects and randomize their materials\n        for i in range(20):  # Check first 20 dynamic objects\n            obj_path = f"/World/DynamicObject_{i}"\n            obj_prim = self.stage.GetPrimAtPath(obj_path)\n            if obj_prim and obj_prim.IsValid():\n                # Create or modify material for this object\n                material_path = f"/World/Materials/DynamicObject_{i}_Material"\n                material = UsdShade.Material.Define(self.stage, Sdf.Path(material_path))\n\n                shader = UsdShade.Shader.Define(self.stage, Sdf.Path(f"{material_path}/Shader"))\n                shader.CreateIdAttr("OmniPBR")\n\n                # Randomize color\n                color = Gf.Vec3f(\n                    random.uniform(0.2, 1.0),\n                    random.uniform(0.2, 1.0),\n                    random.uniform(0.2, 1.0)\n                )\n                shader.CreateInput("diffuse_color", Sdf.ValueTypeNames.Color3f).Set(color)\n\n                # Randomize material properties\n                roughness = random.uniform(0.1, 0.9)\n                shader.CreateInput("roughness", Sdf.ValueTypeNames.Float).Set(roughness)\n\n                metallic = random.uniform(0.0, 0.3)\n                shader.CreateInput("metallic", Sdf.ValueTypeNames.Float).Set(metallic)\n\n                # Connect shader to material\n                material.CreateSurfaceOutput().ConnectToSource(shader.ConnectableAPI(), "out")\n\n    def apply_domain_randomization(self):\n        """\n        Apply all domain randomization techniques\n        """\n        self.randomize_textures()\n        self.randomize_lighting()\n        self.randomize_object_appearances()\n\n        print("Domain randomization applied to environment")\n\n# Integration with environment manager\nclass EnhancedDynamicEnvironmentManager(DynamicEnvironmentManager):\n    """\n    Enhanced environment manager with domain randomization\n    """\n    def __init__(self, world: World):\n        super().__init__(world)\n        self.domain_randomizer = DomainRandomizer(self.stage)\n\n    def randomize_environment(self):\n        """\n        Randomize the environment with domain randomization\n        """\n        # Move dynamic objects\n        super().randomize_environment()\n\n        # Apply domain randomization\n        self.domain_randomizer.apply_domain_randomization()\n\n        print("Environment randomized with domain randomization")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-optimization-for-synthetic-data",children:"Performance Optimization for Synthetic Data"}),"\n",(0,a.jsx)(e.h3,{id:"1-efficient-rendering-settings",children:"1. Efficient Rendering Settings"}),"\n",(0,a.jsx)(e.p,{children:"Optimizing Isaac Sim for fast synthetic data generation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# performance_optimization.py - Optimize rendering for synthetic data generation\nimport omni\nfrom omni import ui\nfrom omni.isaac.core.utils.settings import set_carb_setting\n\ndef optimize_rendering_for_synthetic_data():\n    """\n    Optimize Isaac Sim settings for efficient synthetic data generation\n    """\n    # Set rendering quality for performance\n    set_carb_setting(omni.appwindow.get_default_app_window().get_settings(),\n                     "/app/renderer/enabled", True)\n\n    # Reduce anti-aliasing for faster rendering\n    set_carb_setting(omni.appwindow.get_default_app_window().get_settings(),\n                     "/rtx/aa/op", 0)  # No anti-aliasing for synthetic data\n\n    # Disable post-processing effects that aren\'t needed for synthetic data\n    set_carb_setting(omni.appwindow.get_default_app_window().get_settings(),\n                     "/rtx/post/dlss/enable", False)\n    set_carb_setting(omni.appwindow.get_default_app_window().get_settings(),\n                     "/rtx/post/fx/enable", False)\n\n    # Optimize for synthetic data (reduce quality settings that don\'t affect ML)\n    set_carb_setting(omni.appwindow.get_default_app_window().get_settings(),\n                     "/renderer/quality", 0)  # Lowest quality setting\n\n    print("Rendering optimized for synthetic data generation")\n\ndef configure_camera_for_performance(cameras):\n    """\n    Configure cameras for optimal performance during data collection\n    """\n    for camera in cameras:\n        # Reduce unnecessary processing\n        camera.set_fov(60)  # Standard field of view\n\n        # Optimize for the specific task\n        if "depth" in camera.name:\n            # Ensure depth camera has appropriate settings\n            camera.set_resolution((640, 480))  # Lower resolution for performance\n        else:\n            # RGB cameras can use higher resolution if needed\n            camera.set_resolution((640, 480))\n\ndef batch_synthetic_data_collection(world: World, cameras, data_dir: str,\n                                  num_batches: int = 10, samples_per_batch: int = 100):\n    """\n    Collect synthetic data in batches for better performance\n    """\n    import os\n    import cv2\n    import numpy as np\n\n    os.makedirs(data_dir, exist_ok=True)\n\n    env_manager = EnhancedDynamicEnvironmentManager(world)\n\n    for batch in range(num_batches):\n        batch_dir = f"{data_dir}/batch_{batch:03d}"\n        os.makedirs(batch_dir, exist_ok=True)\n        os.makedirs(f"{batch_dir}/rgb", exist_ok=True)\n        os.makedirs(f"{batch_dir}/depth", exist_ok=True)\n\n        for i in range(samples_per_batch):\n            # Randomize environment with domain randomization\n            env_manager.randomize_environment()\n\n            # Step simulation\n            world.step(render=True)\n\n            # Capture data from all cameras\n            for cam_idx, camera in enumerate(cameras):\n                # Get RGB data\n                rgb_data = camera.get_rgb()\n                if rgb_data is not None:\n                    rgb_image = cv2.cvtColor(rgb_data, cv2.COLOR_RGBA2BGR)\n                    cv2.imwrite(f"{batch_dir}/rgb/sample_{batch:03d}_{i:04d}_cam_{cam_idx}.png", rgb_image)\n\n                # Get depth data\n                depth_data = camera.get_depth()\n                if depth_data is not None:\n                    depth_normalized = ((depth_data - depth_data.min()) /\n                                      (depth_data.max() - depth_data.min()) * 255).astype(np.uint8)\n                    cv2.imwrite(f"{batch_dir}/depth/sample_{batch:03d}_{i:04d}_cam_{cam_idx}.png", depth_normalized)\n\n        print(f"Completed batch {batch + 1}/{num_batches}")\n\n    print(f"Synthetic data collection completed: {num_batches * samples_per_batch} samples")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"quality-assurance-for-synthetic-data",children:"Quality Assurance for Synthetic Data"}),"\n",(0,a.jsx)(e.h3,{id:"1-data-validation",children:"1. Data Validation"}),"\n",(0,a.jsx)(e.p,{children:"Validating the quality of synthetic data:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# data_validation.py - Validate synthetic data quality\nimport cv2\nimport numpy as np\nimport os\nfrom typing import List, Tuple\n\ndef validate_synthetic_data(data_dir: str) -> dict:\n    """\n    Validate the quality and completeness of synthetic data\n    """\n    validation_results = {\n        "total_samples": 0,\n        "valid_rgb_samples": 0,\n        "valid_depth_samples": 0,\n        "rgb_quality_issues": [],\n        "depth_quality_issues": [],\n        "completeness_score": 0.0\n    }\n\n    # Check RGB data\n    rgb_dir = f"{data_dir}/rgb"\n    if os.path.exists(rgb_dir):\n        rgb_files = [f for f in os.listdir(rgb_dir) if f.endswith((\'.png\', \'.jpg\', \'.jpeg\'))]\n        validation_results["total_samples"] = len(rgb_files)\n\n        for file in rgb_files:\n            file_path = os.path.join(rgb_dir, file)\n            img = cv2.imread(file_path)\n\n            if img is not None:\n                validation_results["valid_rgb_samples"] += 1\n\n                # Check for common quality issues\n                if img.size == 0:\n                    validation_results["rgb_quality_issues"].append(f"{file}: Empty image")\n                elif np.mean(img) < 10:  # Too dark\n                    validation_results["rgb_quality_issues"].append(f"{file}: Too dark")\n                elif np.mean(img) > 245:  # Too bright\n                    validation_results["rgb_quality_issues"].append(f"{file}: Too bright")\n                elif len(np.unique(img)) < 100:  # Not enough variation\n                    validation_results["rgb_quality_issues"].append(f"{file}: Low variation")\n\n    # Check depth data\n    depth_dir = f"{data_dir}/depth"\n    if os.path.exists(depth_dir):\n        depth_files = [f for f in os.listdir(depth_dir) if f.endswith((\'.png\', \'.jpg\', \'.jpeg\'))]\n\n        for file in depth_files:\n            file_path = os.path.join(depth_dir, file)\n            depth_img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n\n            if depth_img is not None:\n                validation_results["valid_depth_samples"] += 1\n\n                # Check depth quality\n                if np.all(depth_img == 0) or np.all(depth_img == 255):\n                    validation_results["depth_quality_issues"].append(f"{file}: Invalid depth values")\n                elif np.max(depth_img) == np.min(depth_img):  # No variation\n                    validation_results["depth_quality_issues"].append(f"{file}: No depth variation")\n\n    # Calculate completeness score\n    expected_samples = validation_results["total_samples"]\n    if expected_samples > 0:\n        validation_results["completeness_score"] = (\n            (validation_results["valid_rgb_samples"] + validation_results["valid_depth_samples"]) /\n            (expected_samples * 2)  # Assuming RGB and depth for each sample\n        )\n\n    return validation_results\n\ndef print_validation_report(results: dict):\n    """\n    Print a formatted validation report\n    """\n    print("=== Synthetic Data Validation Report ===")\n    print(f"Total samples: {results[\'total_samples\']}")\n    print(f"Valid RGB samples: {results[\'valid_rgb_samples\']}")\n    print(f"Valid depth samples: {results[\'valid_depth_samples\']}")\n    print(f"Completeness score: {results[\'completeness_score\']:.2%}")\n\n    if results[\'rgb_quality_issues\']:\n        print(f"\\nRGB Quality Issues ({len(results[\'rgb_quality_issues\'])}):")\n        for issue in results[\'rgb_quality_issues\'][:5]:  # Show first 5\n            print(f"  - {issue}")\n        if len(results[\'rgb_quality_issues\']) > 5:\n            print(f"  ... and {len(results[\'rgb_quality_issues\']) - 5} more")\n\n    if results[\'depth_quality_issues\']:\n        print(f"\\nDepth Quality Issues ({len(results[\'depth_quality_issues\'])}):")\n        for issue in results[\'depth_quality_issues\'][:5]:  # Show first 5\n            print(f"  - {issue}")\n        if len(results[\'depth_quality_issues\']) > 5:\n            print(f"  ... and {len(results[\'depth_quality_issues\']) - 5} more")\n\n    print("========================================")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"With photorealistic simulation and synthetic data generation properly configured, you're ready to move on to implementing Isaac ROS perception packages. The next section will cover hardware-accelerated perception using Isaac ROS, building on the realistic simulation environment you've created for your humanoid robot applications."})]})}function c(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);