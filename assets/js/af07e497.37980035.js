"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7546],{3984:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vision-language-action/practical-exercises/multimodal-perception","title":"Multimodal Perception Integration","description":"Overview","source":"@site/docs/module-4-vision-language-action/practical-exercises/multimodal-perception.md","sourceDirName":"module-4-vision-language-action/practical-exercises","slug":"/module-4-vision-language-action/practical-exercises/multimodal-perception","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/practical-exercises/multimodal-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/practical-exercises/multimodal-perception.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Action Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/action-execution/vision-action-integration"},"next":{"title":"Module 4 Project: Vision-Language-Action Humanoid Robot","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/module-4-project"}}');var s=t(4848),o=t(8453);const a={sidebar_position:4},r="Multimodal Perception Integration",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Multimodal Architecture Design",id:"multimodal-architecture-design",level:2},{value:"Sensor Fusion Framework",id:"sensor-fusion-framework",level:3},{value:"Cross-Modal Attention Mechanisms",id:"cross-modal-attention-mechanisms",level:3},{value:"Computer Vision Integration",id:"computer-vision-integration",level:2},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:3},{value:"Scene Understanding and Context",id:"scene-understanding-and-context",level:3},{value:"Audio-Visual Integration",id:"audio-visual-integration",level:2},{value:"Sound-Object Association",id:"sound-object-association",level:3},{value:"Context-Aware Perception",id:"context-aware-perception",level:2},{value:"Dynamic Context Management",id:"dynamic-context-management",level:3},{value:"Integration with VLA System",id:"integration-with-vla-system",level:2},{value:"Main Multimodal Perception Node",id:"main-multimodal-perception-node",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"1. Set Up the Perception Package",id:"1-set-up-the-perception-package",level:3},{value:"2. Install Dependencies",id:"2-install-dependencies",level:3},{value:"3. Configure the System",id:"3-configure-the-system",level:3},{value:"4. Testing the System",id:"4-testing-the-system",level:3},{value:"Best Practices and Considerations",id:"best-practices-and-considerations",level:2},{value:"1. Sensor Synchronization",id:"1-sensor-synchronization",level:3},{value:"2. Computational Efficiency",id:"2-computational-efficiency",level:3},{value:"3. Robustness and Reliability",id:"3-robustness-and-reliability",level:3},{value:"4. Privacy and Security",id:"4-privacy-and-security",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Tips",id:"debugging-tips",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multimodal-perception-integration",children:"Multimodal Perception Integration"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Multimodal perception integration is the sensory foundation of Vision-Language-Action (VLA) systems for humanoid robots. This component seamlessly combines visual, auditory, and contextual information to create a comprehensive understanding of the environment that enables intelligent decision-making and action execution."}),"\n",(0,s.jsx)(n.p,{children:"The integration of multiple sensory modalities allows humanoid robots to perceive and interpret complex real-world scenarios, bridging the gap between raw sensor data and high-level cognitive understanding. This module covers the implementation of multimodal perception systems that combine computer vision, audio processing, and contextual awareness for robust humanoid robot operation."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement multimodal sensor fusion for humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Integrate computer vision with language understanding"}),"\n",(0,s.jsx)(n.li,{children:"Create contextual perception systems for dynamic environments"}),"\n",(0,s.jsx)(n.li,{children:"Develop robust object detection and recognition pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Design scene understanding systems for humanoid navigation"}),"\n",(0,s.jsx)(n.li,{children:"Implement cross-modal attention mechanisms for perception"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before implementing multimodal perception integration, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed Module 3 (AI-Robot Brain) focusing on perception systems"}),"\n",(0,s.jsx)(n.li,{children:"Voice recognition and LLM integration systems from previous sections"}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of computer vision concepts (object detection, segmentation)"}),"\n",(0,s.jsx)(n.li,{children:"Experience with ROS 2 sensor message types (Image, PointCloud2, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with deep learning frameworks (PyTorch, TensorFlow)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-architecture-design",children:"Multimodal Architecture Design"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion-framework",children:"Sensor Fusion Framework"}),"\n",(0,s.jsx)(n.p,{children:"Design a comprehensive sensor fusion framework that combines multiple modalities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass SensorData:\n    """Container for multimodal sensor data"""\n    timestamp: float\n    visual_data: np.ndarray = None  # RGB image\n    depth_data: np.ndarray = None   # Depth map\n    audio_data: np.ndarray = None   # Audio signal\n    imu_data: Dict = None           # IMU readings\n    pose_data: Dict = None          # Robot pose\n\nclass SensorFusionNode:\n    """Node for fusing multimodal sensor data"""\n\n    def __init__(self):\n        self.visual_buffer = []\n        self.audio_buffer = []\n        self.depth_buffer = []\n        self.imu_buffer = []\n        self.buffer_size = 10  # Keep last 10 frames for temporal consistency\n\n    def add_visual_data(self, image: np.ndarray, timestamp: float):\n        """Add visual data to fusion buffer"""\n        sensor_data = SensorData(\n            timestamp=timestamp,\n            visual_data=image\n        )\n        self.visual_buffer.append(sensor_data)\n        if len(self.visual_buffer) > self.buffer_size:\n            self.visual_buffer.pop(0)\n\n    def add_audio_data(self, audio: np.ndarray, timestamp: float):\n        """Add audio data to fusion buffer"""\n        sensor_data = SensorData(\n            timestamp=timestamp,\n            audio_data=audio\n        )\n        self.audio_buffer.append(sensor_data)\n        if len(self.audio_buffer) > self.buffer_size:\n            self.audio_buffer.pop(0)\n\n    def add_depth_data(self, depth: np.ndarray, timestamp: float):\n        """Add depth data to fusion buffer"""\n        sensor_data = SensorData(\n            timestamp=timestamp,\n            depth_data=depth\n        )\n        self.depth_buffer.append(sensor_data)\n        if len(self.depth_buffer) > self.buffer_size:\n            self.depth_buffer.pop(0)\n\n    def synchronize_sensors(self) -> Dict[str, Any]:\n        """Synchronize sensor data based on timestamps"""\n        # Find the most recent common timestamp\n        if not all([self.visual_buffer, self.audio_buffer, self.depth_buffer]):\n            return {}\n\n        latest_timestamp = min([\n            self.visual_buffer[-1].timestamp,\n            self.audio_buffer[-1].timestamp,\n            self.depth_buffer[-1].timestamp\n        ])\n\n        # Find closest data for each modality\n        synchronized_data = {\n            \'visual\': self._find_closest_data(self.visual_buffer, latest_timestamp),\n            \'audio\': self._find_closest_data(self.audio_buffer, latest_timestamp),\n            \'depth\': self._find_closest_data(self.depth_buffer, latest_timestamp)\n        }\n\n        return synchronized_data\n\n    def _find_closest_data(self, buffer: List[SensorData], target_time: float) -> SensorData:\n        """Find the sensor data closest to target timestamp"""\n        closest = min(buffer, key=lambda x: abs(x.timestamp - target_time))\n        return closest\n'})}),"\n",(0,s.jsx)(n.h3,{id:"cross-modal-attention-mechanisms",children:"Cross-Modal Attention Mechanisms"}),"\n",(0,s.jsx)(n.p,{children:"Implement attention mechanisms that allow different modalities to influence each other:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CrossModalAttention(nn.Module):\n    """Cross-modal attention for fusing visual and audio information"""\n\n    def __init__(self, visual_dim: int, audio_dim: int, hidden_dim: int = 512):\n        super().__init__()\n        self.visual_dim = visual_dim\n        self.audio_dim = audio_dim\n        self.hidden_dim = hidden_dim\n\n        # Projection layers for each modality\n        self.visual_proj = nn.Linear(visual_dim, hidden_dim)\n        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n\n        # Attention mechanism\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8,\n            dropout=0.1\n        )\n\n        # Fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n    def forward(self, visual_features: torch.Tensor,\n                audio_features: torch.Tensor) -> torch.Tensor:\n        """\n        Forward pass for cross-modal attention\n        Args:\n            visual_features: (batch_size, seq_len, visual_dim)\n            audio_features: (batch_size, seq_len, audio_dim)\n        Returns:\n            fused_features: (batch_size, seq_len, hidden_dim)\n        """\n        # Project features to common space\n        visual_proj = self.visual_proj(visual_features)\n        audio_proj = self.audio_proj(audio_features)\n\n        # Apply cross-attention (visual attending to audio and vice versa)\n        audio_attn, _ = self.attention(\n            visual_proj.transpose(0, 1),\n            audio_proj.transpose(0, 1),\n            audio_proj.transpose(0, 1)\n        )\n        audio_attn = audio_attn.transpose(0, 1)\n\n        visual_attn, _ = self.attention(\n            audio_proj.transpose(0, 1),\n            visual_proj.transpose(0, 1),\n            visual_proj.transpose(0, 1)\n        )\n        visual_attn = visual_attn.transpose(0, 1)\n\n        # Concatenate and fuse\n        combined = torch.cat([audio_attn, visual_attn], dim=-1)\n        fused = self.fusion(combined)\n\n        return fused\n\nclass MultimodalFeatureExtractor(nn.Module):\n    """Extract and combine features from multiple modalities"""\n\n    def __init__(self, config: Dict):\n        super().__init__()\n        self.config = config\n\n        # Visual feature extractor (using pre-trained model)\n        import torchvision.models as models\n        self.visual_backbone = models.resnet50(pretrained=True)\n        self.visual_backbone.fc = nn.Identity()  # Remove final classification layer\n\n        # Audio feature extractor\n        self.audio_backbone = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=80, stride=4),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=3),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(128)\n        )\n\n        # Cross-modal attention\n        self.cross_attention = CrossModalAttention(\n            visual_dim=2048,  # ResNet50 feature dimension\n            audio_dim=128,    # Audio feature dimension\n            hidden_dim=512\n        )\n\n        # Output projection\n        self.output_proj = nn.Linear(512, config.get(\'output_dim\', 256))\n\n    def forward(self, images: torch.Tensor,\n                audio: torch.Tensor) -> torch.Tensor:\n        """Extract and fuse multimodal features"""\n        # Extract visual features\n        visual_features = self.visual_backbone(images)  # (batch, 2048)\n\n        # Extract audio features\n        audio_features = self.audio_backbone(audio.unsqueeze(1))  # (batch, 128, time)\n        audio_features = audio_features.mean(dim=-1)  # Average over time\n\n        # Expand dimensions for attention mechanism\n        visual_features = visual_features.unsqueeze(1)  # (batch, 1, 2048)\n        audio_features = audio_features.unsqueeze(1)    # (batch, 1, 128)\n\n        # Apply cross-modal attention\n        fused_features = self.cross_attention(visual_features, audio_features)\n\n        # Project to output dimension\n        output = self.output_proj(fused_features.squeeze(1))\n\n        return output\n'})}),"\n",(0,s.jsx)(n.h2,{id:"computer-vision-integration",children:"Computer Vision Integration"}),"\n",(0,s.jsx)(n.h3,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Implement robust object detection for humanoid environments:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision.transforms as T\n\nclass MultimodalObjectDetector(Node):\n    \"\"\"Object detection system with multimodal integration\"\"\"\n\n    def __init__(self):\n        super().__init__('multimodal_object_detector')\n\n        # Initialize ROS 2 interfaces\n        self.bridge = CvBridge()\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10)\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/multimodal_detections', 10)\n\n        # Load pre-trained object detection model\n        self.detection_model = self._load_detection_model()\n        self.transforms = T.Compose([\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n        ])\n\n        # Object recognition confidence threshold\n        self.confidence_threshold = 0.5\n\n        # Class names for COCO dataset\n        self.class_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def _load_detection_model(self):\n        \"\"\"Load pre-trained object detection model\"\"\"\n        import torchvision.models.detection as detection_models\n        model = detection_models.fasterrcnn_resnet50_fpn(pretrained=True)\n        model.eval()\n        return model\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process incoming image and perform object detection\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Preprocess image\n            input_tensor = self.transforms(cv_image).unsqueeze(0)\n\n            # Perform detection\n            with torch.no_grad():\n                detections = self.detection_model(input_tensor)\n\n            # Process detections\n            processed_detections = self._process_detections(\n                detections[0], cv_image.shape[:2])\n\n            # Publish results\n            self._publish_detections(processed_detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error in image callback: {e}\")\n\n    def _process_detections(self, detection_result: dict,\n                          image_shape: Tuple[int, int]) -> List[Dict]:\n        \"\"\"Process raw detection results\"\"\"\n        boxes = detection_result['boxes'].cpu().numpy()\n        scores = detection_result['scores'].cpu().numpy()\n        labels = detection_result['labels'].cpu().numpy()\n\n        processed_detections = []\n        for i in range(len(boxes)):\n            if scores[i] > self.confidence_threshold:\n                box = boxes[i]\n                label = int(labels[i])\n                class_name = self.class_names[label] if label < len(self.class_names) else f\"unknown_{label}\"\n\n                detection = {\n                    'bbox': {\n                        'xmin': float(box[0]),\n                        'ymin': float(box[1]),\n                        'xmax': float(box[2]),\n                        'ymax': float(box[3])\n                    },\n                    'class_name': class_name,\n                    'confidence': float(scores[i]),\n                    'class_id': label\n                }\n                processed_detections.append(detection)\n\n        return processed_detections\n\n    def _publish_detections(self, detections: List[Dict], header):\n        \"\"\"Publish detections to ROS 2 topic\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            detection_msg = Detection2D()\n            detection_msg.header = header\n\n            # Set bounding box\n            bbox = detection['bbox']\n            detection_msg.bbox.size_x = bbox['xmax'] - bbox['xmin']\n            detection_msg.bbox.size_y = bbox['ymax'] - bbox['ymin']\n            detection_msg.bbox.center.x = (bbox['xmin'] + bbox['xmax']) / 2\n            detection_msg.bbox.center.y = (bbox['ymin'] + bbox['ymax']) / 2\n\n            # Set hypothesis\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = str(detection['class_id'])\n            hypothesis.hypothesis.score = detection['confidence']\n            detection_msg.results.append(hypothesis)\n\n            detection_array.detections.append(detection_msg)\n\n        self.detection_pub.publish(detection_array)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"scene-understanding-and-context",children:"Scene Understanding and Context"}),"\n",(0,s.jsx)(n.p,{children:"Implement scene understanding capabilities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SceneUnderstandingNode(Node):\n    \"\"\"Scene understanding with contextual awareness\"\"\"\n\n    def __init__(self):\n        super().__init__('scene_understanding_node')\n\n        # Publishers and subscribers\n        self.scene_pub = self.create_publisher(String, '/scene_description', 10)\n        self.object_sub = self.create_subscription(\n            Detection2DArray, '/multimodal_detections',\n            self.detection_callback, 10)\n        self.location_sub = self.create_subscription(\n            String, '/current_location', self.location_callback, 10)\n\n        # Scene context management\n        self.current_scene = {}\n        self.location_context = {}\n        self.object_relationships = {}\n\n        # Timer for periodic scene updates\n        self.timer = self.create_timer(1.0, self._update_scene_context)\n\n    def detection_callback(self, msg: Detection2DArray):\n        \"\"\"Process object detections and update scene context\"\"\"\n        current_objects = {}\n\n        for detection in msg.detections:\n            if detection.results:\n                best_result = max(detection.results,\n                                key=lambda x: x.hypothesis.score)\n                object_info = {\n                    'class': best_result.hypothesis.class_id,\n                    'confidence': best_result.hypothesis.score,\n                    'position': detection.bbox.center,\n                    'size': (detection.bbox.size_x, detection.bbox.size_y)\n                }\n                current_objects[best_result.hypothesis.class_id] = object_info\n\n        self.current_scene = {\n            'timestamp': msg.header.stamp,\n            'objects': current_objects,\n            'location': self.location_context.get('name', 'unknown')\n        }\n\n    def location_callback(self, msg: String):\n        \"\"\"Update location context\"\"\"\n        self.location_context = {\n            'name': msg.data,\n            'timestamp': self.get_clock().now().to_msg()\n        }\n\n    def _update_scene_context(self):\n        \"\"\"Update and publish scene context\"\"\"\n        if self.current_scene:\n            scene_description = self._generate_scene_description()\n            scene_msg = String()\n            scene_msg.data = scene_description\n            self.scene_pub.publish(scene_msg)\n\n    def _generate_scene_description(self) -> str:\n        \"\"\"Generate natural language description of current scene\"\"\"\n        if not self.current_scene.get('objects'):\n            return f\"Currently in {self.current_scene.get('location', 'unknown location')}. No objects detected.\"\n\n        objects = self.current_scene['objects']\n        object_list = []\n\n        for obj_class, obj_info in objects.items():\n            if obj_info['confidence'] > 0.7:  # High confidence objects only\n                object_list.append(obj_class)\n\n        if len(object_list) == 1:\n            description = f\"In {self.current_scene['location']}, I see a {object_list[0]}.\"\n        elif len(object_list) == 2:\n            description = f\"In {self.current_scene['location']}, I see a {object_list[0]} and a {object_list[1]}.\"\n        else:\n            description = f\"In {self.current_scene['location']}, I see: {', '.join(object_list[:3])}.\"\n            if len(object_list) > 3:\n                description += f\" and {len(object_list) - 3} other objects.\"\n\n        return description\n\n    def get_object_relationships(self, object_a: str, object_b: str) -> Dict:\n        \"\"\"Get spatial relationships between objects\"\"\"\n        objects = self.current_scene.get('objects', {})\n\n        if object_a not in objects or object_b not in objects:\n            return {}\n\n        pos_a = objects[object_a]['position']\n        pos_b = objects[object_b]['position']\n\n        # Calculate relative position\n        dx = pos_b.x - pos_a.x\n        dy = pos_b.y - pos_a.y\n\n        relationship = {\n            'distance': np.sqrt(dx**2 + dy**2),\n            'angle': np.arctan2(dy, dx),\n            'relative_position': self._get_relative_position(dx, dy)\n        }\n\n        return relationship\n\n    def _get_relative_position(self, dx: float, dy: float) -> str:\n        \"\"\"Get relative position description\"\"\"\n        if abs(dx) > abs(dy):\n            if dx > 0:\n                return \"to the right of\"\n            else:\n                return \"to the left of\"\n        else:\n            if dy > 0:\n                return \"below\"\n            else:\n                return \"above\"\n"})}),"\n",(0,s.jsx)(n.h2,{id:"audio-visual-integration",children:"Audio-Visual Integration"}),"\n",(0,s.jsx)(n.h3,{id:"sound-object-association",children:"Sound-Object Association"}),"\n",(0,s.jsx)(n.p,{children:"Associate audio events with visual objects:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class AudioVisualAssociation(Node):\n    \"\"\"Associate audio events with visual objects\"\"\"\n\n    def __init__(self):\n        super().__init__('audio_visual_association')\n\n        # Subscriptions\n        self.audio_sub = self.create_subscription(\n            String, '/audio_events', self.audio_callback, 10)\n        self.object_sub = self.create_subscription(\n            Detection2DArray, '/multimodal_detections',\n            self.detection_callback, 10)\n        self.audio_visual_pub = self.create_publisher(\n            String, '/audio_visual_associations', 10)\n\n        # Association tracking\n        self.audio_events = {}\n        self.visual_objects = {}\n        self.associations = {}\n\n        # Association parameters\n        self.temporal_window = 2.0  # seconds\n        self.spatial_threshold = 50  # pixels\n\n    def audio_callback(self, msg: String):\n        \"\"\"Process audio events\"\"\"\n        import json\n        try:\n            audio_data = json.loads(msg.data)\n            timestamp = self.get_clock().now().nanoseconds / 1e9\n\n            self.audio_events[timestamp] = {\n                'event': audio_data.get('event'),\n                'confidence': audio_data.get('confidence', 1.0),\n                'location': audio_data.get('location', 'unknown')\n            }\n\n            # Check for recent visual objects to associate\n            self._find_associations(timestamp)\n\n        except json.JSONDecodeError:\n            self.get_logger().warn(\"Invalid audio event JSON\")\n\n    def detection_callback(self, msg: Detection2DArray):\n        \"\"\"Process visual detections\"\"\"\n        timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec / 1e9\n\n        objects = {}\n        for detection in msg.detections:\n            if detection.results:\n                best_result = max(detection.results,\n                                key=lambda x: x.hypothesis.score)\n                objects[best_result.hypothesis.class_id] = {\n                    'bbox': detection.bbox,\n                    'confidence': best_result.hypothesis.score\n                }\n\n        self.visual_objects[timestamp] = objects\n\n        # Check for recent audio events to associate\n        self._find_associations(timestamp)\n\n    def _find_associations(self, current_time: float):\n        \"\"\"Find associations between audio events and visual objects\"\"\"\n        # Clean up old events\n        cutoff_time = current_time - self.temporal_window\n        self.audio_events = {k: v for k, v in self.audio_events.items() if k > cutoff_time}\n        self.visual_objects = {k: v for k, v in self.visual_objects.items() if k > cutoff_time}\n\n        # Find associations within temporal window\n        for audio_time, audio_event in self.audio_events.items():\n            for visual_time, visual_objects in self.visual_objects.items():\n                if abs(audio_time - visual_time) <= self.temporal_window:\n                    # Check for spatial associations if location info available\n                    self._associate_spatially(audio_event, visual_objects, audio_time, visual_time)\n\n    def _associate_spatially(self, audio_event: Dict, visual_objects: Dict,\n                           audio_time: float, visual_time: float):\n        \"\"\"Associate audio events with visual objects based on spatial information\"\"\"\n        associations = []\n\n        for obj_class, obj_data in visual_objects.items():\n            if obj_data['confidence'] > 0.5:  # Only confident detections\n                # For now, we'll use simple temporal association\n                # In a real system, you'd use audio source localization\n                association = {\n                    'audio_event': audio_event['event'],\n                    'visual_object': obj_class,\n                    'confidence': min(audio_event['confidence'], obj_data['confidence']),\n                    'timestamp': (audio_time + visual_time) / 2\n                }\n                associations.append(association)\n\n        if associations:\n            # Publish the strongest association\n            best_assoc = max(associations, key=lambda x: x['confidence'])\n            assoc_msg = String()\n            assoc_msg.data = json.dumps(best_assoc)\n            self.audio_visual_pub.publish(assoc_msg)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"context-aware-perception",children:"Context-Aware Perception"}),"\n",(0,s.jsx)(n.h3,{id:"dynamic-context-management",children:"Dynamic Context Management"}),"\n",(0,s.jsx)(n.p,{children:"Implement dynamic context management for changing environments:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from collections import defaultdict, deque\nimport threading\n\nclass ContextAwarePerception(Node):\n    \"\"\"Context-aware perception system for dynamic environments\"\"\"\n\n    def __init__(self):\n        super().__init__('context_aware_perception')\n\n        # Publishers and subscribers\n        self.perception_pub = self.create_publisher(\n            String, '/contextual_perception', 10)\n        self.scene_sub = self.create_subscription(\n            String, '/scene_description', self.scene_callback, 10)\n        self.voice_command_sub = self.create_subscription(\n            String, '/voice_command', self.voice_command_callback, 10)\n\n        # Context management\n        self.context_memory = defaultdict(deque)\n        self.context_size = 10  # Keep last 10 context items per category\n        self.context_lock = threading.Lock()\n\n        # Context categories\n        self.context_categories = [\n            'objects', 'locations', 'activities',\n            'people', 'commands', 'responses'\n        ]\n\n        # Timer for context updates\n        self.timer = self.create_timer(0.5, self._update_context)\n\n        # Initialize context\n        self.current_context = {\n            'timestamp': self.get_clock().now().to_msg(),\n            'objects': [],\n            'location': 'unknown',\n            'activity': 'idle',\n            'attention_objects': [],\n            'recent_commands': []\n        }\n\n    def scene_callback(self, msg: String):\n        \"\"\"Process scene descriptions\"\"\"\n        with self.context_lock:\n            # Parse scene description and update context\n            scene_data = self._parse_scene_description(msg.data)\n            self.current_context.update(scene_data)\n            self.current_context['timestamp'] = self.get_clock().now().to_msg()\n\n            # Store in memory\n            self._store_context('objects', scene_data.get('objects', []))\n            self._store_context('location', scene_data.get('location', 'unknown'))\n\n    def voice_command_callback(self, msg: String):\n        \"\"\"Process voice commands to update context\"\"\"\n        with self.context_lock:\n            self.current_context['recent_commands'].append({\n                'command': msg.data,\n                'timestamp': self.get_clock().now().to_msg()\n            })\n\n            # Limit recent commands to 5\n            if len(self.current_context['recent_commands']) > 5:\n                self.current_context['recent_commands'] = \\\n                    self.current_context['recent_commands'][-5:]\n\n            self._store_context('commands', msg.data)\n\n    def _parse_scene_description(self, description: str) -> Dict:\n        \"\"\"Parse natural language scene description\"\"\"\n        # Simple parsing - in practice, you'd use NLP techniques\n        result = {\n            'objects': [],\n            'location': 'unknown',\n            'description': description\n        }\n\n        # Extract location from common phrases\n        location_keywords = ['in', 'at', 'near', 'by']\n        words = description.lower().split()\n\n        for i, word in enumerate(words):\n            if word in location_keywords and i + 1 < len(words):\n                result['location'] = words[i + 1]\n\n        # Extract objects from the description\n        # This is simplified - real NLP would be more sophisticated\n        object_keywords = ['see', 'detect', 'find', 'observe']\n        for keyword in object_keywords:\n            if keyword in description.lower():\n                # Extract objects following the keyword\n                parts = description.lower().split(keyword)\n                if len(parts) > 1:\n                    following_text = parts[1]\n                    # Simple object extraction (would use proper NLP in practice)\n                    for obj in ['person', 'cup', 'chair', 'table', 'book', 'bottle']:\n                        if obj in following_text:\n                            if obj not in result['objects']:\n                                result['objects'].append(obj)\n\n        return result\n\n    def _store_context(self, category: str, data: Any):\n        \"\"\"Store context data with size limit\"\"\"\n        self.context_memory[category].append({\n            'data': data,\n            'timestamp': self.get_clock().now().to_msg()\n        })\n\n        # Maintain size limit\n        if len(self.context_memory[category]) > self.context_size:\n            self.context_memory[category].popleft()\n\n    def _update_context(self):\n        \"\"\"Periodically update and publish context\"\"\"\n        with self.context_lock:\n            context_msg = String()\n            context_msg.data = json.dumps(self.current_context, default=str)\n            self.perception_pub.publish(context_msg)\n\n    def get_context_for_query(self, query: str) -> Dict:\n        \"\"\"Get relevant context for a specific query\"\"\"\n        with self.context_lock:\n            relevant_context = {\n                'current_scene': self.current_context.copy(),\n                'recent_objects': list(self.context_memory['objects'])[-3:],\n                'recent_commands': self.current_context['recent_commands'][-3:],\n                'location_history': list(self.context_memory['location'])[-5:]\n            }\n\n        return relevant_context\n\n    def get_attention_objects(self, focus_area: str = None) -> List[str]:\n        \"\"\"Get objects that should receive attention\"\"\"\n        with self.context_lock:\n            if focus_area:\n                # Filter objects by location or other criteria\n                attention_objects = [\n                    obj for obj in self.current_context['objects']\n                    if focus_area.lower() in obj.lower()\n                ]\n            else:\n                # Return all current objects\n                attention_objects = self.current_context['objects']\n\n        return attention_objects\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-vla-system",children:"Integration with VLA System"}),"\n",(0,s.jsx)(n.h3,{id:"main-multimodal-perception-node",children:"Main Multimodal Perception Node"}),"\n",(0,s.jsx)(n.p,{children:"Create the main integration node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultimodalPerceptionNode(Node):\n    """Main node for multimodal perception integration"""\n\n    def __init__(self):\n        super().__init__(\'multimodal_perception_node\')\n\n        # Initialize sub-components\n        self.object_detector = MultimodalObjectDetector()\n        self.scene_understanding = SceneUnderstandingNode()\n        self.audio_visual_association = AudioVisualAssociation()\n        self.context_aware_perception = ContextAwarePerception()\n\n        # Publishers for integrated output\n        self.integrated_perception_pub = self.create_publisher(\n            String, \'/integrated_perception\', 10)\n\n        # Timer for integration updates\n        self.integration_timer = self.create_timer(0.1, self._integrate_perception)\n\n        self.get_logger().info("Multimodal Perception Node initialized")\n\n    def _integrate_perception(self):\n        """Integrate all perception components"""\n        try:\n            # Gather perception data from all components\n            perception_data = {\n                \'timestamp\': self.get_clock().now().to_msg(),\n                \'objects\': self._get_current_objects(),\n                \'scene_description\': self._get_scene_description(),\n                \'audio_visual_associations\': self._get_audio_visual_associations(),\n                \'context\': self._get_current_context(),\n                \'attention_objects\': self._get_attention_objects()\n            }\n\n            # Publish integrated perception\n            perception_msg = String()\n            perception_msg.data = json.dumps(perception_data, default=str)\n            self.integrated_perception_pub.publish(perception_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Error in perception integration: {e}")\n\n    def _get_current_objects(self) -> List[Dict]:\n        """Get current object detections"""\n        # This would interface with the object detector\n        # For now, returning empty list\n        return []\n\n    def _get_scene_description(self) -> str:\n        """Get current scene description"""\n        # This would interface with scene understanding\n        # For now, returning placeholder\n        return "Scene description not available"\n\n    def _get_audio_visual_associations(self) -> List[Dict]:\n        """Get current audio-visual associations"""\n        # This would interface with audio-visual association\n        # For now, returning empty list\n        return []\n\n    def _get_current_context(self) -> Dict:\n        """Get current context"""\n        # This would interface with context-aware perception\n        # For now, returning basic context\n        return {\n            \'location\': \'unknown\',\n            \'activity\': \'idle\',\n            \'timestamp\': self.get_clock().now().to_msg()\n        }\n\n    def _get_attention_objects(self) -> List[str]:\n        """Get objects requiring attention"""\n        # This would interface with attention system\n        # For now, returning empty list\n        return []\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    # Create and run the multimodal perception node\n    node = MultimodalPerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsx)(n.h3,{id:"1-set-up-the-perception-package",children:"1. Set Up the Perception Package"}),"\n",(0,s.jsx)(n.p,{children:"Create the ROS 2 package for multimodal perception:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create package\ncd ~/ros2_ws/src\nros2 pkg create --build-type ament_python multimodal_perception\ncd multimodal_perception\nmkdir -p multimodal_perception/config multimodal_perception/launch\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-install-dependencies",children:"2. Install Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Create requirements file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create requirements.txt\ncat > multimodal_perception/requirements.txt << EOF\ntorch>=2.0.0\ntorchvision>=0.15.0\nopencv-python>=4.8.0\nnumpy>=1.21.0\ncv-bridge>=3.0.0\nsensor-msgs>=4.0.0\nvision-msgs>=4.0.0\nEOF\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-configure-the-system",children:"3. Configure the System"}),"\n",(0,s.jsx)(n.p,{children:"Create a launch file for the multimodal perception system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- multimodal_perception/launch/multimodal_perception.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config_dir = os.path.join(\n        get_package_share_directory('multimodal_perception'), 'config')\n\n    return LaunchDescription([\n        Node(\n            package='multimodal_perception',\n            executable='multimodal_perception_node',\n            name='multimodal_perception_node',\n            parameters=[],\n            output='screen'\n        ),\n        Node(\n            package='multimodal_perception',\n            executable='multimodal_object_detector',\n            name='multimodal_object_detector',\n            output='screen'\n        ),\n        Node(\n            package='multimodal_perception',\n            executable='scene_understanding_node',\n            name='scene_understanding_node',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-testing-the-system",children:"4. Testing the System"}),"\n",(0,s.jsx)(n.p,{children:"Create a test script to verify multimodal perception:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# test_multimodal_perception.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nimport time\n\nclass MultimodalTestClient(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_test_client\')\n\n        # Publishers for testing\n        self.test_image_pub = self.create_publisher(\n            Image, \'/camera/rgb/image_raw\', 10)\n        self.test_audio_pub = self.create_publisher(\n            String, \'/audio_events\', 10)\n\n        # Subscription to integrated perception\n        self.perception_sub = self.create_subscription(\n            String, \'/integrated_perception\',\n            self.perception_callback, 10)\n\n        self.timer = self.create_timer(2.0, self.send_test_data)\n        self.test_count = 0\n\n    def send_test_data(self):\n        """Send test data to multimodal system"""\n        if self.test_count < 5:\n            # Send test audio event\n            audio_msg = String()\n            audio_msg.data = \'{"event": "speech", "confidence": 0.9, "location": "front"}\'\n            self.test_audio_pub.publish(audio_msg)\n\n            self.get_logger().info(f"Sent test audio event #{self.test_count}")\n            self.test_count += 1\n\n    def perception_callback(self, msg: String):\n        """Handle integrated perception results"""\n        self.get_logger().info(f"Received integrated perception: {msg.data[:100]}...")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    test_client = MultimodalTestClient()\n\n    try:\n        rclpy.spin(test_client)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        test_client.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-and-considerations",children:"Best Practices and Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"1-sensor-synchronization",children:"1. Sensor Synchronization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement proper timestamp synchronization between modalities"}),"\n",(0,s.jsx)(n.li,{children:"Use hardware or software triggers for precise alignment"}),"\n",(0,s.jsx)(n.li,{children:"Account for sensor latency differences"}),"\n",(0,s.jsx)(n.li,{children:"Implement buffer management for temporal consistency"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-computational-efficiency",children:"2. Computational Efficiency"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use lightweight models for real-time processing"}),"\n",(0,s.jsx)(n.li,{children:"Implement model quantization where possible"}),"\n",(0,s.jsx)(n.li,{children:"Use GPU acceleration for deep learning components"}),"\n",(0,s.jsx)(n.li,{children:"Optimize data pipelines to reduce latency"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-robustness-and-reliability",children:"3. Robustness and Reliability"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Handle sensor failures gracefully"}),"\n",(0,s.jsx)(n.li,{children:"Implement fallback mechanisms for missing modalities"}),"\n",(0,s.jsx)(n.li,{children:"Use uncertainty quantification for perception confidence"}),"\n",(0,s.jsx)(n.li,{children:"Validate sensor data quality before processing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-privacy-and-security",children:"4. Privacy and Security"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Protect sensitive visual and audio data"}),"\n",(0,s.jsx)(n.li,{children:"Implement data encryption for transmission"}),"\n",(0,s.jsx)(n.li,{children:"Consider privacy implications of persistent monitoring"}),"\n",(0,s.jsx)(n.li,{children:"Follow data retention policies"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Usage"}),": Monitor GPU and system memory usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synchronization Problems"}),": Check timestamp alignment between sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Performance"}),": Fine-tune models for specific humanoid environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Constraints"}),": Optimize for computational efficiency"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Enable detailed logging for each perception component"}),"\n",(0,s.jsx)(n.li,{children:"Monitor data flow between nodes"}),"\n",(0,s.jsx)(n.li,{children:"Use visualization tools to verify sensor alignment"}),"\n",(0,s.jsx)(n.li,{children:"Test individual components before system integration"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This multimodal perception integration system provides the sensory foundation for Vision-Language-Action systems, enabling humanoid robots to understand and interact with their environment through multiple sensory modalities working in harmony."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);