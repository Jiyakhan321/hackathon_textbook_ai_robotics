"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6736],{760:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-2-digital-twin/unity-simulation/sensor-simulation","title":"Sensor Simulation in Unity","description":"Overview","source":"@site/docs/module-2-digital-twin/unity-simulation/sensor-simulation.md","sourceDirName":"module-2-digital-twin/unity-simulation","slug":"/module-2-digital-twin/unity-simulation/sensor-simulation","permalink":"/hackathon_textbook_ai_robotics/docs/module-2-digital-twin/unity-simulation/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-2-digital-twin/unity-simulation/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Unity-ROS Communication Implementation","permalink":"/hackathon_textbook_ai_robotics/docs/module-2-digital-twin/unity-simulation/unity-ros-communication"},"next":{"title":"Module 2 Conclusion: Complete Digital Twin Implementation","permalink":"/hackathon_textbook_ai_robotics/docs/module-2-digital-twin/conclusion"}}');var t=i(4848),o=i(8453);const r={sidebar_position:5},s="Sensor Simulation in Unity",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Camera Sensor Simulation",id:"camera-sensor-simulation",level:2},{value:"1. Basic Camera Sensor Setup",id:"1-basic-camera-sensor-setup",level:3},{value:"2. Stereo Camera System",id:"2-stereo-camera-system",level:3},{value:"LiDAR Sensor Simulation",id:"lidar-sensor-simulation",level:2},{value:"1. 2D LiDAR Implementation",id:"1-2d-lidar-implementation",level:3},{value:"2. 3D LiDAR Implementation",id:"2-3d-lidar-implementation",level:3},{value:"IMU Sensor Simulation",id:"imu-sensor-simulation",level:2},{value:"1. Comprehensive IMU Implementation",id:"1-comprehensive-imu-implementation",level:3},{value:"Force/Torque Sensor Simulation",id:"forcetorque-sensor-simulation",level:2},{value:"1. Joint Force/Torque Sensors",id:"1-joint-forcetorque-sensors",level:3},{value:"Sensor Fusion and Data Processing",id:"sensor-fusion-and-data-processing",level:2},{value:"1. Sensor Data Aggregator",id:"1-sensor-data-aggregator",level:3},{value:"Sensor Calibration and Validation",id:"sensor-calibration-and-validation",level:2},{value:"1. Sensor Calibration System",id:"1-sensor-calibration-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Sensor Update Manager",id:"1-sensor-update-manager",level:3},{value:"Next Steps",id:"next-steps",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"sensor-simulation-in-unity",children:"Sensor Simulation in Unity"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"Sensor simulation is a critical component of digital twin systems for humanoid robots, enabling realistic perception and interaction with virtual environments. This section covers the implementation of various sensors in Unity, including cameras, LiDAR, IMU, and force/torque sensors, with proper ROS 2 integration for realistic data output."}),"\n",(0,t.jsx)(e.h2,{id:"camera-sensor-simulation",children:"Camera Sensor Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"1-basic-camera-sensor-setup",children:"1. Basic Camera Sensor Setup"}),"\n",(0,t.jsx)(e.p,{children:"Creating realistic camera sensors in Unity involves setting up cameras with appropriate parameters that match real-world sensors:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor;\nusing System.Collections;\nusing System.Collections.Generic;\n\n[RequireComponent(typeof(Camera))]\npublic class UnityCameraSensor : MonoBehaviour\n{\n    [Header("Camera Configuration")]\n    [SerializeField] private string cameraName = "camera";\n    [SerializeField] private string imageTopic = "/camera/image_raw";\n    [SerializeField] private string infoTopic = "/camera/camera_info";\n\n    [Header("Image Settings")]\n    [SerializeField] private int imageWidth = 640;\n    [SerializeField] private int imageHeight = 480;\n    [SerializeField] private float publishRate = 30.0f;\n    [SerializeField] private Camera.StereoscopicEye stereoEye = Camera.StereoscopicEye.Mono;\n\n    [Header("Camera Parameters")]\n    [SerializeField] private float fov = 60.0f;\n    [SerializeField] private float nearClip = 0.1f;\n    [SerializeField] private float farClip = 100.0f;\n\n    [Header("Noise Settings")]\n    [SerializeField] private float noiseIntensity = 0.01f;\n    [SerializeField] private float noiseFrequency = 10.0f;\n\n    private Camera cam;\n    private RenderTexture renderTexture;\n    private Texture2D texture2D;\n    private byte[] imageBytes;\n    private float publishTimer = 0.0f;\n    private ROSConnection rosConnection;\n\n    // Camera info parameters\n    private double[] cameraMatrix;\n    private double[] distortionCoefficients;\n\n    void Start()\n    {\n        InitializeCamera();\n        InitializeCameraParameters();\n        rosConnection = ROSConnection.instance;\n    }\n\n    private void InitializeCamera()\n    {\n        cam = GetComponent<Camera>();\n        if (cam == null)\n            cam = gameObject.AddComponent<Camera>();\n\n        // Set camera parameters\n        cam.fieldOfView = fov;\n        cam.nearClipPlane = nearClip;\n        cam.farClipPlane = farClip;\n        cam.stereoTargetEye = stereoEye;\n\n        // Create render texture for image capture\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24, RenderTextureFormat.ARGB32);\n        texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n\n        cam.targetTexture = renderTexture;\n    }\n\n    private void InitializeCameraParameters()\n    {\n        // Calculate camera matrix based on FOV and resolution\n        float focalLength = (imageHeight / 2.0f) / Mathf.Tan(Mathf.Deg2Rad * fov / 2.0f);\n        float centerX = imageWidth / 2.0f;\n        float centerY = imageHeight / 2.0f;\n\n        cameraMatrix = new double[] {\n            focalLength, 0, centerX,\n            0, focalLength, centerY,\n            0, 0, 1\n        };\n\n        // Initialize distortion coefficients (assumes no distortion initially)\n        distortionCoefficients = new double[] { 0, 0, 0, 0, 0 };\n    }\n\n    void Update()\n    {\n        publishTimer += Time.deltaTime;\n        float publishInterval = 1.0f / publishRate;\n\n        if (publishTimer >= publishInterval)\n        {\n            PublishCameraData();\n            publishTimer = 0.0f;\n        }\n    }\n\n    private void PublishCameraData()\n    {\n        if (rosConnection == null || cam == null)\n            return;\n\n        // Render the camera to texture\n        RenderTexture.active = renderTexture;\n        cam.Render();\n\n        // Read pixels from render texture\n        texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        texture2D.Apply();\n\n        // Apply noise to simulate real camera\n        ApplyCameraNoise(texture2D);\n\n        // Convert to byte array\n        imageBytes = texture2D.EncodeToJPG(85); // 85% quality for realistic compression\n\n        // Create and publish image message\n        ImageMsg imageMsg = new ImageMsg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = cameraName\n            },\n            height = (uint)imageHeight,\n            width = (uint)imageWidth,\n            encoding = "rgb8",\n            is_bigendian = 0,\n            step = (uint)(imageWidth * 3), // 3 bytes per pixel for RGB\n            data = imageBytes\n        };\n\n        rosConnection.SendUnityMessage(imageTopic, imageMsg);\n\n        // Publish camera info\n        PublishCameraInfo();\n    }\n\n    private void ApplyCameraNoise(Texture2D texture)\n    {\n        if (noiseIntensity <= 0) return;\n\n        Color[] pixels = texture.GetPixels();\n\n        for (int i = 0; i < pixels.Length; i++)\n        {\n            // Add random noise\n            float noise = Mathf.PerlinNoise(i * noiseFrequency, Time.time * noiseFrequency) * noiseIntensity;\n            pixels[i] = pixels[i] + new Color(noise, noise, noise, 0);\n        }\n\n        texture.SetPixels(pixels);\n        texture.Apply();\n    }\n\n    private void PublishCameraInfo()\n    {\n        CameraInfoMsg cameraInfo = new CameraInfoMsg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = cameraName\n            },\n            height = (uint)imageHeight,\n            width = (uint)imageWidth,\n            distortion_model = "plumb_bob",\n            D = distortionCoefficients,\n            K = cameraMatrix,\n            R = new double[] { 1, 0, 0, 0, 1, 0, 0, 0, 1 },\n            P = new double[] {\n                cameraMatrix[0], 0, cameraMatrix[2], 0,\n                0, cameraMatrix[4], cameraMatrix[5], 0,\n                0, 0, 1, 0\n            }\n        };\n\n        rosConnection.SendUnityMessage(infoTopic, cameraInfo);\n    }\n\n    void OnDestroy()\n    {\n        if (renderTexture != null)\n            RenderTexture.ReleaseTemporary(renderTexture);\n    }\n\n    // Public methods for runtime configuration\n    public void SetResolution(int width, int height)\n    {\n        imageWidth = width;\n        imageHeight = height;\n\n        if (renderTexture != null)\n            RenderTexture.ReleaseTemporary(renderTexture);\n\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24, RenderTextureFormat.ARGB32);\n        texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n        cam.targetTexture = renderTexture;\n\n        InitializeCameraParameters();\n    }\n\n    public void SetFov(float newFov)\n    {\n        fov = newFov;\n        cam.fieldOfView = fov;\n        InitializeCameraParameters();\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"2-stereo-camera-system",children:"2. Stereo Camera System"}),"\n",(0,t.jsx)(e.p,{children:"For humanoid robots that require depth perception:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class StereoCameraSystem : MonoBehaviour\n{\n    [Header("Stereo Configuration")]\n    [SerializeField] private float interocularDistance = 0.064f; // Average human eye distance in meters\n    [SerializeField] private GameObject leftCameraGO;\n    [SerializeField] private GameObject rightCameraGO;\n\n    [Header("Synchronization")]\n    [SerializeField] private bool syncCameraParameters = true;\n\n    private UnityCameraSensor leftCamera;\n    private UnityCameraSensor rightCamera;\n\n    void Start()\n    {\n        SetupStereoCameras();\n    }\n\n    private void SetupStereoCameras()\n    {\n        if (leftCameraGO == null || rightCameraGO == null)\n        {\n            // Create camera game objects if not provided\n            leftCameraGO = new GameObject("LeftCamera");\n            rightCameraGO = new GameObject("RightCamera");\n\n            leftCameraGO.transform.SetParent(transform);\n            rightCameraGO.transform.SetParent(transform);\n\n            leftCameraGO.transform.localPosition = new Vector3(-interocularDistance / 2, 0, 0);\n            rightCameraGO.transform.localPosition = new Vector3(interocularDistance / 2, 0, 0);\n        }\n\n        leftCamera = leftCameraGO.GetComponent<UnityCameraSensor>();\n        if (leftCamera == null)\n            leftCamera = leftCameraGO.AddComponent<UnityCameraSensor>();\n\n        rightCamera = rightCameraGO.GetComponent<UnityCameraSensor>();\n        if (rightCamera == null)\n            rightCamera = rightCameraGO.AddComponent<UnityCameraSensor>();\n\n        // Configure stereo-specific settings\n        ConfigureStereoCameras();\n    }\n\n    private void ConfigureStereoCameras()\n    {\n        if (leftCamera == null || rightCamera == null) return;\n\n        // Set different topics for stereo cameras\n        leftCamera.imageTopic = "/stereo/left/image_raw";\n        leftCamera.infoTopic = "/stereo/left/camera_info";\n        leftCamera.cameraName = "left_camera";\n\n        rightCamera.imageTopic = "/stereo/right/image_raw";\n        rightCamera.infoTopic = "/stereo/right/camera_info";\n        rightCamera.cameraName = "right_camera";\n\n        // Synchronize parameters if requested\n        if (syncCameraParameters)\n        {\n            SynchronizeCameraParameters();\n        }\n    }\n\n    private void SynchronizeCameraParameters()\n    {\n        if (leftCamera == null || rightCamera == null) return;\n\n        // Copy parameters from left to right camera\n        rightCamera.SetResolution((int)leftCamera.GetType().GetField("imageWidth").GetValue(leftCamera),\n                                  (int)leftCamera.GetType().GetField("imageHeight").GetValue(leftCamera));\n        // Note: In a real implementation, you\'d want to expose these as public properties\n        // or use reflection carefully\n    }\n\n    public UnityCameraSensor GetLeftCamera() { return leftCamera; }\n    public UnityCameraSensor GetRightCamera() { return rightCamera; }\n\n    public void SetInterocularDistance(float distance)\n    {\n        interocularDistance = distance;\n        if (leftCameraGO != null && rightCameraGO != null)\n        {\n            leftCameraGO.transform.localPosition = new Vector3(-interocularDistance / 2, 0, 0);\n            rightCameraGO.transform.localPosition = new Vector3(interocularDistance / 2, 0, 0);\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"lidar-sensor-simulation",children:"LiDAR Sensor Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"1-2d-lidar-implementation",children:"1. 2D LiDAR Implementation"}),"\n",(0,t.jsx)(e.p,{children:"Simulating 2D LiDAR for navigation and obstacle detection:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor;\nusing System.Collections.Generic;\n\n[RequireComponent(typeof(RaycastHit2D))]\npublic class UnityLidar2D : MonoBehaviour\n{\n    [Header("LiDAR Configuration")]\n    [SerializeField] private string scanTopic = "/scan";\n    [SerializeField] private string frameId = "laser_frame";\n\n    [Header("Scan Parameters")]\n    [SerializeField] private float minAngle = -Mathf.PI / 2; // -90 degrees\n    [SerializeField] private float maxAngle = Mathf.PI / 2;  // 90 degrees\n    [SerializeField] private int numRays = 360;\n    [SerializeField] private float maxRange = 10.0f;\n    [SerializeField] private float minRange = 0.1f;\n    [SerializeField] private float publishRate = 10.0f;\n\n    [Header("Physics Settings")]\n    [SerializeField] private LayerMask collisionLayers = -1;\n    [SerializeField] private float noiseLevel = 0.01f;\n\n    private float publishTimer = 0.0f;\n    private ROSConnection rosConnection;\n    private float angleIncrement;\n    private List<float> ranges;\n\n    void Start()\n    {\n        rosConnection = ROSConnection.instance;\n        angleIncrement = (maxAngle - minAngle) / (numRays - 1);\n        ranges = new List<float>(new float[numRays]);\n    }\n\n    void Update()\n    {\n        publishTimer += Time.deltaTime;\n        float publishInterval = 1.0f / publishRate;\n\n        if (publishTimer >= publishInterval)\n        {\n            PerformLidarScan();\n            publishTimer = 0.0f;\n        }\n    }\n\n    private void PerformLidarScan()\n    {\n        if (rosConnection == null) return;\n\n        // Perform raycasting for each angle\n        for (int i = 0; i < numRays; i++)\n        {\n            float angle = minAngle + (i * angleIncrement);\n            Vector2 direction = new Vector2(Mathf.Cos(angle), Mathf.Sin(angle));\n\n            RaycastHit2D hit = Physics2D.Raycast(transform.position, direction, maxRange, collisionLayers);\n\n            if (hit.collider != null)\n            {\n                float distance = hit.distance;\n                // Add noise to simulate real sensor\n                distance += Random.Range(-noiseLevel, noiseLevel);\n                ranges[i] = Mathf.Clamp(distance, minRange, maxRange);\n            }\n            else\n            {\n                ranges[i] = maxRange;\n            }\n        }\n\n        // Publish scan message\n        LaserScanMsg scanMsg = new LaserScanMsg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = frameId\n            },\n            angle_min = minAngle,\n            angle_max = maxAngle,\n            angle_increment = angleIncrement,\n            time_increment = 0, // Not applicable for simulated lidar\n            scan_time = 1.0f / publishRate,\n            range_min = minRange,\n            range_max = maxRange,\n            ranges = ranges.ToArray(),\n            intensities = new float[numRays] // Empty intensities array\n        };\n\n        rosConnection.SendUnityMessage(scanTopic, scanMsg);\n    }\n\n    // Visualization in editor\n    void OnDrawGizmosSelected()\n    {\n        if (!Application.isPlaying) return;\n\n        for (int i = 0; i < numRays; i++)\n        {\n            float angle = minAngle + (i * angleIncrement);\n            Vector2 direction = new Vector2(Mathf.Cos(angle), Mathf.Sin(angle));\n            Vector2 endPos = (Vector2)transform.position + direction * maxRange;\n\n            if (i < ranges.Count && ranges[i] < maxRange)\n            {\n                Gizmos.color = Color.red;\n                Vector2 hitPos = (Vector2)transform.position + direction * ranges[i];\n                Gizmos.DrawLine(transform.position, hitPos);\n            }\n            else\n            {\n                Gizmos.color = Color.green;\n                Gizmos.DrawLine(transform.position, endPos);\n            }\n        }\n    }\n\n    // Public methods for configuration\n    public void SetScanParameters(float minAng, float maxAng, int rays, float range)\n    {\n        minAngle = minAng;\n        maxAngle = maxAng;\n        numRays = rays;\n        maxRange = range;\n        angleIncrement = (maxAngle - minAngle) / (numRays - 1);\n        ranges = new List<float>(new float[numRays]);\n    }\n\n    public void SetPublishRate(float rate)\n    {\n        publishRate = Mathf.Clamp(rate, 1.0f, 100.0f);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"2-3d-lidar-implementation",children:"2. 3D LiDAR Implementation"}),"\n",(0,t.jsx)(e.p,{children:"For more complex humanoid robot perception:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor;\nusing System.Collections.Generic;\n\npublic class UnityLidar3D : MonoBehaviour\n{\n    [Header("3D LiDAR Configuration")]\n    [SerializeField] private string pointCloudTopic = "/point_cloud";\n    [SerializeField] private string frameId = "lidar_frame";\n\n    [Header("Scan Parameters")]\n    [SerializeField] private float horizontalMinAngle = -Mathf.PI; // -180 degrees\n    [SerializeField] private float horizontalMaxAngle = Mathf.PI;  // 180 degrees\n    [SerializeField] private int horizontalRays = 360;\n    [SerializeField] private float verticalMinAngle = -Mathf.PI / 6; // -30 degrees\n    [SerializeField] private float verticalMaxAngle = Mathf.PI / 6;  // 30 degrees\n    [SerializeField] private int verticalRays = 16;\n    [SerializeField] private float maxRange = 50.0f;\n    [SerializeField] private float minRange = 0.1f;\n    [SerializeField] private float publishRate = 10.0f;\n\n    [Header("Physics Settings")]\n    [SerializeField] private LayerMask collisionLayers = -1;\n    [SerializeField] private float noiseLevel = 0.01f;\n\n    private float publishTimer = 0.0f;\n    private ROSConnection rosConnection;\n    private float horizontalAngleIncrement;\n    private float verticalAngleIncrement;\n    private List<PointFieldMsg> pointFields;\n\n    void Start()\n    {\n        rosConnection = ROSConnection.instance;\n        horizontalAngleIncrement = (horizontalMaxAngle - horizontalMinAngle) / (horizontalRays - 1);\n        verticalAngleIncrement = (verticalMaxAngle - verticalMinAngle) / (verticalRays - 1);\n\n        // Define point cloud fields (x, y, z, intensity)\n        pointFields = new List<PointFieldMsg>\n        {\n            new PointFieldMsg { name = "x", offset = 0, datatype = 7, count = 1 }, // FLOAT32\n            new PointFieldMsg { name = "y", offset = 4, datatype = 7, count = 1 }, // FLOAT32\n            new PointFieldMsg { name = "z", offset = 8, datatype = 7, count = 1 }, // FLOAT32\n            new PointFieldMsg { name = "intensity", offset = 12, datatype = 7, count = 1 } // FLOAT32\n        };\n    }\n\n    void Update()\n    {\n        publishTimer += Time.deltaTime;\n        float publishInterval = 1.0f / publishRate;\n\n        if (publishTimer >= publishInterval)\n        {\n            Perform3DLidarScan();\n            publishTimer = 0.0f;\n        }\n    }\n\n    private void Perform3DLidarScan()\n    {\n        if (rosConnection == null) return;\n\n        List<float> pointCloudData = new List<float>();\n\n        // Perform raycasting for each horizontal and vertical angle\n        for (int v = 0; v < verticalRays; v++)\n        {\n            float vertAngle = verticalMinAngle + (v * verticalAngleIncrement);\n\n            for (int h = 0; h < horizontalRays; h++)\n            {\n                float horizAngle = horizontalMinAngle + (h * horizontalAngleIncrement);\n\n                // Convert spherical to Cartesian coordinates\n                float x = Mathf.Cos(vertAngle) * Mathf.Cos(horizAngle);\n                float y = Mathf.Cos(vertAngle) * Mathf.Sin(horizAngle);\n                float z = Mathf.Sin(vertAngle);\n                Vector3 direction = new Vector3(x, y, z);\n\n                if (Physics.Raycast(transform.position, direction, out RaycastHit hit, maxRange, collisionLayers))\n                {\n                    // Add noise to simulate real sensor\n                    float distance = hit.distance + Random.Range(-noiseLevel, noiseLevel);\n\n                    if (distance >= minRange && distance <= maxRange)\n                    {\n                        // Calculate point position\n                        Vector3 point = transform.position + direction * distance;\n\n                        // Transform to sensor frame\n                        Vector3 localPoint = transform.InverseTransformPoint(point);\n\n                        // Add to point cloud data (x, y, z, intensity)\n                        pointCloudData.Add(localPoint.x);\n                        pointCloudData.Add(localPoint.y);\n                        pointCloudData.Add(localPoint.z);\n                        pointCloudData.Add(100.0f); // Intensity value\n                    }\n                }\n            }\n        }\n\n        // Create point cloud message\n        PointCloud2Msg pointCloudMsg = new PointCloud2Msg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = frameId\n            },\n            height = 1, // Unorganized point cloud\n            width = (uint)pointCloudData.Count / 4, // 4 values per point (x, y, z, intensity)\n            fields = pointFields.ToArray(),\n            is_bigendian = false,\n            point_step = 16, // 4 floats * 4 bytes each\n            row_step = (uint)((pointCloudData.Count / 4) * 16),\n            data = FloatArrayToByteArray(pointCloudData.ToArray()),\n            is_dense = true\n        };\n\n        rosConnection.SendUnityMessage(pointCloudTopic, pointCloudMsg);\n    }\n\n    private byte[] FloatArrayToByteArray(float[] floatArray)\n    {\n        byte[] byteArray = new byte[floatArray.Length * 4];\n        for (int i = 0; i < floatArray.Length; i++)\n        {\n            byte[] floatBytes = System.BitConverter.GetBytes(floatArray[i]);\n            System.Buffer.BlockCopy(floatBytes, 0, byteArray, i * 4, 4);\n        }\n        return byteArray;\n    }\n\n    // Visualization in editor\n    void OnDrawGizmosSelected()\n    {\n        if (!Application.isPlaying) return;\n\n        for (int v = 0; v < Mathf.Min(4, verticalRays); v++) // Only show a few vertical rays for clarity\n        {\n            float vertAngle = verticalMinAngle + (v * verticalAngleIncrement);\n\n            for (int h = 0; h < 36; h += 10) // Show every 10th horizontal ray for clarity\n            {\n                float horizAngle = horizontalMinAngle + (h * horizontalAngleIncrement);\n\n                float x = Mathf.Cos(vertAngle) * Mathf.Cos(horizAngle);\n                float y = Mathf.Cos(vertAngle) * Mathf.Sin(horizAngle);\n                float z = Mathf.Sin(vertAngle);\n                Vector3 direction = new Vector3(x, y, z);\n\n                Gizmos.color = Color.blue;\n                Vector3 endPos = transform.position + direction * maxRange;\n                Gizmos.DrawLine(transform.position, endPos);\n            }\n        }\n    }\n\n    public void Set3DScanParameters(\n        float hMin, float hMax, int hRays,\n        float vMin, float vMax, int vRays,\n        float range)\n    {\n        horizontalMinAngle = hMin;\n        horizontalMaxAngle = hMax;\n        horizontalRays = hRays;\n        verticalMinAngle = vMin;\n        verticalMaxAngle = vMax;\n        verticalRays = vRays;\n        maxRange = range;\n\n        horizontalAngleIncrement = (horizontalMaxAngle - horizontalMinAngle) / (horizontalRays - 1);\n        verticalAngleIncrement = (verticalMaxAngle - verticalMinAngle) / (verticalRays - 1);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"imu-sensor-simulation",children:"IMU Sensor Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"1-comprehensive-imu-implementation",children:"1. Comprehensive IMU Implementation"}),"\n",(0,t.jsx)(e.p,{children:"Creating a realistic IMU sensor with proper physics integration:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry;\n\npublic class UnityIMUSensor : MonoBehaviour\n{\n    [Header("IMU Configuration")]\n    [SerializeField] private string imuTopic = "/imu/data";\n    [SerializeField] private string frameId = "imu_frame";\n\n    [Header("Noise Parameters")]\n    [SerializeField] private float orientationNoise = 0.001f;\n    [SerializeField] private float angularVelocityNoise = 0.01f;\n    [SerializeField] private float linearAccelerationNoise = 0.05f;\n\n    [Header("Bias Parameters")]\n    [SerializeField] private float orientationBias = 0.0001f;\n    [SerializeField] private float angularVelocityBias = 0.001f;\n    [SerializeField] private float linearAccelerationBias = 0.01f;\n\n    [Header("Publish Settings")]\n    [SerializeField] private float publishRate = 100.0f;\n\n    private float publishTimer = 0.0f;\n    private ROSConnection rosConnection;\n    private Rigidbody attachedRigidbody;\n    private Vector3 lastAngularVelocity;\n\n    void Start()\n    {\n        rosConnection = ROSConnection.instance;\n        attachedRigidbody = GetComponentInParent<Rigidbody>();\n        if (attachedRigidbody == null)\n            attachedRigidbody = GetComponent<Rigidbody>();\n    }\n\n    void Update()\n    {\n        publishTimer += Time.deltaTime;\n        float publishInterval = 1.0f / publishRate;\n\n        if (publishTimer >= publishInterval)\n        {\n            PublishIMUData();\n            publishTimer = 0.0f;\n        }\n    }\n\n    private void PublishIMUData()\n    {\n        if (rosConnection == null) return;\n\n        // Get orientation (convert Unity to ROS coordinate system)\n        Quaternion orientation = transform.rotation;\n        // Convert from Unity (left-handed) to ROS (right-handed) coordinate system\n        orientation = new Quaternion(orientation.x, orientation.y, -orientation.z, -orientation.w);\n\n        // Apply noise and bias to orientation\n        orientation = ApplyOrientationNoiseAndBias(orientation);\n\n        // Get angular velocity\n        Vector3 angularVelocity = Vector3.zero;\n        if (attachedRigidbody != null)\n        {\n            angularVelocity = attachedRigidbody.angularVelocity;\n        }\n\n        // Apply noise and bias to angular velocity\n        angularVelocity = ApplyAngularVelocityNoiseAndBias(angularVelocity);\n\n        // Get linear acceleration\n        Vector3 linearAcceleration = Physics.gravity;\n        if (attachedRigidbody != null)\n        {\n            // Calculate linear acceleration from velocity change\n            Vector3 currentVelocity = attachedRigidbody.velocity;\n            Vector3 acceleration = (currentVelocity - lastAngularVelocity) / Time.fixedDeltaTime;\n            linearAcceleration += acceleration;\n            lastAngularVelocity = currentVelocity;\n        }\n\n        // Apply noise and bias to linear acceleration\n        linearAcceleration = ApplyLinearAccelerationNoiseAndBias(linearAcceleration);\n\n        // Convert to ROS coordinate system\n        angularVelocity = new Vector3(angularVelocity.x, angularVelocity.y, -angularVelocity.z);\n        linearAcceleration = new Vector3(linearAcceleration.x, linearAcceleration.y, -linearAcceleration.z);\n\n        // Create IMU message\n        ImuMsg imuMsg = new ImuMsg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = frameId\n            },\n            orientation = new QuaternionMsg\n            {\n                x = orientation.x,\n                y = orientation.y,\n                z = orientation.z,\n                w = orientation.w\n            },\n            orientation_covariance = new double[] {\n                orientationNoise * orientationNoise, 0, 0,\n                0, orientationNoise * orientationNoise, 0,\n                0, 0, orientationNoise * orientationNoise\n            },\n            angular_velocity = new Vector3Msg\n            {\n                x = angularVelocity.x,\n                y = angularVelocity.y,\n                z = angularVelocity.z\n            },\n            angular_velocity_covariance = new double[] {\n                angularVelocityNoise * angularVelocityNoise, 0, 0,\n                0, angularVelocityNoise * angularVelocityNoise, 0,\n                0, 0, angularVelocityNoise * angularVelocityNoise\n            },\n            linear_acceleration = new Vector3Msg\n            {\n                x = linearAcceleration.x,\n                y = linearAcceleration.y,\n                z = linearAcceleration.z\n            },\n            linear_acceleration_covariance = new double[] {\n                linearAccelerationNoise * linearAccelerationNoise, 0, 0,\n                0, linearAccelerationNoise * linearAccelerationNoise, 0,\n                0, 0, linearAccelerationNoise * linearAccelerationNoise\n            }\n        };\n\n        rosConnection.SendUnityMessage(imuTopic, imuMsg);\n    }\n\n    private Quaternion ApplyOrientationNoiseAndBias(Quaternion orientation)\n    {\n        float noiseX = Random.Range(-orientationNoise, orientationNoise);\n        float noiseY = Random.Range(-orientationNoise, orientationNoise);\n        float noiseZ = Random.Range(-orientationNoise, orientationNoise);\n        float noiseW = Random.Range(-orientationNoise, orientationNoise);\n\n        float biasX = Random.Range(-orientationBias, orientationBias);\n        float biasY = Random.Range(-orientationBias, orientationBias);\n        float biasZ = Random.Range(-orientationBias, orientationBias);\n        float biasW = Random.Range(-orientationBias, orientationBias);\n\n        orientation.x += noiseX + biasX;\n        orientation.y += noiseY + biasY;\n        orientation.z += noiseZ + biasZ;\n        orientation.w += noiseW + biasW;\n\n        // Normalize quaternion\n        float magnitude = Mathf.Sqrt(orientation.x * orientation.x +\n                                   orientation.y * orientation.y +\n                                   orientation.z * orientation.z +\n                                   orientation.w * orientation.w);\n        if (magnitude > 0)\n        {\n            orientation.x /= magnitude;\n            orientation.y /= magnitude;\n            orientation.z /= magnitude;\n            orientation.w /= magnitude;\n        }\n\n        return orientation;\n    }\n\n    private Vector3 ApplyAngularVelocityNoiseAndBias(Vector3 angularVelocity)\n    {\n        angularVelocity.x += Random.Range(-angularVelocityNoise, angularVelocityNoise) +\n                            Random.Range(-angularVelocityBias, angularVelocityBias);\n        angularVelocity.y += Random.Range(-angularVelocityNoise, angularVelocityNoise) +\n                            Random.Range(-angularVelocityBias, angularVelocityBias);\n        angularVelocity.z += Random.Range(-angularVelocityNoise, angularVelocityNoise) +\n                            Random.Range(-angularVelocityBias, angularVelocityBias);\n        return angularVelocity;\n    }\n\n    private Vector3 ApplyLinearAccelerationNoiseAndBias(Vector3 linearAcceleration)\n    {\n        linearAcceleration.x += Random.Range(-linearAccelerationNoise, linearAccelerationNoise) +\n                               Random.Range(-linearAccelerationBias, linearAccelerationBias);\n        linearAcceleration.y += Random.Range(-linearAccelerationNoise, linearAccelerationNoise) +\n                               Random.Range(-linearAccelerationBias, linearAccelerationBias);\n        linearAcceleration.z += Random.Range(-linearAccelerationNoise, linearAccelerationNoise) +\n                               Random.Range(-linearAccelerationBias, linearAccelerationBias);\n        return linearAcceleration;\n    }\n\n    public void SetNoiseParameters(float orientNoise, float angVelNoise, float linAccNoise)\n    {\n        orientationNoise = orientNoise;\n        angularVelocityNoise = angVelNoise;\n        linearAccelerationNoise = linAccNoise;\n    }\n\n    public void SetBiasParameters(float orientBias, float angVelBias, float linAccBias)\n    {\n        orientationBias = orientBias;\n        angularVelocityBias = angVelBias;\n        linearAccelerationBias = linAccBias;\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"forcetorque-sensor-simulation",children:"Force/Torque Sensor Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"1-joint-forcetorque-sensors",children:"1. Joint Force/Torque Sensors"}),"\n",(0,t.jsx)(e.p,{children:"Simulating force and torque measurements at joints:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry;\nusing System.Collections.Generic;\n\npublic class UnityForceTorqueSensor : MonoBehaviour\n{\n    [Header("Force/Torque Configuration")]\n    [SerializeField] private string wrenchTopic = "/ft_sensor/wrench";\n    [SerializeField] private string frameId = "ft_sensor_frame";\n\n    [Header("Sensor Parameters")]\n    [SerializeField] private float forceNoise = 0.1f;\n    [SerializeField] private float torqueNoise = 0.01f;\n    [SerializeField] private float publishRate = 100.0f;\n\n    [Header("Joint Configuration")]\n    [SerializeField] private ArticulationBody jointBody;\n    [SerializeField] private Transform sensorTransform;\n\n    private float publishTimer = 0.0f;\n    private ROSConnection rosConnection;\n    private Vector3 lastForce;\n    private Vector3 lastTorque;\n\n    void Start()\n    {\n        rosConnection = ROSConnection.instance;\n\n        if (sensorTransform == null)\n            sensorTransform = transform;\n\n        if (jointBody == null)\n            jointBody = GetComponent<ArticulationBody>();\n    }\n\n    void Update()\n    {\n        publishTimer += Time.deltaTime;\n        float publishInterval = 1.0f / publishRate;\n\n        if (publishTimer >= publishInterval)\n        {\n            PublishForceTorqueData();\n            publishTimer = 0.0f;\n        }\n    }\n\n    private void PublishForceTorqueData()\n    {\n        if (rosConnection == null) return;\n\n        // Calculate forces and torques from joint state\n        Vector3 force = Vector3.zero;\n        Vector3 torque = Vector3.zero;\n\n        if (jointBody != null)\n        {\n            // Get joint forces (this is a simplified approach)\n            // In a real implementation, you might use joint constraints or other physics methods\n            force = jointBody.jointForce;\n            torque = jointBody.jointTorque;\n        }\n\n        // Apply noise to simulate real sensor\n        force = ApplyForceNoise(force);\n        torque = ApplyTorqueNoise(torque);\n\n        // Convert to ROS coordinate system\n        force = new Vector3(force.x, force.y, -force.z);\n        torque = new Vector3(torque.x, torque.y, -torque.z);\n\n        // Create wrench message\n        WrenchMsg wrenchMsg = new WrenchMsg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = frameId\n            },\n            force = new Vector3Msg\n            {\n                x = force.x,\n                y = force.y,\n                z = force.z\n            },\n            torque = new Vector3Msg\n            {\n                x = torque.x,\n                y = torque.y,\n                z = torque.z\n            }\n        };\n\n        rosConnection.SendUnityMessage(wrenchTopic, wrenchMsg);\n    }\n\n    private Vector3 ApplyForceNoise(Vector3 force)\n    {\n        force.x += Random.Range(-forceNoise, forceNoise);\n        force.y += Random.Range(-forceNoise, forceNoise);\n        force.z += Random.Range(-forceNoise, forceNoise);\n        return force;\n    }\n\n    private Vector3 ApplyTorqueNoise(Vector3 torque)\n    {\n        torque.x += Random.Range(-torqueNoise, torqueNoise);\n        torque.y += Random.Range(-torqueNoise, torqueNoise);\n        torque.z += Random.Range(-torqueNoise, torqueNoise);\n        return torque;\n    }\n\n    public void SetJointBody(ArticulationBody body)\n    {\n        jointBody = body;\n    }\n\n    public void SetNoiseParameters(float forceN, float torqueN)\n    {\n        forceNoise = forceN;\n        torqueNoise = torqueN;\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"sensor-fusion-and-data-processing",children:"Sensor Fusion and Data Processing"}),"\n",(0,t.jsx)(e.h3,{id:"1-sensor-data-aggregator",children:"1. Sensor Data Aggregator"}),"\n",(0,t.jsx)(e.p,{children:"Combining multiple sensor readings for better perception:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor;\nusing System.Collections.Generic;\n\npublic class SensorDataAggregator : MonoBehaviour\n{\n    [Header("Aggregation Settings")]\n    [SerializeField] private string aggregatedTopic = "/sensors/aggregate";\n    [SerializeField] private float publishRate = 50.0f;\n\n    [Header("Sensor References")]\n    [SerializeField] private List<UnityCameraSensor> cameraSensors = new List<UnityCameraSensor>();\n    [SerializeField] private List<UnityLidar2D> lidarSensors = new List<UnityLidar2D>();\n    [SerializeField] private List<UnityIMUSensor> imuSensors = new List<UnityIMUSensor>();\n\n    private float publishTimer = 0.0f;\n    private ROSConnection rosConnection;\n    private Dictionary<string, object> sensorDataCache = new Dictionary<string, object>();\n\n    void Start()\n    {\n        rosConnection = ROSConnection.instance;\n    }\n\n    void Update()\n    {\n        publishTimer += Time.deltaTime;\n        float publishInterval = 1.0f / publishRate;\n\n        if (publishTimer >= publishInterval)\n        {\n            AggregateSensorData();\n            publishTimer = 0.0f;\n        }\n    }\n\n    private void AggregateSensorData()\n    {\n        if (rosConnection == null) return;\n\n        // This is a simplified example - in practice, you would collect real-time data\n        // from sensors and aggregate it into a comprehensive message\n\n        // For now, we\'ll just send a status message indicating sensor availability\n        DiagnosticArrayMsg diagnosticArray = new DiagnosticArrayMsg\n        {\n            header = new Unity.Robotics.ROSTCPConnector.MessageTypes.Std.HeaderMsg\n            {\n                stamp = new Unity.Robotics.ROSTCPConnector.MessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1000000000)\n                },\n                frame_id = "sensor_aggregator"\n            },\n            status = new Unity.Robotics.ROSTCPConnector.MessageTypes.Diagnostic.DiagnosticStatusMsg[]\n            {\n                CreateDiagnosticStatus("cameras", cameraSensors.Count > 0, "Camera sensors active"),\n                CreateDiagnosticStatus("lidars", lidarSensors.Count > 0, "LiDAR sensors active"),\n                CreateDiagnosticStatus("imus", imuSensors.Count > 0, "IMU sensors active")\n            }\n        };\n\n        rosConnection.SendUnityMessage(aggregatedTopic, diagnosticArray);\n    }\n\n    private Unity.Robotics.ROSTCPConnector.MessageTypes.Diagnostic.DiagnosticStatusMsg CreateDiagnosticStatus(\n        string name, bool isActive, string message)\n    {\n        return new Unity.Robotics.ROSTCPConnector.MessageTypes.Diagnostic.DiagnosticStatusMsg\n        {\n            name = name,\n            level = isActive ? (byte)0 : (byte)2, // 0=OK, 2=ERROR\n            message = message,\n            hardware_id = "unity_sensor_system"\n        };\n    }\n\n    public void AddCameraSensor(UnityCameraSensor sensor)\n    {\n        if (!cameraSensors.Contains(sensor))\n            cameraSensors.Add(sensor);\n    }\n\n    public void AddLidarSensor(UnityLidar2D sensor)\n    {\n        if (!lidarSensors.Contains(sensor))\n            lidarSensors.Add(sensor);\n    }\n\n    public void AddIMUSensor(UnityIMUSensor sensor)\n    {\n        if (!imuSensors.Contains(sensor))\n            imuSensors.Add(sensor);\n    }\n\n    public void RemoveCameraSensor(UnityCameraSensor sensor)\n    {\n        cameraSensors.Remove(sensor);\n    }\n\n    public void RemoveLidarSensor(UnityLidar2D sensor)\n    {\n        lidarSensors.Remove(sensor);\n    }\n\n    public void RemoveIMUSensor(UnityIMUSensor sensor)\n    {\n        imuSensors.Remove(sensor);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"sensor-calibration-and-validation",children:"Sensor Calibration and Validation"}),"\n",(0,t.jsx)(e.h3,{id:"1-sensor-calibration-system",children:"1. Sensor Calibration System"}),"\n",(0,t.jsx)(e.p,{children:"Implementing sensor calibration for accurate simulation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections;\n\npublic class SensorCalibrationSystem : MonoBehaviour\n{\n    [Header("Calibration Settings")]\n    [SerializeField] private float calibrationDuration = 5.0f;\n    [SerializeField] private bool autoCalibrateOnStart = true;\n\n    [Header("Calibration Results")]\n    [SerializeField] private bool isCalibrated = false;\n    [SerializeField] private float calibrationAccuracy = 0.0f;\n\n    private UnityIMUSensor imuSensor;\n    private UnityCameraSensor cameraSensor;\n    private UnityLidar2D lidarSensor;\n\n    void Start()\n    {\n        FindSensors();\n\n        if (autoCalibrateOnStart)\n        {\n            StartCoroutine(CalibrateSensors());\n        }\n    }\n\n    private void FindSensors()\n    {\n        imuSensor = GetComponent<UnityIMUSensor>();\n        cameraSensor = GetComponent<UnityCameraSensor>();\n        lidarSensor = GetComponent<UnityLidar2D>();\n    }\n\n    private IEnumerator CalibrateSensors()\n    {\n        Debug.Log("Starting sensor calibration...");\n\n        // Calibrate IMU (measure bias while stationary)\n        if (imuSensor != null)\n        {\n            yield return StartCoroutine(CalibrateIMU());\n        }\n\n        // Calibrate other sensors as needed\n        // For camera: measure intrinsic parameters\n        // For LiDAR: verify range and accuracy\n\n        isCalibrated = true;\n        calibrationAccuracy = 0.95f; // Example accuracy after calibration\n\n        Debug.Log($"Sensor calibration completed. Accuracy: {calibrationAccuracy * 100:F1}%");\n    }\n\n    private IEnumerator CalibrateIMU()\n    {\n        Debug.Log("Calibrating IMU sensor...");\n\n        // Collect baseline readings while sensor should be stationary\n        Vector3 sumAngularVelocity = Vector3.zero;\n        Vector3 sumLinearAcceleration = Vector3.zero;\n        int sampleCount = 0;\n        float startTime = Time.time;\n\n        while (Time.time - startTime < calibrationDuration)\n        {\n            if (imuSensor != null)\n            {\n                // In a real implementation, you would access raw sensor data\n                // For now, we\'ll just wait and simulate calibration\n                sampleCount++;\n            }\n            yield return new WaitForEndOfFrame();\n        }\n\n        Debug.Log("IMU calibration completed.");\n    }\n\n    public bool IsCalibrated()\n    {\n        return isCalibrated;\n    }\n\n    public float GetCalibrationAccuracy()\n    {\n        return calibrationAccuracy;\n    }\n\n    public void StartCalibration()\n    {\n        if (!isCalibrated)\n        {\n            StartCoroutine(CalibrateSensors());\n        }\n    }\n\n    public void ResetCalibration()\n    {\n        isCalibrated = false;\n        calibrationAccuracy = 0.0f;\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"1-sensor-update-manager",children:"1. Sensor Update Manager"}),"\n",(0,t.jsx)(e.p,{children:"Optimizing sensor updates for better performance:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing System.Collections.Generic;\n\npublic class SensorUpdateManager : MonoBehaviour\n{\n    [System.Serializable]\n    public class SensorUpdateConfig\n    {\n        public MonoBehaviour sensor;\n        public float updateRate = 30.0f;\n        public bool isActive = true;\n        public float lastUpdateTime = 0.0f;\n    }\n\n    [SerializeField] private List<SensorUpdateConfig> sensorConfigs = new List<SensorUpdateConfig>();\n\n    void Update()\n    {\n        float currentTime = Time.time;\n\n        foreach (var config in sensorConfigs)\n        {\n            if (config.isActive && config.sensor != null)\n            {\n                float updateInterval = 1.0f / config.updateRate;\n\n                if ((currentTime - config.lastUpdateTime) >= updateInterval)\n                {\n                    // Trigger sensor update (this would depend on sensor implementation)\n                    // In practice, you might use reflection or interfaces to call update methods\n                    config.lastUpdateTime = currentTime;\n                }\n            }\n        }\n    }\n\n    public void AddSensor(MonoBehaviour sensor, float updateRate = 30.0f)\n    {\n        SensorUpdateConfig config = new SensorUpdateConfig\n        {\n            sensor = sensor,\n            updateRate = updateRate,\n            isActive = true,\n            lastUpdateTime = Time.time\n        };\n\n        sensorConfigs.Add(config);\n    }\n\n    public void RemoveSensor(MonoBehaviour sensor)\n    {\n        sensorConfigs.RemoveAll(config => config.sensor == sensor);\n    }\n\n    public void SetSensorRate(MonoBehaviour sensor, float newRate)\n    {\n        var config = sensorConfigs.Find(c => c.sensor == sensor);\n        if (config != null)\n        {\n            config.updateRate = newRate;\n        }\n    }\n\n    public void SetSensorActive(MonoBehaviour sensor, bool active)\n    {\n        var config = sensorConfigs.Find(c => c.sensor == sensor);\n        if (config != null)\n        {\n            config.isActive = active;\n        }\n    }\n}\n"})}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"With comprehensive sensor simulation implemented, you now have a complete digital twin system in Unity with realistic camera, LiDAR, IMU, and force/torque sensors. These sensors provide the perception capabilities needed for humanoid robot simulation and control."}),"\n",(0,t.jsx)(e.p,{children:"In the next section, we'll explore how to integrate these sensors with AI perception systems and create a complete perception pipeline for your humanoid robots in the Unity digital twin environment."})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var a=i(6540);const t={},o=a.createContext(t);function r(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);