"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6155],{2524:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>f,frontMatter:()=>o,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"module-3-ai-robot-brain/practical-exercises/vslam-implementation","title":"VSLAM Implementation for Humanoid Robots","description":"Overview","source":"@site/docs/module-3-ai-robot-brain/practical-exercises/vslam-implementation.md","sourceDirName":"module-3-ai-robot-brain/practical-exercises","slug":"/module-3-ai-robot-brain/practical-exercises/vslam-implementation","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/practical-exercises/vslam-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-3-ai-robot-brain/practical-exercises/vslam-implementation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Nav2: Bipedal Path Planning for Humanoid Robots","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/navigation/nav2-bipedal-navigation"},"next":{"title":"Photorealistic Simulation and Synthetic Data Generation","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/practical-exercises/photorealistic-simulation"}}');var r=a(4848),i=a(8453);const o={sidebar_position:5},s="VSLAM Implementation for Humanoid Robots",l={},m=[{value:"Overview",id:"overview",level:2},{value:"VSLAM Architecture for Humanoids",id:"vslam-architecture-for-humanoids",level:2},{value:"1. System Architecture Overview",id:"1-system-architecture-overview",level:3},{value:"2. Humanoid-Specific Considerations",id:"2-humanoid-specific-considerations",level:3},{value:"Isaac Sim VSLAM Environment Setup",id:"isaac-sim-vslam-environment-setup",level:2},{value:"1. Creating VSLAM Test Environments",id:"1-creating-vslam-test-environments",level:3},{value:"2. Stereo Camera Configuration for VSLAM",id:"2-stereo-camera-configuration-for-vslam",level:3},{value:"Isaac ROS VSLAM Integration",id:"isaac-ros-vslam-integration",level:2},{value:"1. Isaac ROS Visual Inertial Odometry Setup",id:"1-isaac-ros-visual-inertial-odometry-setup",level:3},{value:"2. Feature Tracking and Mapping",id:"2-feature-tracking-and-mapping",level:3},{value:"Humanoid-Specific VSLAM Considerations",id:"humanoid-specific-vslam-considerations",level:2},{value:"1. Walking Motion Compensation",id:"1-walking-motion-compensation",level:3},{value:"Performance Optimization and Real-time Considerations",id:"performance-optimization-and-real-time-considerations",level:2},{value:"1. Multi-threaded VSLAM Architecture",id:"1-multi-threaded-vslam-architecture",level:3},{value:"Launch Files for Complete VSLAM System",id:"launch-files-for-complete-vslam-system",level:2},{value:"1. Complete VSLAM Launch File",id:"1-complete-vslam-launch-file",level:3},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vslam-implementation-for-humanoid-robots",children:"VSLAM Implementation for Humanoid Robots"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Visual Simultaneous Localization and Mapping (VSLAM) is crucial for humanoid robots operating in unknown environments. This section covers the implementation of hardware-accelerated VSLAM using NVIDIA Isaac Sim and Isaac ROS, focusing on the unique challenges of bipedal locomotion and dynamic environments."}),"\n",(0,r.jsx)(n.p,{children:"VSLAM for humanoid robots must handle the challenges of walking motion, changing viewpoints, and the need for real-time performance while maintaining accuracy for safe navigation."}),"\n",(0,r.jsx)(n.h2,{id:"vslam-architecture-for-humanoids",children:"VSLAM Architecture for Humanoids"}),"\n",(0,r.jsx)(n.h3,{id:"1-system-architecture-overview",children:"1. System Architecture Overview"}),"\n",(0,r.jsx)(n.p,{children:"The VSLAM system for humanoid robots consists of several interconnected components:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    HUMANOID VSLAM SYSTEM                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   SENSORS   \u2502    \u2502  VSLAM CORE \u2502    \u2502   MAPPING   \u2502     \u2502\n\u2502  \u2502             \u2502\u2500\u2500\u2500\u25b6\u2502             \u2502\u2500\u2500\u2500\u25b6\u2502             \u2502     \u2502\n\u2502  \u2502 \u2022 Stereo    \u2502    \u2502 \u2022 Feature   \u2502    \u2502 \u2022 Global    \u2502     \u2502\n\u2502  \u2502   Cameras   \u2502    \u2502   Extract.  \u2502    \u2502   Map       \u2502     \u2502\n\u2502  \u2502 \u2022 IMU       \u2502    \u2502 \u2022 Tracking  \u2502    \u2502 \u2022 Local     \u2502     \u2502\n\u2502  \u2502 \u2022 LIDAR     \u2502    \u2502 \u2022 Loop Clos.\u2502    \u2502   Map       \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-humanoid-specific-considerations",children:"2. Humanoid-Specific Considerations"}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots present unique challenges for VSLAM:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Motion"}),": Walking motion creates complex camera trajectories"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Height Variation"}),": Head height changes during walking affect viewpoint"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Balance Constraints"}),": VSLAM must not compromise robot stability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Requirements"}),": Processing must keep pace with locomotion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-floor Navigation"}),": 3D mapping for stair navigation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"isaac-sim-vslam-environment-setup",children:"Isaac Sim VSLAM Environment Setup"}),"\n",(0,r.jsx)(n.h3,{id:"1-creating-vslam-test-environments",children:"1. Creating VSLAM Test Environments"}),"\n",(0,r.jsx)(n.p,{children:"Setting up environments specifically for VSLAM testing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vslam_environment.py - Create environments for VSLAM testing\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import create_prim\nfrom pxr import Gf, Sdf, UsdGeom, UsdLux\nimport numpy as np\n\ndef create_vslam_test_environment(world: World):\n    """\n    Create a VSLAM test environment with features suitable for localization\n    """\n    stage = omni.usd.get_context().get_stage()\n\n    # Create multi-level environment with stairs\n    create_multi_level_environment(stage)\n\n    # Add distinctive visual features for tracking\n    add_vslam_features(stage)\n\n    # Configure lighting for consistent feature detection\n    configure_tracking_lighting(stage)\n\n    print("VSLAM test environment created")\n\ndef create_multi_level_environment(stage):\n    """\n    Create a multi-level environment with stairs for 3D mapping\n    """\n    # Ground floor\n    ground_floor = UsdGeom.Cube.Define(stage, Sdf.Path("/World/GroundFloor"))\n    ground_floor.CreateSizeAttr(20.0)\n    xform = UsdGeom.Xformable(ground_floor.GetPrim())\n    xform.AddTranslateOp().Set((0, 0, -0.05))\n    xform.AddScaleOp().Set((10.0, 8.0, 0.1))\n\n    # First floor platform\n    first_floor = UsdGeom.Cube.Define(stage, Sdf.Path("/World/FirstFloor"))\n    first_floor.CreateSizeAttr(6.0)\n    xform = UsdGeom.Xformable(first_floor.GetPrim())\n    xform.AddTranslateOp().Set((0, 0, 1.0))  # 1 meter high\n    xform.AddScaleOp().Set((3.0, 2.5, 0.1))\n\n    # Stairs connecting floors\n    create_stairs(stage)\n\n    # Walls for structure\n    create_walls(stage)\n\ndef create_stairs(stage):\n    """\n    Create stairs for multi-floor navigation\n    """\n    num_steps = 8\n    step_height = 0.15  # 15cm per step (human scale)\n    step_depth = 0.3   # 30cm depth\n    step_width = 1.2   # 1.2m width\n\n    for i in range(num_steps):\n        step_path = Sdf.Path(f"/World/Stairs/Step_{i}")\n        step = UsdGeom.Cube.Define(stage, step_path)\n        step.CreateSizeAttr(1.0)\n\n        xform = UsdGeom.Xformable(step.GetPrim())\n        xform.AddTranslateOp().Set((\n            -1.0 + i * step_depth * 0.5,  # Gradually move forward\n            0,\n            i * step_height + step_height / 2\n        ))\n        xform.AddScaleOp().Set((step_depth, step_width, step_height))\n\ndef create_walls(stage):\n    """\n    Create walls with distinctive features for VSLAM\n    """\n    wall_configs = [\n        # Main room walls\n        {"position": (5, 0, 1.5), "scale": (0.1, 8, 3), "name": "RightWall"},\n        {"position": (-5, 0, 1.5), "scale": (0.1, 8, 3), "name": "LeftWall"},\n        {"position": (0, 4, 1.5), "scale": (10, 0.1, 3), "name": "FrontWall"},\n        {"position": (0, -4, 1.5), "scale": (10, 0.1, 3), "name": "BackWall"},\n        # Interior walls with doors\n        {"position": (0, -1, 1.5), "scale": (3, 0.1, 3), "name": "InteriorWall1"},\n    ]\n\n    for config in wall_configs:\n        wall_path = Sdf.Path(f"/World/Walls/{config[\'name\']}")\n        wall = UsdGeom.Cube.Define(stage, wall_path)\n        wall.CreateSizeAttr(1.0)\n\n        xform = UsdGeom.Xformable(wall.GetPrim())\n        xform.AddTranslateOp().Set(config["position"])\n        xform.AddScaleOp().Set(config["scale"])\n\ndef add_vslam_features(stage):\n    """\n    Add distinctive visual features for robust tracking\n    """\n    # Add textured patterns on walls\n    feature_configs = [\n        # Wall patterns\n        {"position": (4, 2, 1.5), "type": "pattern", "name": "Pattern1"},\n        {"position": (-4, -2, 1.5), "type": "pattern", "name": "Pattern2"},\n        {"position": (0, 3, 1.5), "type": "pattern", "name": "Pattern3"},\n        # Objects with distinctive features\n        {"position": (2, -1, 0.5), "type": "object", "name": "Bookshelf"},\n        {"position": (-2, 1, 0.4), "type": "object", "name": "Plant"},\n    ]\n\n    for config in feature_configs:\n        if config["type"] == "pattern":\n            # Create textured pattern\n            pattern_path = Sdf.Path(f"/World/Features/{config[\'name\']}")\n            pattern = UsdGeom.Cube.Define(stage, pattern_path)\n            pattern.CreateSizeAttr(0.5)\n\n            xform = UsdGeom.Xformable(pattern.GetPrim())\n            xform.AddTranslateOp().Set(config["position"])\n            xform.AddScaleOp().Set((0.8, 0.01, 1.0))  # Flat pattern\n\ndef configure_tracking_lighting(stage):\n    """\n    Configure lighting for consistent feature tracking\n    """\n    # Main overhead lighting\n    main_light = UsdLux.DistantLight.Define(stage, Sdf.Path("/World/MainTrackingLight"))\n    main_light.CreateIntensityAttr(2500)\n    main_light.CreateColorAttr(Gf.Vec3f(0.98, 0.98, 0.95))  # Daylight white\n    main_light.AddRotateXYZOp().Set((70, 0, 0))\n\n    # Fill lights to reduce shadows\n    fill_light1 = UsdLux.SphereLight.Define(stage, Sdf.Path("/World/FillLight1"))\n    fill_light1.CreateIntensityAttr(500)\n    fill_light1.CreateColorAttr(Gf.Vec3f(0.9, 0.9, 1.0))\n    fill_light1.AddTranslateOp().Set((3, 2, 2))\n\n    fill_light2 = UsdLux.SphereLight.Define(stage, Sdf.Path("/World/FillLight2"))\n    fill_light2.CreateIntensityAttr(500)\n    fill_light2.CreateColorAttr(Gf.Vec3f(0.9, 0.9, 1.0))\n    fill_light2.AddTranslateOp().Set((-3, -2, 2))\n\n# Example usage\ndef setup_vslam_world():\n    """\n    Complete setup for VSLAM testing world\n    """\n    world = World(stage_units_in_meters=1.0)\n    configure_humanoid_physics()  # From previous module\n    create_vslam_test_environment(world)\n    return world\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-stereo-camera-configuration-for-vslam",children:"2. Stereo Camera Configuration for VSLAM"}),"\n",(0,r.jsx)(n.p,{children:"Setting up stereo cameras optimized for VSLAM:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# stereo_camera_setup.py - Configure stereo cameras for VSLAM\nimport omni\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_prim\nfrom pxr import Gf, Sdf, UsdGeom\nimport numpy as np\n\ndef setup_vslam_stereo_cameras(world: World):\n    """\n    Set up stereo cameras optimized for VSLAM applications\n    """\n    # Create stereo camera rig\n    create_camera_rig()\n\n    # Configure left camera\n    left_camera = Camera(\n        prim_path="/World/HumanoidHead/LeftCamera",\n        frequency=30,  # 30 Hz for VSLAM\n        resolution=(640, 480),  # Good balance of speed/quality\n        position=(0.0, -0.05, 0.0),  # 10cm baseline\n        orientation=(0, 0, 0, 1),  # Default orientation\n        fov=60,  # 60 degree field of view\n        clipping_range=(0.1, 10.0)  # 10m max range\n    )\n\n    # Configure right camera\n    right_camera = Camera(\n        prim_path="/World/HumanoidHead/RightCamera",\n        frequency=30,\n        resolution=(640, 480),\n        position=(0.0, 0.05, 0.0),  # 10cm baseline\n        orientation=(0, 0, 0, 1),\n        fov=60,\n        clipping_range=(0.1, 10.0)\n    )\n\n    # Add cameras to world\n    world.scene.add(left_camera)\n    world.scene.add(right_camera)\n\n    print("VSLAM stereo cameras configured")\n\ndef create_camera_rig():\n    """\n    Create a physical camera rig to maintain calibration\n    """\n    stage = omni.usd.get_context().get_stage()\n\n    # Create camera mount\n    mount_path = Sdf.Path("/World/HumanoidHead/CameraMount")\n    mount = UsdGeom.Cube.Define(stage, mount_path)\n    mount.CreateSizeAttr(0.1)  # 10cm cube mount\n\n    xform = UsdGeom.Xformable(mount.GetPrim())\n    xform.AddTranslateOp().Set((0.1, 0, 0))  # Position on head\n    xform.AddScaleOp().Set((0.05, 0.1, 0.05))  # Small mount\n\ndef get_stereo_calibration():\n    """\n    Return stereo calibration parameters for VSLAM\n    """\n    # Camera intrinsic parameters\n    fx = 320.0  # Focal length in pixels\n    fy = 320.0\n    cx = 320.0  # Principal point\n    cy = 240.0\n\n    # Camera matrix\n    K = np.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1]\n    ])\n\n    # Distortion coefficients (assuming calibrated)\n    D = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n\n    # Stereo baseline (distance between cameras)\n    baseline = 0.1  # 10cm\n\n    # Projection matrix for stereo\n    P = np.array([\n        [fx, 0, cx, -fx * baseline],\n        [0, fy, cy, 0],\n        [0, 0, 1, 0]\n    ])\n\n    return {\n        \'K\': K,\n        \'D\': D,\n        \'baseline\': baseline,\n        \'P\': P,\n        \'resolution\': (640, 480),\n        \'fov\': 60\n    }\n\nclass StereoVSLAMInterface:\n    """\n    Interface for stereo VSLAM with Isaac Sim cameras\n    """\n    def __init__(self, world: World):\n        self.world = world\n        self.calibration = get_stereo_calibration()\n        self.left_image = None\n        self.right_image = None\n        self.frame_timestamp = None\n\n    def capture_stereo_pair(self):\n        """\n        Capture synchronized stereo image pair\n        """\n        # In Isaac Sim, this would access the camera data\n        # For simulation, we\'ll create synchronized captures\n        current_time = self.world.current_time\n        self.frame_timestamp = current_time\n\n        # This is where we\'d get the actual camera data\n        # left_img = self.left_camera.get_rgb()\n        # right_img = self.right_camera.get_rgb()\n\n        # For this example, return dummy data\n        dummy_img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        return dummy_img, dummy_img  # Return left, right images\n\n    def get_camera_pose(self):\n        """\n        Get current camera pose for VSLAM\n        """\n        # This would return the current pose of the stereo rig\n        # In a real implementation, this would interface with the physics engine\n        return np.eye(4)  # Identity for now\n'})}),"\n",(0,r.jsx)(n.h2,{id:"isaac-ros-vslam-integration",children:"Isaac ROS VSLAM Integration"}),"\n",(0,r.jsx)(n.h3,{id:"1-isaac-ros-visual-inertial-odometry-setup",children:"1. Isaac ROS Visual Inertial Odometry Setup"}),"\n",(0,r.jsx)(n.p,{children:"Implementing hardware-accelerated VIO using Isaac ROS:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# isaac_ros_vio_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, TwistStamped\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport tf2_ros\nfrom geometry_msgs.msg import TransformStamped\n\nclass IsaacROSVisualInertialOdometry(Node):\n    """\n    Isaac ROS Visual Inertial Odometry for humanoid robots\n    """\n    def __init__(self):\n        super().__init__(\'isaac_ros_vio\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # TF broadcaster for poses\n        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\n\n        # Create subscribers for stereo and IMU\n        self.left_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_rect\',\n            self.left_image_callback,\n            5\n        )\n\n        self.right_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/right/image_rect\',\n            self.right_image_callback,\n            5\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.left_camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/stereo/left/camera_info\',\n            self.left_camera_info_callback,\n            10\n        )\n\n        self.right_camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/stereo/right/camera_info\',\n            self.right_camera_info_callback,\n            10\n        )\n\n        # Publishers\n        self.odom_pub = self.create_publisher(\n            Odometry,\n            \'/visual_inertial_odom\',\n            10\n        )\n\n        self.pose_pub = self.create_publisher(\n            PoseStamped,\n            \'/vio/pose\',\n            10\n        )\n\n        # VIO state\n        self.left_image = None\n        self.right_image = None\n        self.imu_data = None\n        self.camera_info = None\n        self.initialized = False\n\n        # Pose tracking\n        self.current_pose = np.eye(4)\n        self.previous_pose = np.eye(4)\n\n        # IMU bias tracking\n        self.accel_bias = np.zeros(3)\n        self.gyro_bias = np.zeros(3)\n\n        # Performance tracking\n        self.frame_count = 0\n        self.start_time = self.get_clock().now()\n\n        self.get_logger().info(\'Isaac ROS Visual Inertial Odometry initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left camera image"""\n        try:\n            self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n            self.process_vio()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing left image: {e}\')\n\n    def right_image_callback(self, msg):\n        """Process right camera image"""\n        try:\n            self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n            self.process_vio()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing right image: {e}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.imu_data = msg\n\n    def left_camera_info_callback(self, msg):\n        """Store left camera info"""\n        self.camera_info = msg\n\n    def right_camera_info_callback(self, msg):\n        """Store right camera info"""\n        # We\'ll use left camera info as primary\n\n    def process_vio(self):\n        """\n        Process VIO using Isaac ROS acceleration\n        """\n        if (self.left_image is not None and\n            self.right_image is not None and\n            self.imu_data is not None and\n            self.camera_info is not None):\n\n            # In Isaac ROS, this would call the actual VIO algorithm\n            # which uses GPU acceleration for feature tracking and IMU integration\n            pose_increment = self.accelerated_vio_processing()\n\n            if pose_increment is not None:\n                # Update pose with the increment\n                self.previous_pose = self.current_pose.copy()\n                self.current_pose = self.current_pose @ pose_increment\n\n                # Publish results\n                self.publish_odometry()\n                self.publish_pose()\n                self.broadcast_transform()\n\n                # Update performance metrics\n                self.frame_count += 1\n                current_time = self.get_clock().now()\n                elapsed = (current_time - self.start_time).nanoseconds / 1e9\n                if elapsed > 0 and self.frame_count % 30 == 0:\n                    fps = self.frame_count / elapsed\n                    self.get_logger().info(f\'VIO processing at {fps:.2f} FPS\')\n\n    def accelerated_vio_processing(self):\n        """\n        Hardware-accelerated VIO processing using Isaac ROS\n        """\n        # This is a placeholder - in real Isaac ROS implementation:\n        # 1. GPU-accelerated feature extraction\n        # 2. Real-time stereo matching\n        # 3. IMU integration with visual features\n        # 4. Loop closure detection\n\n        # For simulation, create a small pose increment based on IMU\n        if self.imu_data:\n            # Extract angular velocity and linear acceleration\n            gyro = np.array([\n                self.imu_data.angular_velocity.x,\n                self.imu_data.angular_velocity.y,\n                self.imu_data.angular_velocity.z\n            ])\n\n            accel = np.array([\n                self.imu_data.linear_acceleration.x,\n                self.imu_data.linear_acceleration.y,\n                self.imu_data.linear_acceleration.z\n            ])\n\n            # Remove bias (simplified)\n            gyro_corrected = gyro - self.gyro_bias\n            accel_corrected = accel - self.accel_bias\n\n            # Integrate to get pose change (simplified)\n            dt = 1.0 / 30.0  # 30 FPS\n            angle_change = gyro_corrected * dt\n            linear_change = accel_corrected * dt * dt * 0.5  # s = 0.5*a*t^2\n\n            # Create transformation matrix from changes\n            pose_inc = self.create_pose_increment(angle_change, linear_change)\n\n            # Update bias estimates (simplified)\n            self.update_bias_estimates(gyro, accel)\n\n            return pose_inc\n\n        # Default: small forward movement\n        pose_inc = np.eye(4)\n        pose_inc[0, 3] = 0.01  # Move forward 1cm\n        return pose_inc\n\n    def create_pose_increment(self, angle_change, linear_change):\n        """\n        Create pose increment matrix from angle and linear changes\n        """\n        pose_inc = np.eye(4)\n\n        # Convert angle changes to rotation matrix (small angle approximation)\n        rx, ry, rz = angle_change\n        rot_x = np.array([[1, 0, 0], [0, np.cos(rx), -np.sin(rx)], [0, np.sin(rx), np.cos(rx)]])\n        rot_y = np.array([[np.cos(ry), 0, np.sin(ry)], [0, 1, 0], [-np.sin(ry), 0, np.cos(ry)]])\n        rot_z = np.array([[np.cos(rz), -np.sin(rz), 0], [np.sin(rz), np.cos(rz), 0], [0, 0, 1]])\n\n        rotation = rot_z @ rot_y @ rot_x\n        pose_inc[:3, :3] = rotation\n\n        # Add linear translation\n        pose_inc[:3, 3] = linear_change\n\n        return pose_inc\n\n    def update_bias_estimates(self, gyro, accel):\n        """\n        Update bias estimates using simple averaging\n        """\n        # Simple bias estimation (in practice, more sophisticated methods are used)\n        alpha = 0.01  # Learning rate\n        self.gyro_bias = self.gyro_bias * (1 - alpha) + gyro * alpha\n        self.accel_bias = self.accel_bias * (1 - alpha) + accel * alpha\n\n    def publish_odometry(self):\n        """\n        Publish odometry message\n        """\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'odom\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set pose\n        pos = self.current_pose[:3, 3]\n        odom_msg.pose.pose.position.x = float(pos[0])\n        odom_msg.pose.pose.position.y = float(pos[1])\n        odom_msg.pose.pose.position.z = float(pos[2])\n\n        # Convert rotation matrix to quaternion\n        quat = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\n        odom_msg.pose.pose.orientation.x = quat[0]\n        odom_msg.pose.pose.orientation.y = quat[1]\n        odom_msg.pose.pose.orientation.z = quat[2]\n        odom_msg.pose.pose.orientation.w = quat[3]\n\n        # Set velocity (approximate from pose difference)\n        dt = 1.0 / 30.0  # 30 FPS\n        pos_diff = pos - self.previous_pose[:3, 3]\n        velocity = pos_diff / dt\n\n        odom_msg.twist.twist.linear.x = float(velocity[0])\n        odom_msg.twist.twist.linear.y = float(velocity[1])\n        odom_msg.twist.twist.linear.z = float(velocity[2])\n\n        self.odom_pub.publish(odom_msg)\n\n    def publish_pose(self):\n        """\n        Publish pose message\n        """\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'odom\'\n\n        pos = self.current_pose[:3, 3]\n        pose_msg.pose.position.x = float(pos[0])\n        pose_msg.pose.position.y = float(pos[1])\n        pose_msg.pose.position.z = float(pos[2])\n\n        quat = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\n        pose_msg.pose.orientation.x = quat[0]\n        pose_msg.pose.orientation.y = quat[1]\n        pose_msg.pose.orientation.z = quat[2]\n        pose_msg.pose.orientation.w = quat[3]\n\n        self.pose_pub.publish(pose_msg)\n\n    def broadcast_transform(self):\n        """\n        Broadcast TF transform\n        """\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = \'odom\'\n        t.child_frame_id = \'base_link\'\n\n        pos = self.current_pose[:3, 3]\n        t.transform.translation.x = float(pos[0])\n        t.transform.translation.y = float(pos[1])\n        t.transform.translation.z = float(pos[2])\n\n        quat = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def rotation_matrix_to_quaternion(self, rotation_matrix):\n        """\n        Convert rotation matrix to quaternion\n        """\n        trace = np.trace(rotation_matrix)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            w = 0.25 * s\n            x = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n            y = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n            z = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n        else:\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\n                s = np.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\n                w = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n                x = 0.25 * s\n                y = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                z = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\n                s = np.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\n                w = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n                x = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                y = 0.25 * s\n                z = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\n                w = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n                x = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n                y = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n                z = 0.25 * s\n\n        return [x, y, z, w]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vio_node = IsaacROSVisualInertialOdometry()\n\n    try:\n        rclpy.spin(vio_node)\n    except KeyboardInterrupt:\n        vio_node.get_logger().info(\'Shutting down Isaac ROS VIO\')\n    finally:\n        vio_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-feature-tracking-and-mapping",children:"2. Feature Tracking and Mapping"}),"\n",(0,r.jsx)(n.p,{children:"Implementing feature-based mapping for humanoid VSLAM:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# feature_tracking_mapping.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PointStamped, PoseStamped\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom collections import deque\n\nclass IsaacROSFeatureTracker(Node):\n    """\n    Feature tracker for Isaac ROS VSLAM system\n    """\n    def __init__(self):\n        super().__init__(\'isaac_ros_feature_tracker\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_rect\',\n            self.image_callback,\n            5\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/stereo/left/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publishers\n        self.feature_pub = self.create_publisher(\n            MarkerArray,\n            \'/vslam/features\',\n            10\n        )\n\n        self.map_pub = self.create_publisher(\n            MarkerArray,\n            \'/vslam/map\',\n            10\n        )\n\n        # Initialize feature tracking\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.previous_features = None\n        self.current_features = None\n        self.feature_ids = {}\n        self.feature_poses = {}  # 3D positions of features\n        self.next_feature_id = 0\n\n        # Feature tracking parameters\n        self.max_features = 1000\n        self.min_distance = 20\n        self.quality_level = 0.01\n        self.block_size = 7\n\n        # Pose tracking\n        self.current_pose = np.eye(4)\n        self.frame_count = 0\n\n        # Feature history for mapping\n        self.feature_history = deque(maxlen=100)\n\n        self.get_logger().info(\'Isaac ROS Feature Tracker initialized\')\n\n    def image_callback(self, msg):\n        """\n        Process image for feature tracking\n        """\n        try:\n            # Convert ROS Image to OpenCV format\n            gray = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n\n            # Track features\n            self.track_features(gray, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def camera_info_callback(self, msg):\n        """\n        Store camera calibration information\n        """\n        self.camera_matrix = np.array(msg.k).reshape(3, 2)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def track_features(self, gray, header):\n        """\n        Track features using Lucas-Kanade optical flow\n        """\n        if self.camera_matrix is None:\n            return\n\n        # Detect new features if we don\'t have enough\n        if self.current_features is None or len(self.current_features) < 100:\n            new_features = self.detect_features(gray)\n            if new_features is not None:\n                if self.current_features is None:\n                    self.current_features = new_features\n                else:\n                    # Combine with existing features\n                    self.current_features = np.vstack([self.current_features, new_features])\n\n        # Track existing features\n        if self.previous_features is not None and len(self.previous_features) > 0:\n            # Use Lucas-Kanade tracker\n            next_features, status, error = cv2.calcOpticalFlowPyrLK(\n                self.previous_gray, gray,\n                self.previous_features.reshape(-1, 1, 2),\n                None,\n                winSize=(21, 21),\n                maxLevel=3,\n                criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01)\n            )\n\n            # Filter good features\n            good_new = next_features[status == 1]\n            good_old = self.previous_features[status == 1]\n\n            # Update current features\n            self.current_features = good_new\n\n            # Estimate motion from feature correspondences\n            if len(good_new) >= 8:  # Need minimum for pose estimation\n                self.estimate_camera_motion(good_old, good_new, header)\n\n        # Store for next iteration\n        self.previous_gray = gray.copy()\n        self.previous_features = self.current_features.copy() if self.current_features is not None else None\n\n        # Publish features\n        self.publish_features(header)\n\n        self.frame_count += 1\n\n    def detect_features(self, gray):\n        """\n        Detect new features to track\n        """\n        # Use Shi-Tomasi corner detection\n        corners = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=self.max_features,\n            qualityLevel=self.quality_level,\n            minDistance=self.min_distance,\n            blockSize=self.block_size\n        )\n\n        return corners\n\n    def estimate_camera_motion(self, old_features, new_features, header):\n        """\n        Estimate camera motion from feature correspondences\n        """\n        # In a real VSLAM system, this would involve:\n        # 1. Essential matrix estimation\n        # 2. Pose extraction\n        # 3. Scale estimation (for monocular, needs other sensors)\n        # 4. Bundle adjustment\n\n        # For this example, we\'ll use a simplified approach\n        if len(old_features) >= 8:\n            # Calculate fundamental matrix\n            fundamental_matrix, mask = cv2.findFundamentalMat(\n                old_features, new_features, cv2.RANSAC, 4, 0.999\n            )\n\n            # Use camera matrix to compute essential matrix\n            if self.camera_matrix is not None:\n                essential_matrix = self.camera_matrix.T @ fundamental_matrix @ self.camera_matrix\n\n                # Decompose essential matrix to get rotation and translation\n                _, rotation, translation, _ = cv2.recoverPose(essential_matrix, old_features, new_features, self.camera_matrix)\n\n                # Create pose increment\n                pose_inc = np.eye(4)\n                pose_inc[:3, :3] = rotation\n                pose_inc[:3, 3] = translation.flatten() * 0.1  # Scale factor for simulation\n\n                # Update current pose\n                self.current_pose = self.current_pose @ pose_inc\n\n                # Update 3D positions of tracked features\n                self.update_feature_positions(old_features, new_features)\n\n    def update_feature_positions(self, old_features, new_features):\n        """\n        Update 3D positions of tracked features using triangulation\n        """\n        if self.frame_count > 1:  # Need at least 2 poses\n            for i in range(min(len(old_features), len(new_features))):\n                old_pt = old_features[i].flatten()\n                new_pt = new_features[i].flatten()\n\n                # Triangulate 3D point\n                point_3d = self.triangulate_point(old_pt, new_pt)\n\n                if point_3d is not None:\n                    # Store or update feature position\n                    feature_key = (int(old_pt[0]), int(old_pt[1]))\n                    self.feature_poses[feature_key] = point_3d\n\n    def triangulate_point(self, old_pt, new_pt):\n        """\n        Triangulate 3D point from stereo correspondences\n        """\n        # This is a simplified version - in real implementation:\n        # 1. Use known camera poses\n        # 2. Apply proper triangulation algorithm\n        # 3. Use stereo baseline information\n\n        # For simulation, return a plausible 3D point\n        depth = 2.0  # Assume 2m depth\n        if self.camera_matrix is not None:\n            fx = self.camera_matrix[0, 0]\n            fy = self.camera_matrix[1, 1]\n            cx = self.camera_matrix[0, 2]\n            cy = self.camera_matrix[1, 2]\n\n            x = (old_pt[0] - cx) * depth / fx\n            y = (old_pt[1] - cy) * depth / fy\n            z = depth\n\n            return np.array([x, y, z])\n\n        return None\n\n    def publish_features(self, header):\n        """\n        Publish tracked features as visualization markers\n        """\n        if self.current_features is not None:\n            marker_array = MarkerArray()\n\n            for i, pt in enumerate(self.current_features):\n                if i < 50:  # Limit visualization\n                    marker = Marker()\n                    marker.header = header\n                    marker.ns = "features"\n                    marker.id = i\n                    marker.type = Marker.SPHERE\n                    marker.action = Marker.ADD\n\n                    # Position (simplified - would be 3D in real system)\n                    marker.pose.position.x = float(pt[0])\n                    marker.pose.position.y = float(pt[1])\n                    marker.pose.position.z = 1.0  # Fixed height for visualization\n\n                    marker.pose.orientation.w = 1.0\n                    marker.scale.x = 0.05\n                    marker.scale.y = 0.05\n                    marker.scale.z = 0.05\n\n                    marker.color.r = 1.0\n                    marker.color.g = 0.0\n                    marker.color.b = 0.0\n                    marker.color.a = 1.0\n\n                    marker_array.markers.append(marker)\n\n            self.feature_pub.publish(marker_array)\n\n    def publish_map(self):\n        """\n        Publish the 3D map of features\n        """\n        marker_array = MarkerArray()\n\n        for i, (key, pos) in enumerate(list(self.feature_poses.items())[:100]):  # Limit visualization\n            marker = Marker()\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.header.frame_id = "map"\n            marker.ns = "map_points"\n            marker.id = i\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n\n            marker.pose.position.x = float(pos[0])\n            marker.pose.position.y = float(pos[1])\n            marker.pose.position.z = float(pos[2])\n\n            marker.pose.orientation.w = 1.0\n            marker.scale.x = 0.02\n            marker.scale.y = 0.02\n            marker.scale.z = 0.02\n\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n            marker.color.a = 1.0\n\n            marker_array.markers.append(marker)\n\n        self.map_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    feature_tracker = IsaacROSFeatureTracker()\n\n    # Create timer to periodically publish map\n    feature_tracker.create_timer(1.0, feature_tracker.publish_map)\n\n    try:\n        rclpy.spin(feature_tracker)\n    except KeyboardInterrupt:\n        feature_tracker.get_logger().info(\'Shutting down feature tracker\')\n    finally:\n        feature_tracker.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"humanoid-specific-vslam-considerations",children:"Humanoid-Specific VSLAM Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"1-walking-motion-compensation",children:"1. Walking Motion Compensation"}),"\n",(0,r.jsx)(n.p,{children:"Compensating for humanoid walking motion in VSLAM:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# walking_motion_compensation.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Vector3Stamped\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom scipy import signal\n\nclass WalkingMotionCompensation(Node):\n    \"\"\"\n    Compensate for humanoid walking motion in VSLAM\n    \"\"\"\n    def __init__(self):\n        super().__init__('walking_motion_compensation')\n\n        # Subscribers\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Publishers\n        self.compensated_imu_pub = self.create_publisher(\n            Imu,\n            '/imu/compensated',\n            10\n        )\n\n        self.walk_state_pub = self.create_publisher(\n            Float64,\n            '/walking_state',\n            10\n        )\n\n        # Walking pattern detection\n        self.imu_buffer = []\n        self.max_buffer_size = 100\n        self.walk_state = 0.0  # 0.0 = standing, 1.0 = walking\n\n        # Walking parameters\n        self.leg_joints = ['left_hip_pitch', 'left_knee', 'left_ankle_pitch',\n                          'right_hip_pitch', 'right_knee', 'right_ankle_pitch']\n        self.joint_positions = {}\n        self.joint_velocities = {}\n\n        # Butterworth filter for motion separation\n        self.filter_b, self.filter_a = signal.butter(4, 0.1, btype='high', fs=100.0)\n\n        # Walking pattern detection\n        self.walking_detected = False\n        self.walking_confidence = 0.0\n\n        self.get_logger().info('Walking Motion Compensation initialized')\n\n    def imu_callback(self, msg):\n        \"\"\"\n        Process IMU data for walking compensation\n        \"\"\"\n        # Add to buffer for pattern analysis\n        self.imu_buffer.append({\n            'linear_acceleration': np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ]),\n            'angular_velocity': np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ]),\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n        })\n\n        # Keep buffer at max size\n        if len(self.imu_buffer) > self.max_buffer_size:\n            self.imu_buffer.pop(0)\n\n        # Detect walking patterns\n        self.detect_walking_pattern()\n\n        # Compensate IMU data\n        compensated_imu = self.compensate_imu_data(msg)\n\n        # Publish compensated data\n        self.compensated_imu_pub.publish(compensated_imu)\n\n        # Publish walking state\n        walk_state_msg = Float64()\n        walk_state_msg.data = self.walk_state\n        self.walk_state_pub.publish(walk_state_msg)\n\n    def joint_state_callback(self, msg):\n        \"\"\"\n        Process joint state data for walking analysis\n        \"\"\"\n        for i, name in enumerate(msg.name):\n            if name in self.leg_joints:\n                if i < len(msg.position):\n                    self.joint_positions[name] = msg.position[i]\n                if i < len(msg.velocity):\n                    self.joint_velocities[name] = msg.velocity[i]\n\n    def detect_walking_pattern(self):\n        \"\"\"\n        Detect walking pattern from IMU and joint data\n        \"\"\"\n        if len(self.imu_buffer) < 50:  # Need sufficient data\n            return\n\n        # Analyze IMU data for walking patterns\n        linear_accs = np.array([d['linear_acceleration'] for d in self.imu_buffer])\n\n        # Look for periodic patterns in vertical acceleration (due to footsteps)\n        vertical_acc = linear_accs[:, 2]  # Z-axis (vertical)\n\n        # Apply FFT to detect periodic components\n        fft_result = np.fft.fft(vertical_acc)\n        freqs = np.fft.fftfreq(len(vertical_acc), d=0.01)  # Assuming 100Hz IMU\n\n        # Look for typical walking frequencies (1-3 Hz)\n        walking_freq_range = (freqs > 1.0) & (freqs < 3.0)\n        walking_energy = np.sum(np.abs(fft_result[walking_freq_range]))\n\n        # Also check for joint patterns\n        joint_activity = self.calculate_joint_activity()\n\n        # Combine indicators for walking confidence\n        self.walking_confidence = min(1.0, (walking_energy * 0.001 + joint_activity * 0.5))\n\n        # Update walk state with hysteresis\n        if self.walking_confidence > 0.7 and not self.walking_detected:\n            self.walking_detected = True\n            self.get_logger().info('Walking detected')\n        elif self.walking_confidence < 0.3 and self.walking_detected:\n            self.walking_detected = False\n            self.get_logger().info('Walking stopped')\n\n        self.walk_state = self.walking_confidence\n\n    def calculate_joint_activity(self):\n        \"\"\"\n        Calculate joint activity as indicator of walking\n        \"\"\"\n        activity = 0.0\n        for joint in self.leg_joints:\n            if joint in self.joint_velocities:\n                activity += abs(self.joint_velocities[joint])\n\n        return min(1.0, activity / 10.0)  # Normalize\n\n    def compensate_imu_data(self, original_imu):\n        \"\"\"\n        Compensate IMU data for walking motion\n        \"\"\"\n        compensated_imu = Imu()\n        compensated_imu.header = original_imu.header\n\n        # In a real system, this would involve:\n        # 1. Estimating walking motion from joint data\n        # 2. Predicting IMU readings due to walking\n        # 3. Subtracting predicted walking motion\n\n        # For this example, we'll apply a simplified compensation\n        # based on detected walking state\n\n        walking_factor = 1.0 - self.walk_state  # Reduce trust in IMU when walking\n\n        # Copy orientation (this is what we trust most)\n        compensated_imu.orientation = original_imu.orientation\n        compensated_imu.orientation_covariance = original_imu.orientation_covariance\n\n        # Scale linear acceleration based on walking confidence\n        compensated_imu.linear_acceleration.x = original_imu.linear_acceleration.x * walking_factor\n        compensated_imu.linear_acceleration.y = original_imu.linear_acceleration.y * walking_factor\n        compensated_imu.linear_acceleration.z = original_imu.linear_acceleration.z * walking_factor\n\n        # Increase covariance when walking (less reliable)\n        walking_cov_scale = 1.0 + self.walk_state * 2.0\n        compensated_imu.linear_acceleration_covariance = [\n            original_imu.linear_acceleration_covariance[i] * walking_cov_scale\n            for i in range(9)\n        ]\n\n        # Similar for angular velocity\n        compensated_imu.angular_velocity.x = original_imu.angular_velocity.x * walking_factor\n        compensated_imu.angular_velocity.y = original_imu.angular_velocity.y * walking_factor\n        compensated_imu.angular_velocity.z = original_imu.angular_velocity.z * walking_factor\n\n        return compensated_imu\n\ndef main(args=None):\n    rclpy.init(args=args)\n    motion_comp = WalkingMotionCompensation()\n\n    try:\n        rclpy.spin(motion_comp)\n    except KeyboardInterrupt:\n        motion_comp.get_logger().info('Shutting down walking motion compensation')\n    finally:\n        motion_comp.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization-and-real-time-considerations",children:"Performance Optimization and Real-time Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"1-multi-threaded-vslam-architecture",children:"1. Multi-threaded VSLAM Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Implementing a multi-threaded architecture for real-time VSLAM:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# multithreaded_vslam.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom threading import Thread, Lock\nfrom queue import Queue\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass MultithreadedVSLAM(Node):\n    """\n    Multi-threaded VSLAM implementation for real-time performance\n    """\n    def __init__(self):\n        super().__init__(\'multithreaded_vslam\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create queues for inter-thread communication\n        self.image_queue = Queue(maxsize=5)\n        self.imu_queue = Queue(maxsize=10)\n        self.feature_queue = Queue(maxsize=10)\n\n        # Create locks for shared data\n        self.pose_lock = Lock()\n        self.feature_lock = Lock()\n\n        # Shared state\n        self.current_pose = np.eye(4)\n        self.features = []\n        self.is_running = True\n\n        # Create subscribers\n        self.left_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_rect\',\n            self.image_callback,\n            5\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Start processing threads\n        self.feature_thread = Thread(target=self.feature_extraction_loop)\n        self.tracking_thread = Thread(target=self.tracking_loop)\n        self.mapping_thread = Thread(target=self.mapping_loop)\n\n        self.feature_thread.start()\n        self.tracking_thread.start()\n        self.mapping_thread.start()\n\n        self.get_logger().info(\'Multithreaded VSLAM initialized\')\n\n    def image_callback(self, msg):\n        """\n        Image callback - adds to processing queue\n        """\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n            # Add to queue with timestamp\n            image_data = {\n                \'image\': cv_image,\n                \'timestamp\': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\n                \'header\': msg.header\n            }\n            if not self.image_queue.full():\n                self.image_queue.put(image_data)\n        except:\n            pass  # Drop frame if queue is full\n\n    def imu_callback(self, msg):\n        """\n        IMU callback - adds to processing queue\n        """\n        try:\n            imu_data = {\n                \'linear_acceleration\': np.array([\n                    msg.linear_acceleration.x,\n                    msg.linear_acceleration.y,\n                    msg.linear_acceleration.z\n                ]),\n                \'angular_velocity\': np.array([\n                    msg.angular_velocity.x,\n                    msg.angular_velocity.y,\n                    msg.angular_velocity.z\n                ]),\n                \'timestamp\': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9,\n                \'orientation\': msg.orientation\n            }\n            if not self.imu_queue.full():\n                self.imu_queue.put(imu_data)\n        except:\n            pass  # Drop if queue is full\n\n    def feature_extraction_loop(self):\n        """\n        Thread for feature extraction\n        """\n        while self.is_running:\n            try:\n                # Get image from queue\n                if not self.image_queue.empty():\n                    image_data = self.image_queue.get(timeout=0.01)\n                    cv_image = image_data[\'image\']\n\n                    # Extract features\n                    features = self.extract_features(cv_image)\n\n                    # Add to feature queue\n                    feature_data = {\n                        \'features\': features,\n                        \'timestamp\': image_data[\'timestamp\'],\n                        \'header\': image_data[\'header\']\n                    }\n                    if not self.feature_queue.full():\n                        self.feature_queue.put(feature_data)\n                else:\n                    # Small sleep to prevent busy waiting\n                    import time\n                    time.sleep(0.001)\n            except:\n                continue\n\n    def tracking_loop(self):\n        """\n        Thread for feature tracking and pose estimation\n        """\n        previous_features = None\n        previous_image = None\n\n        while self.is_running:\n            try:\n                if not self.feature_queue.empty():\n                    feature_data = self.feature_queue.get(timeout=0.01)\n                    current_features = feature_data[\'features\']\n\n                    if previous_features is not None and len(previous_features) > 0:\n                        # Track features and estimate motion\n                        tracked_features, status = self.track_features(\n                            previous_features, current_features\n                        )\n\n                        if len(tracked_features) > 10:  # Minimum for pose estimation\n                            pose_increment = self.estimate_motion(\n                                previous_features[status.flatten() == 1],\n                                tracked_features\n                            )\n\n                            # Update pose\n                            with self.pose_lock:\n                                self.current_pose = self.current_pose @ pose_increment\n\n                    previous_features = current_features.copy()\n                else:\n                    # Small sleep to prevent busy waiting\n                    import time\n                    time.sleep(0.001)\n            except:\n                continue\n\n    def mapping_loop(self):\n        """\n        Thread for map building and optimization\n        """\n        while self.is_running:\n            # In a real implementation, this would:\n            # 1. Build 3D map from tracked features\n            # 2. Perform loop closure detection\n            # 3. Optimize map using bundle adjustment\n            # 4. Manage map size and memory usage\n\n            # For this example, just sleep and occasionally log\n            import time\n            time.sleep(0.1)\n\n    def extract_features(self, image):\n        """\n        Extract features from image (simplified)\n        """\n        # Use ORB features for efficiency\n        orb = cv2.ORB_create(nfeatures=500)\n        keypoints, descriptors = orb.detectAndCompute(image, None)\n\n        if keypoints is not None:\n            # Convert keypoints to numpy array\n            features = np.array([kp.pt for kp in keypoints], dtype=np.float32)\n            return features\n        else:\n            return np.empty((0, 2), dtype=np.float32)\n\n    def track_features(self, previous_features, current_features):\n        """\n        Track features between frames\n        """\n        # This is a simplified version\n        # In real implementation, use optical flow or descriptor matching\n        if len(previous_features) > 0 and len(current_features) > 0:\n            # For this example, return all current features with status\n            status = np.ones((len(current_features), 1), dtype=np.uint8)\n            return current_features, status\n        else:\n            return np.empty((0, 2), dtype=np.float32), np.empty((0, 1), dtype=np.uint8)\n\n    def estimate_motion(self, old_features, new_features):\n        """\n        Estimate camera motion from feature correspondences\n        """\n        if len(old_features) >= 8:\n            # Calculate transformation matrix\n            transform, mask = cv2.estimateAffinePartial2D(\n                old_features, new_features, method=cv2.RANSAC\n            )\n\n            if transform is not None:\n                # Convert to 4x4 transformation matrix\n                pose_inc = np.eye(4)\n                pose_inc[:2, :2] = transform[:2, :2]\n                pose_inc[:2, 3] = transform[:2, 2]\n                return pose_inc\n\n        # Default: small identity transformation\n        return np.eye(4)\n\n    def destroy_node(self):\n        """\n        Clean shutdown of threads\n        """\n        self.is_running = False\n        if self.feature_thread.is_alive():\n            self.feature_thread.join()\n        if self.tracking_thread.is_alive():\n            self.tracking_thread.join()\n        if self.mapping_thread.is_alive():\n            self.mapping_thread.join()\n\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vslam_node = MultithreadedVSLAM()\n\n    try:\n        rclpy.spin(vslam_node)\n    except KeyboardInterrupt:\n        vslam_node.get_logger().info(\'Shutting down multithreaded VSLAM\')\n    finally:\n        vslam_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"launch-files-for-complete-vslam-system",children:"Launch Files for Complete VSLAM System"}),"\n",(0,r.jsx)(n.h3,{id:"1-complete-vslam-launch-file",children:"1. Complete VSLAM Launch File"}),"\n",(0,r.jsx)(n.p,{children:"Creating launch files for the complete VSLAM system:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# launch/humanoid_vslam.launch.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    camera_namespace = LaunchConfiguration('camera_namespace')\n    robot_namespace = LaunchConfiguration('robot_namespace')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='true',\n        description='Use simulation time'\n    )\n\n    declare_camera_namespace = DeclareLaunchArgument(\n        'camera_namespace',\n        default_value='/stereo',\n        description='Namespace for stereo camera topics'\n    )\n\n    declare_robot_namespace = DeclareLaunchArgument(\n        'robot_namespace',\n        default_value='',\n        description='Robot namespace'\n    )\n\n    # Isaac ROS Visual Inertial Odometry\n    vio_node = Node(\n        package='isaac_ros_visual_inertial_odometry',\n        executable='visual_inertial_odometry_node',\n        name='visual_inertial_odometry',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'enable_debug_mode': False,\n            'publish_tf': True,\n            'world_frame': 'odom',\n            'base_frame': 'base_link'\n        }],\n        remappings=[\n            ('left/image_rect', [camera_namespace, '/left/image_rect']),\n            ('left/camera_info', [camera_namespace, '/left/camera_info']),\n            ('right/image_rect', [camera_namespace, '/right/image_rect']),\n            ('right/camera_info', [camera_namespace, '/right/camera_info']),\n            ('imu', '/imu/data'),\n            ('visual_odometry', '/visual_odometry'),\n        ]\n    )\n\n    # Isaac ROS Apriltag detector (for loop closure)\n    apriltag_node = Node(\n        package='isaac_ros_apriltag',\n        executable='apriltag_node',\n        name='apriltag',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'family': 'tag36h11',\n            'max_tags': 64,\n            'tag36h11_size': 0.16\n        }],\n        remappings=[\n            ('image', [camera_namespace, '/left/image_rect']),\n            ('camera_info', [camera_namespace, '/left/camera_info']),\n            ('detections', '/apriltag/detections'),\n        ]\n    )\n\n    # Walking motion compensation\n    walk_compensation = Node(\n        package='humanoid_vslam',\n        executable='walking_motion_compensation',\n        name='walking_motion_compensation',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        remappings=[\n            ('/imu/data', '/imu/data'),\n            ('/joint_states', '/joint_states'),\n            ('/imu/compensated', '/imu/compensated'),\n        ]\n    )\n\n    # Feature tracker\n    feature_tracker = Node(\n        package='humanoid_vslam',\n        executable='feature_tracker',\n        name='feature_tracker',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        remappings=[\n            ('/stereo/left/image_rect', [camera_namespace, '/left/image_rect']),\n            ('/stereo/left/camera_info', [camera_namespace, '/left/camera_info']),\n            ('/vslam/features', '/vslam/features'),\n            ('/vslam/map', '/vslam/map'),\n        ]\n    )\n\n    # Multi-threaded VSLAM processor\n    multithreaded_vslam = Node(\n        package='humanoid_vslam',\n        executable='multithreaded_vslam',\n        name='multithreaded_vslam',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        remappings=[\n            ('/stereo/left/image_rect', [camera_namespace, '/left/image_rect']),\n            ('/stereo/right/image_rect', [camera_namespace, '/right/image_rect']),\n            ('/imu/data', '/imu/compensated'),  # Use compensated IMU\n        ]\n    )\n\n    # Robot state publisher for visualization\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }]\n    )\n\n    # Static transform publisher for camera to base link\n    camera_tf_publisher = Node(\n        package='tf2_ros',\n        executable='static_transform_publisher',\n        name='camera_tf_publisher',\n        arguments=['0.1', '0', '0.1', '0', '0', '0', 'base_link', 'camera_link']\n    )\n\n    return LaunchDescription([\n        declare_use_sim_time,\n        declare_camera_namespace,\n        declare_robot_namespace,\n\n        robot_state_publisher,\n        camera_tf_publisher,\n\n        # Start basic nodes first\n        vio_node,\n        apriltag_node,\n\n        # Then start humanoid-specific nodes\n        walk_compensation,\n        feature_tracker,\n        multithreaded_vslam,\n    ])\n"})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"With the VSLAM implementation complete, you're ready to move on to configuring Nav2 for bipedal navigation. The next section will cover adapting the Nav2 stack for humanoid robots, including footstep planning, balance-aware path planning, and multi-floor navigation specific to bipedal locomotion."})]})}function f(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>s});var t=a(6540);const r={},i=t.createContext(r);function o(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);