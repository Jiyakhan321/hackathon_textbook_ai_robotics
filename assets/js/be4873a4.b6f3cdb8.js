"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1161],{5935:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vision-language-action/action-execution/vision-action-integration","title":"Vision-Action Integration","description":"Introduction","source":"@site/docs/module-4-vision-language-action/action-execution/vision-action-integration.md","sourceDirName":"module-4-vision-language-action/action-execution","slug":"/module-4-vision-language-action/action-execution/vision-action-integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/action-execution/vision-action-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/action-execution/vision-action-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation and Interaction Systems","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/action-execution/manipulation-interaction"},"next":{"title":"Multimodal Perception Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/practical-exercises/multimodal-perception"}}');var o=t(4848),s=t(8453);const a={sidebar_position:4},r="Vision-Action Integration",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Object Detection Pipelines",id:"object-detection-pipelines",level:2},{value:"RGB-Based Object Detection",id:"rgb-based-object-detection",level:3},{value:"Depth-Based Object Detection",id:"depth-based-object-detection",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:3},{value:"Perception \u2192 Decision \u2192 Action Loop",id:"perception--decision--action-loop",level:2},{value:"Real-Time Processing Pipeline",id:"real-time-processing-pipeline",level:3},{value:"Decision Making Under Uncertainty",id:"decision-making-under-uncertainty",level:3},{value:"Humanoid Manipulation Logic",id:"humanoid-manipulation-logic",level:2},{value:"Grasping Strategy Selection",id:"grasping-strategy-selection",level:3},{value:"Reachability Analysis",id:"reachability-analysis",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"ROS 2 Nodes and Topics",id:"ros-2-nodes-and-topics",level:3},{value:"Message Definitions and Service Calls",id:"message-definitions-and-service-calls",level:3},{value:"Diagrams and Architecture",id:"diagrams-and-architecture",level:2},{value:"Vision-Action Integration Architecture",id:"vision-action-integration-architecture",level:3},{value:"Perception-Decision-Action Loop Timing",id:"perception-decision-action-loop-timing",level:3},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 1: Implement a Simple Grasp Planner",id:"exercise-1-implement-a-simple-grasp-planner",level:3},{value:"Exercise 2: Multi-Modal Object Tracking",id:"exercise-2-multi-modal-object-tracking",level:3},{value:"Exercise 3: ROS 2 Vision-Action Bridge",id:"exercise-3-ros-2-vision-action-bridge",level:3},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vision-action-integration",children:"Vision-Action Integration"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Vision-action integration represents the crucial bridge between perceiving the environment and executing physical actions in humanoid robots. This module explores the complex pipeline that transforms visual input into coordinated robotic movements, enabling robots to interact meaningfully with their surroundings."}),"\n",(0,o.jsx)(n.h2,{id:"object-detection-pipelines",children:"Object Detection Pipelines"}),"\n",(0,o.jsx)(n.h3,{id:"rgb-based-object-detection",children:"RGB-Based Object Detection"}),"\n",(0,o.jsx)(n.p,{children:"RGB cameras provide rich color information that enables sophisticated object recognition. Modern approaches leverage deep learning models like YOLO (You Only Look Once), SSD (Single Shot Detector), and Mask R-CNN for real-time object detection."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\nclass RGBObjectDetector:\n    def __init__(self, model_path):\n        self.model = torch.load(model_path)\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize((416, 416))\n        ])\n\n    def detect_objects(self, image):\n        # Preprocess image\n        input_tensor = self.transform(image).unsqueeze(0)\n\n        # Run inference\n        with torch.no_grad():\n            predictions = self.model(input_tensor)\n\n        # Post-process detections\n        boxes = []\n        scores = []\n        classes = []\n\n        for pred in predictions[0]:\n            if pred['score'] > 0.5:\n                boxes.append(pred['bbox'])\n                scores.append(pred['score'])\n                classes.append(pred['class'])\n\n        return boxes, scores, classes\n"})}),"\n",(0,o.jsx)(n.h3,{id:"depth-based-object-detection",children:"Depth-Based Object Detection"}),"\n",(0,o.jsx)(n.p,{children:"Depth sensors like Intel RealSense, Kinect, or stereo cameras provide crucial 3D spatial information. Depth data enables precise distance measurements, volume estimation, and 3D scene reconstruction."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import open3d as o3d\nimport numpy as np\n\nclass DepthObjectDetector:\n    def __init__(self, camera_intrinsics):\n        self.intrinsics = camera_intrinsics\n\n    def detect_3d_objects(self, rgb_image, depth_image):\n        # Convert to point cloud\n        rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n            o3d.geometry.Image(rgb_image),\n            o3d.geometry.Image(depth_image),\n            convert_rgb_to_intensity=False\n        )\n\n        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n            rgbd_image,\n            self.intrinsics\n        )\n\n        # Extract geometric features\n        plane_model, inliers = pcd.segment_plane(\n            distance_threshold=0.01,\n            ransac_n=3,\n            num_iterations=1000\n        )\n\n        # Separate objects from ground plane\n        object_points = pcd.select_by_index(inliers, invert=True)\n\n        # Cluster objects using DBSCAN\n        labels = np.array(object_points.cluster_dbscan(\n            eps=0.02,\n            min_points=10\n        ))\n\n        return self.extract_object_properties(object_points, labels)\n\n    def extract_object_properties(self, point_cloud, labels):\n        objects = []\n        for label in set(labels):\n            if label == -1:\n                continue  # Skip noise points\n\n            cluster = point_cloud.select_by_index(np.where(labels == label)[0])\n            center = cluster.get_center()\n            dimensions = cluster.get_max_bound() - cluster.get_min_bound()\n\n            objects.append({\n                'center': center,\n                'dimensions': dimensions,\n                'points': len(cluster.points)\n            })\n\n        return objects\n"})}),"\n",(0,o.jsx)(n.h3,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,o.jsx)(n.p,{children:"Combining RGB and depth information creates a more robust perception system. Multi-modal fusion techniques include early fusion (combining raw data), late fusion (combining decisions), and intermediate fusion (at feature level)."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class MultiModalFusion:\n    def __init__(self):\n        self.rgb_detector = RGBObjectDetector('yolo_weights.pt')\n        self.depth_detector = DepthObjectDetector(self.get_camera_intrinsics())\n\n    def fused_detection(self, rgb_image, depth_image):\n        # Get detections from both modalities\n        rgb_boxes, rgb_scores, rgb_classes = self.rgb_detector.detect_objects(rgb_image)\n        depth_objects = self.depth_detector.detect_3d_objects(rgb_image, depth_image)\n\n        # Associate RGB and depth detections\n        fused_objects = []\n        for i, (box, score, cls) in enumerate(zip(rgb_boxes, rgb_scores, rgb_classes)):\n            box_center_2d = np.array([(box[0] + box[2]) / 2, (box[1] + box[3]) / 2])\n\n            closest_depth_obj = None\n            min_distance = float('inf')\n\n            for depth_obj in depth_objects:\n                # Project 3D point to 2D\n                projected_2d = self.project_3d_to_2d(depth_obj['center'])\n                distance = np.linalg.norm(box_center_2d - projected_2d)\n\n                if distance < min_distance and distance < 20:  # Threshold in pixels\n                    min_distance = distance\n                    closest_depth_obj = depth_obj\n\n            if closest_depth_obj:\n                fused_objects.append({\n                    'class': cls,\n                    'confidence': score,\n                    'bbox_2d': box,\n                    'position_3d': closest_depth_obj['center'],\n                    'dimensions_3d': closest_depth_obj['dimensions']\n                })\n\n        return fused_objects\n\n    def project_3d_to_2d(self, point_3d):\n        # Simplified projection using camera intrinsics\n        # Actual implementation would use proper camera matrix\n        return np.array([point_3d[0] / point_3d[2], point_3d[1] / point_3d[2]])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"perception--decision--action-loop",children:"Perception \u2192 Decision \u2192 Action Loop"}),"\n",(0,o.jsx)(n.h3,{id:"real-time-processing-pipeline",children:"Real-Time Processing Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The perception-decision-action loop operates in real-time, continuously updating the robot's understanding of its environment and adjusting its behavior accordingly."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rospy\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nimport threading\nimport time\n\nclass PerceptionDecisionActionLoop:\n    def __init__(self):\n        # Initialize ROS node\n        rospy.init_node('vision_action_loop')\n\n        # Publishers and subscribers\n        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.image_callback)\n        self.depth_sub = rospy.Subscriber('/camera/depth/image_raw', Image, self.depth_callback)\n        self.command_pub = rospy.Publisher('/robot/manipulation_command', PoseStamped, queue_size=10)\n\n        # Object detection components\n        self.fusion_detector = MultiModalFusion()\n        self.llm_planner = LLMPlanner()\n\n        # State variables\n        self.latest_rgb = None\n        self.latest_depth = None\n        self.current_target = None\n\n        # Start processing loop\n        self.processing_thread = threading.Thread(target=self.main_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n    def image_callback(self, msg):\n        self.latest_rgb = self.ros_image_to_cv2(msg)\n\n    def depth_callback(self, msg):\n        self.latest_depth = self.ros_image_to_cv2(msg)\n\n    def main_loop(self):\n        rate = rospy.Rate(10)  # 10 Hz\n\n        while not rospy.is_shutdown():\n            if self.latest_rgb is not None and self.latest_depth is not None:\n                # Perception: Detect objects\n                detections = self.fusion_detector.fused_detection(\n                    self.latest_rgb.copy(),\n                    self.latest_depth.copy()\n                )\n\n                # Decision: Determine appropriate action\n                action_plan = self.make_decision(detections)\n\n                # Action: Execute manipulation\n                if action_plan:\n                    self.execute_action(action_plan)\n\n            rate.sleep()\n\n    def make_decision(self, detections):\n        if not detections:\n            return None\n\n        # Determine priority target based on task\n        target_object = self.select_target(detections)\n\n        if target_object and self.is_reachable(target_object):\n            # Generate manipulation plan\n            manipulation_pose = self.calculate_manipulation_pose(target_object)\n\n            return {\n                'target': target_object,\n                'manipulation_pose': manipulation_pose,\n                'action_type': 'grasp'\n            }\n\n        return None\n\n    def execute_action(self, action_plan):\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = rospy.Time.now()\n        pose_msg.header.frame_id = \"base_link\"\n\n        pose_msg.pose.position.x = action_plan['manipulation_pose']['x']\n        pose_msg.pose.position.y = action_plan['manipulation_pose']['y']\n        pose_msg.pose.position.z = action_plan['manipulation_pose']['z']\n\n        # Set orientation for grasping\n        pose_msg.pose.orientation.w = 1.0\n\n        self.command_pub.publish(pose_msg)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"decision-making-under-uncertainty",children:"Decision Making Under Uncertainty"}),"\n",(0,o.jsx)(n.p,{children:"Robotic systems must handle uncertainty in perception, environment dynamics, and sensor noise. Bayesian approaches and probabilistic reasoning help manage this uncertainty."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass ProbabilisticDecisionMaker:\n    def __init__(self):\n        self.object_poses = {}  # Track estimated poses with uncertainty\n        self.confidence_threshold = 0.7\n\n    def update_belief(self, detection_list):\n        for detection in detection_list:\n            obj_id = detection['class']\n            measured_pose = detection['position_3d']\n            measurement_covariance = self.estimate_measurement_uncertainty(detection)\n\n            if obj_id in self.object_poses:\n                # Kalman filter update\n                predicted_pose, predicted_cov = self.object_poses[obj_id]\n\n                # Innovation\n                innovation = measured_pose - predicted_pose\n                innovation_cov = predicted_cov + measurement_covariance\n\n                # Kalman gain\n                kalman_gain = predicted_cov @ np.linalg.inv(innovation_cov)\n\n                # Updated estimate\n                updated_pose = predicted_pose + kalman_gain @ innovation\n                updated_cov = predicted_cov - kalman_gain @ innovation_cov\n\n                self.object_poses[obj_id] = (updated_pose, updated_cov)\n            else:\n                # Initialize belief\n                self.object_poses[obj_id] = (measured_pose, measurement_covariance)\n\n    def select_best_action(self, possible_actions):\n        best_action = None\n        best_expected_utility = float('-inf')\n\n        for action in possible_actions:\n            utility = self.estimate_action_utility(action)\n            probability = self.estimate_success_probability(action)\n\n            expected_utility = utility * probability\n\n            if expected_utility > best_expected_utility:\n                best_expected_utility = expected_utility\n                best_action = action\n\n        return best_action\n\n    def estimate_action_utility(self, action):\n        # Calculate utility based on task goals, safety, efficiency\n        # This is a simplified example\n        if action['type'] == 'grasp':\n            # Higher utility for objects closer to gripper workspace\n            distance_to_workspace = self.calculate_distance_to_workspace(\n                action['target_pose']\n            )\n            utility = max(0, 1 - distance_to_workspace / 1.0)  # Normalize\n            return utility\n\n        return 0.5  # Default utility\n\n    def estimate_success_probability(self, action):\n        # Estimate probability of successful execution\n        obj_id = action['target_object']\n\n        if obj_id in self.object_poses:\n            pose_mean, pose_cov = self.object_poses[obj_id]\n\n            # Calculate confidence based on covariance determinant\n            confidence = 1.0 / (np.sqrt(np.linalg.det(pose_cov)) + 1e-6)\n\n            # Normalize confidence\n            normalized_confidence = min(1.0, confidence / 10.0)\n\n            return normalized_confidence\n\n        return 0.3  # Low confidence if object not tracked\n"})}),"\n",(0,o.jsx)(n.h2,{id:"humanoid-manipulation-logic",children:"Humanoid Manipulation Logic"}),"\n",(0,o.jsx)(n.h3,{id:"grasping-strategy-selection",children:"Grasping Strategy Selection"}),"\n",(0,o.jsx)(n.p,{children:"Different objects require different grasping strategies based on their shape, size, weight, and material properties."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class GraspStrategySelector:\n    def __init__(self):\n        self.grasp_strategies = {\n            'pinch': {\n                'aperture_range': (0.01, 0.05),\n                'weight_limit': 0.5,\n                'shape_preference': ['thin', 'small']\n            },\n            'power': {\n                'aperture_range': (0.05, 0.15),\n                'weight_limit': 5.0,\n                'shape_preference': ['cylindrical', 'rectangular']\n            },\n            'tripod': {\n                'aperture_range': (0.03, 0.1),\n                'weight_limit': 2.0,\n                'shape_preference': ['curved', 'round']\n            }\n        }\n\n    def select_grasp_strategy(self, object_properties):\n        object_shape = self.classify_object_shape(object_properties)\n        object_size = object_properties.get('size', 'medium')\n        object_weight = object_properties.get('weight', 1.0)\n        object_surface = object_properties.get('surface', 'smooth')\n\n        candidate_strategies = []\n\n        for strategy_name, strategy_props in self.grasp_strategies.items():\n            score = 0\n\n            # Size compatibility\n            min_aperture, max_aperture = strategy_props['aperture_range']\n            object_width = object_properties.get('width', 0.1)\n\n            if min_aperture <= object_width <= max_aperture:\n                score += 2\n            elif abs(object_width - (min_aperture + max_aperture) / 2) < 0.05:\n                score += 1\n\n            # Weight compatibility\n            if object_weight <= strategy_props['weight_limit']:\n                score += 1\n\n            # Shape preference\n            if object_shape in strategy_props['shape_preference']:\n                score += 1\n\n            # Surface considerations\n            if object_surface == 'rough' and strategy_name != 'pinch':\n                score += 1\n            elif object_surface == 'smooth' and strategy_name in ['pinch', 'tripod']:\n                score += 1\n\n            candidate_strategies.append((strategy_name, score))\n\n        # Select highest scoring strategy\n        best_strategy = max(candidate_strategies, key=lambda x: x[1])\n\n        if best_strategy[1] > 0:\n            return best_strategy[0]\n        else:\n            # Fallback to power grasp for unknown objects\n            return 'power'\n\n    def classify_object_shape(self, properties):\n        dimensions = properties.get('dimensions', [1, 1, 1])\n        length, width, height = dimensions\n\n        # Simple shape classification\n        if height < min(length, width) * 0.5:\n            return 'flat'\n        elif abs(length - width) < 0.02 and abs(width - height) < 0.02:\n            return 'cubic'\n        elif abs(length - width) < 0.02 and height > max(length, width):\n            return 'cylindrical'\n        elif height > length and height > width:\n            return 'tall'\n        else:\n            return 'irregular'\n\nclass GraspController:\n    def __init__(self, robot_interface):\n        self.robot = robot_interface\n        self.selector = GraspStrategySelector()\n        self.safe_grip_force = 50.0  # Newtons\n\n    def execute_grasp(self, target_object):\n        # Plan grasp pose\n        grasp_pose = self.calculate_grasp_pose(target_object)\n\n        # Move to pre-grasp position\n        self.move_to_pregrasp(grasp_pose)\n\n        # Approach object\n        self.approach_object(grasp_pose)\n\n        # Execute grasp based on strategy\n        strategy = self.selector.select_grasp_strategy(target_object)\n        grip_force = self.calculate_grip_force(target_object, strategy)\n\n        self.close_gripper(grip_force)\n\n        # Lift object\n        self.lift_object()\n\n        return True\n\n    def calculate_grasp_pose(self, object_data):\n        # Calculate optimal grasp pose based on object properties\n        object_center = object_data['position_3d']\n        object_dimensions = object_data['dimensions_3d']\n\n        # Simple approach: grasp at top center of object\n        grasp_x = object_center[0]\n        grasp_y = object_center[1]\n        grasp_z = object_center[2] + object_dimensions[2] / 2 + 0.02  # 2cm above object\n\n        # Orientation for stable grasp\n        grasp_orientation = self.calculate_stable_orientation(object_data)\n\n        return {\n            'position': [grasp_x, grasp_y, grasp_z],\n            'orientation': grasp_orientation\n        }\n\n    def calculate_grip_force(self, object_data, strategy):\n        # Calculate appropriate grip force based on object weight and friction\n        object_weight = object_data.get('weight', 1.0)\n        surface_friction = object_data.get('friction_coefficient', 0.5)\n\n        # Calculate minimum required force to prevent slip\n        required_force = object_weight / surface_friction\n\n        # Apply strategy-specific multiplier\n        strategy_multiplier = {'pinch': 1.5, 'power': 2.0, 'tripod': 1.8}[strategy]\n\n        # Final grip force\n        grip_force = min(required_force * strategy_multiplier, self.safe_grip_force)\n\n        return grip_force\n"})}),"\n",(0,o.jsx)(n.h3,{id:"reachability-analysis",children:"Reachability Analysis"}),"\n",(0,o.jsx)(n.p,{children:"Ensuring that the robot can physically reach target objects is crucial for successful manipulation."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy.spatial.distance import euclidean\n\nclass ReachabilityAnalyzer:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.workspace_bounds = self.calculate_workspace_bounds()\n\n    def calculate_workspace_bounds(self):\n        # Calculate reachable workspace based on robot kinematics\n        # This is a simplified representation\n        joint_limits = self.robot.get_joint_limits()\n\n        # Conservative estimate of workspace\n        max_reach = sum([link.length for link in self.robot.links])\n\n        return {\n            'min_x': -max_reach,\n            'max_x': max_reach,\n            'min_y': -max_reach,\n            'max_y': max_reach,\n            'min_z': 0.1,  # Minimum height to avoid ground\n            'max_z': max_reach\n        }\n\n    def is_reachable(self, target_position, posture=None):\n        \"\"\"\n        Check if target position is reachable by the robot.\n\n        Args:\n            target_position: [x, y, z] coordinates\n            posture: Optional current joint configuration\n\n        Returns:\n            bool: True if reachable, False otherwise\n        \"\"\"\n        x, y, z = target_position\n\n        # Check bounds\n        if not (self.workspace_bounds['min_x'] <= x <= self.workspace_bounds['max_x'] and\n                self.workspace_bounds['min_y'] <= y <= self.workspace_bounds['max_y'] and\n                self.workspace_bounds['min_z'] <= z <= self.workspace_bounds['max_z']):\n            return False\n\n        # Check inverse kinematics solution\n        ik_solution = self.robot.inverse_kinematics(target_position)\n\n        if ik_solution is None:\n            return False\n\n        # Check joint limits\n        joint_limits = self.robot.get_joint_limits()\n        for joint_val, (min_lim, max_lim) in zip(ik_solution, joint_limits):\n            if not (min_lim <= joint_val <= max_lim):\n                return False\n\n        return True\n\n    def find_alternative_approach_poses(self, target_position):\n        \"\"\"\n        Find alternative approach poses around the target if direct approach fails.\n\n        Args:\n            target_position: [x, y, z] coordinates of target\n\n        Returns:\n            list: Alternative approach poses that are reachable\n        \"\"\"\n        alternatives = []\n        target_x, target_y, target_z = target_position\n\n        # Generate points around the target in a sphere\n        for angle in np.linspace(0, 2*np.pi, 12):\n            for elevation in np.linspace(-np.pi/4, np.pi/4, 5):\n                # Offset from target\n                offset_distance = 0.15  # 15cm from target\n                offset_x = target_x + offset_distance * np.cos(angle) * np.cos(elevation)\n                offset_y = target_y + offset_distance * np.sin(angle) * np.cos(elevation)\n                offset_z = target_z + offset_distance * np.sin(elevation)\n\n                approach_pose = [offset_x, offset_y, offset_z]\n\n                if self.is_reachable(approach_pose):\n                    # Calculate approach direction for proper orientation\n                    approach_direction = np.array([target_x - offset_x,\n                                                 target_y - offset_y,\n                                                 target_z - offset_z])\n                    approach_direction = approach_direction / np.linalg.norm(approach_direction)\n\n                    alternatives.append({\n                        'position': approach_pose,\n                        'direction': approach_direction\n                    })\n\n        return alternatives\n\n    def calculate_manipulation_trajectory(self, start_pose, target_pose):\n        \"\"\"\n        Calculate smooth trajectory from start to target pose.\n\n        Args:\n            start_pose: Starting position [x, y, z]\n            target_pose: Target position [x, y, z]\n\n        Returns:\n            list: Waypoints for smooth trajectory\n        \"\"\"\n        num_waypoints = 20\n        trajectory = []\n\n        start = np.array(start_pose)\n        target = np.array(target_pose)\n\n        for i in range(num_waypoints + 1):\n            t = i / num_waypoints\n            waypoint = start + t * (target - start)\n\n            trajectory.append(waypoint.tolist())\n\n        return trajectory\n\nclass SafetyChecker:\n    def __init__(self, robot_model):\n        self.robot = robot_model\n        self.collision_detector = self.initialize_collision_detector()\n\n    def initialize_collision_detector(self):\n        # Initialize collision detection system\n        # This would typically use libraries like Bullet or FCL\n        return CollisionDetector()\n\n    def check_safety_constraints(self, target_pose):\n        \"\"\"\n        Check if moving to target pose violates safety constraints.\n\n        Args:\n            target_pose: Target position and orientation\n\n        Returns:\n            dict: Safety assessment with constraint violations\n        \"\"\"\n        safety_check = {\n            'collision_free': True,\n            'joint_limits_respected': True,\n            'velocity_limits_respected': True,\n            'torque_limits_respected': True,\n            'violations': []\n        }\n\n        # Check for collisions along trajectory\n        current_pose = self.robot.get_current_end_effector_pose()\n        trajectory = self.interpolate_trajectory(current_pose, target_pose)\n\n        for waypoint in trajectory:\n            if self.collision_detector.check_collision_at_pose(waypoint):\n                safety_check['collision_free'] = False\n                safety_check['violations'].append(f\"Collision at pose: {waypoint}\")\n\n        # Check joint limits\n        ik_solution = self.robot.inverse_kinematics(target_pose)\n        if ik_solution is not None:\n            joint_limits = self.robot.get_joint_limits()\n            for joint_val, (min_lim, max_lim) in zip(ik_solution, joint_limits):\n                if joint_val < min_lim or joint_val > max_lim:\n                    safety_check['joint_limits_respected'] = False\n                    safety_check['violations'].append(f\"Joint limit violation: {joint_val}\")\n\n        return safety_check\n\n    def interpolate_trajectory(self, start_pose, end_pose, steps=10):\n        \"\"\"Interpolate trajectory between start and end poses.\"\"\"\n        trajectory = []\n\n        for i in range(steps + 1):\n            t = i / steps\n            waypoint = {}\n\n            # Interpolate position\n            start_pos = np.array(start_pose['position'])\n            end_pos = np.array(end_pose['position'])\n            pos_interp = start_pos + t * (end_pos - start_pos)\n            waypoint['position'] = pos_interp.tolist()\n\n            # Interpolate orientation\n            start_orient = start_pose['orientation']\n            end_orient = end_pose['orientation']\n            orient_interp = self.slerp_quaternions(start_orient, end_orient, t)\n            waypoint['orientation'] = orient_interp\n\n            trajectory.append(waypoint)\n\n        return trajectory\n\n    def slerp_quaternions(self, q1, q2, t):\n        \"\"\"Spherical linear interpolation of quaternions.\"\"\"\n        # Simplified implementation\n        q1 = np.array(q1) if isinstance(q1, list) else q1\n        q2 = np.array(q2) if isinstance(q2, list) else q2\n\n        dot_product = np.dot(q1, q2)\n\n        if dot_product < 0.0:\n            q2 = -q2\n            dot_product = -dot_product\n\n        DOT_THRESHOLD = 0.9995\n        if dot_product > DOT_THRESHOLD:\n            result = q1 + t * (q2 - q1)\n            return result / np.linalg.norm(result)\n\n        theta_0 = np.arccos(dot_product)\n        sin_theta_0 = np.sin(theta_0)\n        theta = theta_0 * t\n        sin_theta = np.sin(theta)\n\n        s0 = np.cos(theta) - dot_product * sin_theta / sin_theta_0\n        s1 = sin_theta / sin_theta_0\n\n        return (s0 * q1 + s1 * q2) / np.linalg.norm(s0 * q1 + s1 * q2)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-nodes-and-topics",children:"ROS 2 Nodes and Topics"}),"\n",(0,o.jsx)(n.p,{children:"ROS 2 provides the communication infrastructure for coordinating vision and action components."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nfrom builtin_interfaces.msg import Duration\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n\n        # Publishers\n        self.detection_pub = self.create_publisher(String, '/object_detections', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, '/scene_pointcloud', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.bridge = CvBridge()\n        self.detector = MultiModalFusion()  # Our earlier detector\n\n        # Timer for periodic processing\n        self.timer = self.create_timer(0.1, self.process_vision_data)\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image data.\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.latest_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f'Error converting image: {e}')\n\n    def process_vision_data(self):\n        \"\"\"Process vision data and publish detections.\"\"\"\n        if hasattr(self, 'latest_image'):\n            try:\n                # Process image to detect objects\n                # For simplicity, assuming we have depth info\n                detections = self.detector.detect_objects(self.latest_image)\n\n                # Publish detections\n                detection_msg = String()\n                detection_msg.data = str(detections)  # Serialize appropriately\n                self.detection_pub.publish(detection_msg)\n\n            except Exception as e:\n                self.get_logger().error(f'Error processing vision data: {e}')\n\nclass ActionNode(Node):\n    def __init__(self):\n        super().__init__('action_node')\n\n        # Publishers\n        self.manipulation_pub = self.create_publisher(PoseStamped, '/manipulation_target', 10)\n        self.status_pub = self.create_publisher(String, '/action_status', 10)\n\n        # Subscribers\n        self.detection_sub = self.create_subscription(\n            String,\n            '/object_detections',\n            self.detection_callback,\n            10\n        )\n\n        self.planner = LLMPlanner()  # From previous modules\n        self.manipulator = GraspController(robot_interface=None)\n\n        # Action server for complex manipulation tasks\n        from rclpy.action import ActionServer\n        from manipulation_msgs.action import GraspObject\n\n        self._action_server = ActionServer(\n            self,\n            GraspObject,\n            'grasp_object',\n            self.execute_grasp_object\n        )\n\n    def detection_callback(self, msg):\n        \"\"\"Process object detections and plan actions.\"\"\"\n        try:\n            detections = eval(msg.data)  # Deserialize safely in production\n\n            if detections:\n                # Plan manipulation based on detections\n                action_plan = self.planner.plan_manipulation(detections)\n\n                if action_plan:\n                    pose_msg = PoseStamped()\n                    pose_msg.header.stamp = self.get_clock().now().to_msg()\n                    pose_msg.header.frame_id = 'map'\n\n                    pose_msg.pose.position.x = action_plan['target_position'][0]\n                    pose_msg.pose.position.y = action_plan['target_position'][1]\n                    pose_msg.pose.position.z = action_plan['target_position'][2]\n\n                    self.manipulation_pub.publish(pose_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing detection: {e}')\n\n    def execute_grasp_object(self, goal_handle):\n        \"\"\"Execute grasp object action.\"\"\"\n        self.get_logger().info('Executing grasp object action...')\n\n        object_id = goal_handle.request.object_id\n\n        # Implement grasping logic\n        success = self.manipulator.grasp_object_by_id(object_id)\n\n        result = GraspObject.Result()\n        result.success = success\n\n        if success:\n            goal_handle.succeed()\n            return result\n        else:\n            goal_handle.abort()\n            return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    vision_node = VisionNode()\n    action_node = ActionNode()\n\n    try:\n        rclpy.spin(vision_node)\n        rclpy.spin(action_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_node.destroy_node()\n        action_node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"message-definitions-and-service-calls",children:"Message Definitions and Service Calls"}),"\n",(0,o.jsx)(n.p,{children:"Custom message types define the interfaces between vision and action components."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Custom message definition (in msg/ObjectDetection.msg format)\n"""\nHeader header\nstring object_class\nfloat64 confidence\nfloat64[] bbox  # [x_min, y_min, x_max, y_max]\ngeometry_msgs/Point32[] points  # 3D points associated with object\n"""\n\n# Service definition (in srv/FindObject.srv format)\n"""\nstring object_name\n---\nbool found\ngeometry_msgs/PoseStamped location\nfloat64 confidence\n"""\n'})}),"\n",(0,o.jsx)(n.h2,{id:"diagrams-and-architecture",children:"Diagrams and Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"vision-action-integration-architecture",children:"Vision-Action Integration Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   RGB Camera    \u2502    \u2502  Depth Sensor    \u2502    \u2502  IMU/Gyro       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                      \u2502                       \u2502\n          \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   Data Fusion        \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502   Engine             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502    Object Detection     \u2502\n                    \u2502    and Classification   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     3D Reconstruction   \u2502\n                    \u2502    and Pose Estimation  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     LLM Cognitive       \u2502\n                    \u2502      Reasoning          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Manipulation Planning \u2502\n                    \u2502    and Trajectory Gen   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Robot Motion Control  \u2502\n                    \u2502     and Execution       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h3,{id:"perception-decision-action-loop-timing",children:"Perception-Decision-Action Loop Timing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Time:     T0 \u2500\u2500\u2500\u2500\u2500\u2500 T1 \u2500\u2500\u2500\u2500\u2500\u2500 T2 \u2500\u2500\u2500\u2500\u2500\u2500 T3 \u2500\u2500\u2500\u2500\u2500\u2500 T4 \u2500\u2500\u2500\u2500\u2500\u2500 T5\n          \u2502        \u2502        \u2502        \u2502        \u2502        \u2502\nVision:   [Capture][Process][Detect  ][Update ][Ready ][Capture]\nDecision:         [Analyze ][Plan   ][Check  ][Execute][Analyze]\nAction:           [Ready   ][Ready   ][Move   ][Wait   ][Ready   ]\n\nWhere:\n- T0-T1: Image capture and initial processing\n- T1-T2: Object detection and scene understanding\n- T2-T3: Decision making and action planning\n- T3-T4: Motion execution and monitoring\n- T4-T5: Ready for next cycle\n"})}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-1-implement-a-simple-grasp-planner",children:"Exercise 1: Implement a Simple Grasp Planner"}),"\n",(0,o.jsx)(n.p,{children:"Create a basic grasp planner that takes object dimensions and generates appropriate grasp poses."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a function that calculates grasp positions based on object dimensions"}),"\n",(0,o.jsx)(n.li,{children:"Implement orientation selection for stable grasping"}),"\n",(0,o.jsx)(n.li,{children:"Add safety margins and collision checking"}),"\n",(0,o.jsx)(n.li,{children:"Test with different object shapes and sizes"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exercise-2-multi-modal-object-tracking",children:"Exercise 2: Multi-Modal Object Tracking"}),"\n",(0,o.jsx)(n.p,{children:"Develop a system that combines RGB and depth data for robust object tracking."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Subscribe to both RGB and depth camera topics"}),"\n",(0,o.jsx)(n.li,{children:"Implement 2D-3D correspondence matching"}),"\n",(0,o.jsx)(n.li,{children:"Track objects across frames using Kalman filtering"}),"\n",(0,o.jsx)(n.li,{children:"Visualize tracked objects with bounding boxes and 3D positions"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exercise-3-ros-2-vision-action-bridge",children:"Exercise 3: ROS 2 Vision-Action Bridge"}),"\n",(0,o.jsx)(n.p,{children:"Build a ROS 2 node that connects perception and action layers."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a node with image subscribers and pose publishers"}),"\n",(0,o.jsx)(n.li,{children:"Implement object detection callbacks"}),"\n",(0,o.jsx)(n.li,{children:"Integrate with existing manipulation stack"}),"\n",(0,o.jsx)(n.li,{children:"Add action servers for high-level commands"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Vision-action integration is the cornerstone of intelligent robotic manipulation. By combining sophisticated perception algorithms with robust decision-making and safe action execution, humanoid robots can operate effectively in unstructured environments. The key components include multi-modal sensing, real-time processing, probabilistic reasoning, and safe execution within the ROS 2 framework."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);