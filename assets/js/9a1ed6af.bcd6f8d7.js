"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9364],{2436:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/llm-integration/llm-integration","title":"LLM Integration for Action Planning","description":"Overview","source":"@site/docs/module-4-vision-language-action/llm-integration/llm-integration.md","sourceDirName":"module-4-vision-language-action/llm-integration","slug":"/module-4-vision-language-action/llm-integration/","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/llm-integration/","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-4-vision-language-action/llm-integration/llm-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Using OpenAI Whisper","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/voice-processing/voice-to-action-whisper"},"next":{"title":"LLM Integration for Cognitive Planning","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/llm-integration/llm-cognitive-planning"}}');var i=t(4848),s=t(8453);const a={sidebar_position:3},r="LLM Integration for Action Planning",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"LLM Selection and Setup",id:"llm-selection-and-setup",level:2},{value:"Choosing the Right LLM for Robotics",id:"choosing-the-right-llm-for-robotics",level:3},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Structured Task Decomposition Prompts",id:"structured-task-decomposition-prompts",level:3},{value:"Context-Aware Prompting",id:"context-aware-prompting",level:3},{value:"Cognitive Planning Implementation",id:"cognitive-planning-implementation",level:2},{value:"Task Decomposition Engine",id:"task-decomposition-engine",level:3},{value:"Safety and Validation Layer",id:"safety-and-validation-layer",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Action Client Interface",id:"action-client-interface",level:3},{value:"Main Integration Node",id:"main-integration-node",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"1. Set Up the LLM Integration Package",id:"1-set-up-the-llm-integration-package",level:3},{value:"2. Install Dependencies",id:"2-install-dependencies",level:3},{value:"3. Configure the System",id:"3-configure-the-system",level:3},{value:"4. Testing the Integration",id:"4-testing-the-integration",level:3},{value:"Best Practices and Considerations",id:"best-practices-and-considerations",level:2},{value:"1. Prompt Optimization",id:"1-prompt-optimization",level:3},{value:"2. Safety First Approach",id:"2-safety-first-approach",level:3},{value:"3. Performance Optimization",id:"3-performance-optimization",level:3},{value:"4. Context Management",id:"4-context-management",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debugging Tips",id:"debugging-tips",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llm-integration-for-action-planning",children:"LLM Integration for Action Planning"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Large Language Model (LLM) integration is the cognitive core of Vision-Language-Action (VLA) systems for humanoid robots. This component bridges natural language understanding with robotic action planning, enabling humanoid robots to interpret complex voice commands and decompose them into executable robotic tasks."}),"\n",(0,i.jsx)(n.p,{children:"LLM integration transforms high-level human instructions into structured action plans that consider environmental constraints, robot capabilities, and safety requirements. This module covers the implementation of prompt engineering, context management, and cognitive planning systems that connect voice recognition with physical execution."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design effective prompts for robotics task decomposition"}),"\n",(0,i.jsx)(n.li,{children:"Implement context management for ongoing conversations"}),"\n",(0,i.jsx)(n.li,{children:"Create cognitive planning pipelines that convert natural language to robot actions"}),"\n",(0,i.jsx)(n.li,{children:"Integrate safety validation layers with LLM outputs"}),"\n",(0,i.jsx)(n.li,{children:"Connect LLM outputs to ROS 2 action execution systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before implementing LLM integration for action planning, ensure you have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completed Module 3 (AI-Robot Brain) focusing on navigation and perception"}),"\n",(0,i.jsx)(n.li,{children:"Voice recognition system from Section 2 of this module"}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of LLM APIs (OpenAI GPT, Anthropic Claude, or open-source alternatives)"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 action client/server implementation knowledge"}),"\n",(0,i.jsx)(n.li,{children:"Familiarity with task planning and execution frameworks"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"llm-selection-and-setup",children:"LLM Selection and Setup"}),"\n",(0,i.jsx)(n.h3,{id:"choosing-the-right-llm-for-robotics",children:"Choosing the Right LLM for Robotics"}),"\n",(0,i.jsx)(n.p,{children:"Different LLM architectures offer varying capabilities for robotics applications:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import openai\nimport anthropic\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass LLMProviderManager:\n    \"\"\"Manage different LLM providers for robotics applications\"\"\"\n\n    def __init__(self):\n        self.providers = {\n            'openai': self._setup_openai,\n            'anthropic': self._setup_anthropic,\n            'open_source': self._setup_open_source\n        }\n        self.current_provider = None\n\n    def _setup_openai(self, api_key: str):\n        \"\"\"Setup OpenAI GPT for robotics tasks\"\"\"\n        openai.api_key = api_key\n        return {\n            'client': openai.OpenAI(api_key=api_key),\n            'model': 'gpt-4-turbo',\n            'max_tokens': 2048,\n            'temperature': 0.3\n        }\n\n    def _setup_anthropic(self, api_key: str):\n        \"\"\"Setup Anthropic Claude for robotics tasks\"\"\"\n        return {\n            'client': anthropic.Anthropic(api_key=api_key),\n            'model': 'claude-3-5-sonnet-20241022',\n            'max_tokens': 2048,\n            'temperature': 0.3\n        }\n\n    def _setup_open_source(self, model_name: str):\n        \"\"\"Setup open-source LLM for robotics tasks\"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        return {\n            'tokenizer': tokenizer,\n            'model': model,\n            'device': model.device\n        }\n"})}),"\n",(0,i.jsx)(n.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Configure your environment for LLM integration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create environment variables file\ncat > .env << EOF\nOPENAI_API_KEY=your_openai_api_key_here\nANTHROPIC_API_KEY=your_anthropic_api_key_here\nLLM_PROVIDER=openai  # or anthropic, open_source\nLLM_MODEL=gpt-4-turbo\nEOF\n\n# Install required dependencies\npip install openai anthropic transformers torch accelerate\npip install python-dotenv  # For environment variable management\n"})}),"\n",(0,i.jsx)(n.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,i.jsx)(n.h3,{id:"structured-task-decomposition-prompts",children:"Structured Task Decomposition Prompts"}),"\n",(0,i.jsx)(n.p,{children:"Effective prompt engineering is crucial for converting natural language to robotic actions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RoboticsPromptEngineer:\n    """Engineer effective prompts for robotics task decomposition"""\n\n    def __init__(self):\n        self.system_prompt = """\n        You are an AI assistant specialized in robotics task planning. Your role is to convert natural language commands into structured robotic action plans.\n\n        ## Capabilities\n        - Navigate to locations (indoors/outdoors)\n        - Manipulate objects (pick/place/grasp)\n        - Interact with humans (follow, greet, assist)\n        - Perform household tasks (cleaning, organizing)\n        - Safety and obstacle avoidance\n\n        ## Constraints\n        - Always validate safety before action\n        - Consider robot physical limitations\n        - Handle ambiguous commands gracefully\n        - Prioritize human safety above all\n        """\n\n    def create_task_decomposition_prompt(self, user_command: str, environment_context: dict):\n        """Create a structured prompt for task decomposition"""\n\n        prompt = f"""\n        {self.system_prompt}\n\n        ## Environment Context\n        - Current robot position: {environment_context.get(\'position\', \'unknown\')}\n        - Nearby objects: {environment_context.get(\'objects\', [])}\n        - Available actions: {environment_context.get(\'capabilities\', [])}\n        - Safety constraints: {environment_context.get(\'safety_constraints\', [])}\n\n        ## User Command\n        "{user_command}"\n\n        ## Response Format\n        Provide your response in the following JSON format:\n        {{\n            "original_command": "...",\n            "intent_classification": "...",\n            "decomposed_tasks": [\n                {{\n                    "task_id": "...",\n                    "description": "...",\n                    "action_type": "...",\n                    "parameters": {{}},\n                    "estimated_duration": "...",\n                    "safety_check_required": true,\n                    "dependencies": []\n                }}\n            ],\n            "confidence_score": 0.0-1.0,\n            "clarification_needed": false,\n            "reasoning": "..."\n        }}\n\n        ## Examples\n        User: "Go to the kitchen and bring me a cup of water"\n        Response: {{\n            "original_command": "Go to the kitchen and bring me a cup of water",\n            "intent_classification": "fetch_object_with_navigation",\n            "decomposed_tasks": [\n                {{\n                    "task_id": "nav_to_kitchen",\n                    "description": "Navigate to kitchen location",\n                    "action_type": "navigation",\n                    "parameters": {{"target_location": "kitchen"}},\n                    "estimated_duration": "2-3 minutes",\n                    "safety_check_required": true,\n                    "dependencies": []\n                }},\n                {{\n                    "task_id": "locate_cup",\n                    "description": "Locate and identify cup in kitchen",\n                    "action_type": "object_detection",\n                    "parameters": {{"object_type": "cup"}},\n                    "estimated_duration": "30 seconds",\n                    "safety_check_required": true,\n                    "dependencies": ["nav_to_kitchen"]\n                }}\n            ],\n            "confidence_score": 0.85,\n            "clarification_needed": false,\n            "reasoning": "Command involves navigation to kitchen and object manipulation to fetch water."\n        }}\n        """\n\n        return prompt\n\n    def create_safety_validation_prompt(self, proposed_actions: list, environment_state: dict):\n        """Create safety validation prompts for proposed actions"""\n\n        safety_prompt = f"""\n        Evaluate the following proposed robotic actions for safety compliance:\n\n        ## Environment State\n        - Human presence: {environment_state.get(\'humans_nearby\', \'unknown\')}\n        - Obstacles: {environment_state.get(\'obstacles\', [])}\n        - Robot health: {environment_state.get(\'robot_status\', \'normal\')}\n        - Safety zones: {environment_state.get(\'safety_zones\', [])}\n\n        ## Proposed Actions\n        {proposed_actions}\n\n        ## Safety Requirements\n        1. No actions that could harm humans\n        2. No actions that could damage robot or environment\n        3. Compliance with operational constraints\n        4. Emergency stop capabilities maintained\n\n        ## Response Format\n        {{\n            "actions_safe": true/false,\n            "safety_issues": [...],\n            "recommended_modifications": [...],\n            "risk_assessment": "low/medium/high",\n            "validation_reasoning": "..."\n        }}\n        """\n\n        return safety_prompt\n'})}),"\n",(0,i.jsx)(n.h3,{id:"context-aware-prompting",children:"Context-Aware Prompting"}),"\n",(0,i.jsx)(n.p,{children:"Implement context management for ongoing conversations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import json\nfrom datetime import datetime, timedelta\n\nclass ContextManager:\n    """Manage conversation and environmental context for LLM interactions"""\n\n    def __init__(self, max_context_length: int = 10):\n        self.conversation_history = []\n        self.environment_memory = {}\n        self.object_locations = {}\n        self.task_states = {}\n        self.max_context_length = max_context_length\n\n    def add_conversation_turn(self, user_input: str, ai_response: str, timestamp: datetime = None):\n        """Add a conversation turn to history"""\n        if timestamp is None:\n            timestamp = datetime.now()\n\n        turn = {\n            \'timestamp\': timestamp.isoformat(),\n            \'user_input\': user_input,\n            \'ai_response\': ai_response,\n            \'turn_id\': len(self.conversation_history)\n        }\n\n        self.conversation_history.append(turn)\n\n        # Maintain context window size\n        if len(self.conversation_history) > self.max_context_length:\n            self.conversation_history = self.conversation_history[-self.max_context_length:]\n\n    def update_environment_memory(self, updates: dict):\n        """Update environmental context"""\n        self.environment_memory.update(updates)\n        self._cleanup_old_memory()\n\n    def update_object_location(self, object_name: str, location: dict, confidence: float = 1.0):\n        """Update known object locations"""\n        self.object_locations[object_name] = {\n            \'location\': location,\n            \'confidence\': confidence,\n            \'last_seen\': datetime.now().isoformat(),\n            \'tracking_id\': f"{object_name}_{len(self.object_locations)}"\n        }\n\n    def get_context_for_prompt(self) -> dict:\n        """Get current context for LLM prompting"""\n        return {\n            \'conversation_history\': self.conversation_history[-3:],  # Last 3 turns\n            \'environment_state\': self.environment_memory,\n            \'known_objects\': self.object_locations,\n            \'ongoing_tasks\': self.task_states,\n            \'current_time\': datetime.now().isoformat()\n        }\n\n    def _cleanup_old_memory(self):\n        """Remove outdated environmental memory"""\n        current_time = datetime.now()\n        cutoff_time = current_time - timedelta(hours=1)  # Keep 1 hour of memory\n\n        # Clean up object locations older than 1 hour\n        old_objects = []\n        for obj_name, obj_data in self.object_locations.items():\n            obj_time = datetime.fromisoformat(obj_data[\'last_seen\'])\n            if obj_time < cutoff_time:\n                old_objects.append(obj_name)\n\n        for obj in old_objects:\n            del self.object_locations[obj]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"cognitive-planning-implementation",children:"Cognitive Planning Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"task-decomposition-engine",children:"Task Decomposition Engine"}),"\n",(0,i.jsx)(n.p,{children:"Implement the core task decomposition system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport json\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ActionType(Enum):\n    NAVIGATION = "navigation"\n    OBJECT_DETECTION = "object_detection"\n    MANIPULATION = "manipulation"\n    INTERACTION = "interaction"\n    MONITORING = "monitoring"\n    SAFETY_CHECK = "safety_check"\n\n@dataclass\nclass Task:\n    task_id: str\n    description: str\n    action_type: ActionType\n    parameters: Dict[str, Any]\n    estimated_duration: str\n    safety_check_required: bool\n    dependencies: List[str]\n    priority: int = 1\n    confidence_threshold: float = 0.7\n\nclass TaskDecompositionEngine:\n    """Core engine for decomposing natural language commands into robotic tasks"""\n\n    def __init__(self, llm_client, prompt_engineer: RoboticsPromptEngineer):\n        self.llm_client = llm_client\n        self.prompt_engineer = prompt_engineer\n        self.context_manager = ContextManager()\n\n    async def decompose_command(self, user_command: str, environment_context: dict) -> List[Task]:\n        """Decompose a user command into executable tasks"""\n\n        # Create prompt with context\n        prompt = self.prompt_engineer.create_task_decomposition_prompt(\n            user_command, environment_context\n        )\n\n        # Call LLM for task decomposition\n        response = await self._call_llm(prompt)\n\n        # Parse and validate response\n        parsed_response = self._parse_task_response(response)\n\n        # Convert to Task objects\n        tasks = self._convert_to_tasks(parsed_response[\'decomposed_tasks\'])\n\n        # Store in context\n        self.context_manager.add_conversation_turn(\n            user_command,\n            json.dumps([t.__dict__ for t in tasks], indent=2)\n        )\n\n        return tasks\n\n    def _parse_task_response(self, llm_response: str) -> dict:\n        """Parse LLM response into structured task format"""\n        try:\n            # Try to find JSON in response\n            start_idx = llm_response.find(\'{\')\n            end_idx = llm_response.rfind(\'}\') + 1\n\n            if start_idx != -1 and end_idx != 0:\n                json_str = llm_response[start_idx:end_idx]\n                parsed = json.loads(json_str)\n\n                # Validate required fields\n                required_fields = [\'decomposed_tasks\', \'confidence_score\']\n                for field in required_fields:\n                    if field not in parsed:\n                        raise ValueError(f"Missing required field: {field}")\n\n                return parsed\n            else:\n                raise ValueError("No valid JSON found in LLM response")\n\n        except json.JSONDecodeError as e:\n            raise ValueError(f"Invalid JSON in LLM response: {e}")\n\n    def _convert_to_tasks(self, task_dicts: List[dict]) -> List[Task]:\n        """Convert dictionary representations to Task objects"""\n        tasks = []\n        for task_dict in task_dicts:\n            try:\n                task = Task(\n                    task_id=task_dict[\'task_id\'],\n                    description=task_dict[\'description\'],\n                    action_type=ActionType(task_dict[\'action_type\']),\n                    parameters=task_dict.get(\'parameters\', {}),\n                    estimated_duration=task_dict.get(\'estimated_duration\', \'unknown\'),\n                    safety_check_required=task_dict.get(\'safety_check_required\', True),\n                    dependencies=task_dict.get(\'dependencies\', []),\n                    priority=task_dict.get(\'priority\', 1),\n                    confidence_threshold=task_dict.get(\'confidence_threshold\', 0.7)\n                )\n                tasks.append(task)\n            except (KeyError, ValueError) as e:\n                print(f"Error converting task: {e}, skipping: {task_dict}")\n                continue\n\n        return tasks\n\n    async def _call_llm(self, prompt: str) -> str:\n        """Call LLM with proper error handling and retry logic"""\n        max_retries = 3\n        for attempt in range(max_retries):\n            try:\n                response = await self.llm_client.chat.completions.create(\n                    model="gpt-4-turbo",\n                    messages=[\n                        {"role": "system", "content": "You are a helpful assistant that responds in JSON."},\n                        {"role": "user", "content": prompt}\n                    ],\n                    temperature=0.3,\n                    max_tokens=2048,\n                    response_format={"type": "json_object"}\n                )\n                return response.choices[0].message.content\n\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    raise e\n                print(f"LLM call failed, retrying... ({attempt + 1}/{max_retries})")\n                await asyncio.sleep(1)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"safety-and-validation-layer",children:"Safety and Validation Layer"}),"\n",(0,i.jsx)(n.p,{children:"Implement safety validation for LLM-generated actions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SafetyValidator:\n    """Validate LLM-generated actions for safety compliance"""\n\n    def __init__(self, llm_client, prompt_engineer: RoboticsPromptEngineer):\n        self.llm_client = llm_client\n        self.prompt_engineer = prompt_engineer\n        self.safety_rules = self._load_safety_rules()\n\n    def _load_safety_rules(self) -> dict:\n        """Load predefined safety rules"""\n        return {\n            "human_safety": [\n                "Maintain safe distance from humans (minimum 1 meter)",\n                "Avoid sudden movements near humans",\n                "Stop immediately if human enters danger zone"\n            ],\n            "robot_safety": [\n                "Don\'t exceed joint angle limits",\n                "Monitor temperature and power consumption",\n                "Avoid collisions with obstacles"\n            ],\n            "environmental_safety": [\n                "Respect restricted areas",\n                "Handle fragile objects carefully",\n                "Follow navigation constraints"\n            ]\n        }\n\n    async def validate_action_plan(self, tasks: List[Task], environment_state: dict) -> dict:\n        """Validate action plan against safety requirements"""\n\n        # Create safety validation prompt\n        safety_prompt = self.prompt_engineer.create_safety_validation_prompt(\n            [task.__dict__ for task in tasks],\n            environment_state\n        )\n\n        # Call LLM for safety evaluation\n        response = await self._call_llm(safety_prompt)\n\n        # Parse safety response\n        try:\n            safety_analysis = json.loads(response)\n        except json.JSONDecodeError:\n            safety_analysis = {\n                "actions_safe": False,\n                "safety_issues": ["Could not parse safety analysis"],\n                "recommended_modifications": [],\n                "risk_assessment": "high",\n                "validation_reasoning": "Failed to parse LLM safety response"\n            }\n\n        # Apply internal safety rules\n        internal_validation = self._apply_internal_safety_rules(tasks, environment_state)\n\n        # Combine results\n        combined_result = {\n            "llm_analysis": safety_analysis,\n            "internal_validation": internal_validation,\n            "overall_safe": safety_analysis["actions_safe"] and internal_validation["passed"],\n            "blocking_issues": self._find_blocking_issues(safety_analysis, internal_validation)\n        }\n\n        return combined_result\n\n    def _apply_internal_safety_rules(self, tasks: List[Task], environment_state: dict) -> dict:\n        """Apply internal safety rules to task list"""\n        issues = []\n        passed = True\n\n        for task in tasks:\n            if task.action_type == ActionType.NAVIGATION:\n                # Check navigation safety\n                target_loc = task.parameters.get(\'target_location\')\n                if target_loc in environment_state.get(\'restricted_areas\', []):\n                    issues.append(f"Navigation to restricted area: {target_loc}")\n                    passed = False\n\n            elif task.action_type == ActionType.MANIPULATION:\n                # Check manipulation safety\n                obj_type = task.parameters.get(\'object_type\')\n                if obj_type in environment_state.get(\'fragile_objects\', []):\n                    issues.append(f"Manipulation of fragile object: {obj_type}")\n\n        return {\n            "passed": passed,\n            "issues": issues,\n            "rules_applied": len(self.safety_rules)\n        }\n\n    def _find_blocking_issues(self, llm_analysis: dict, internal_validation: dict) -> list:\n        """Find issues that block action execution"""\n        blocking_issues = []\n\n        # Add LLM-identified safety issues\n        if not llm_analysis.get("actions_safe", True):\n            blocking_issues.extend(llm_analysis.get("safety_issues", []))\n\n        # Add internal validation issues\n        if not internal_validation["passed"]:\n            blocking_issues.extend(internal_validation["issues"])\n\n        return blocking_issues\n\n    async def _call_llm(self, prompt: str) -> str:\n        """Call LLM for safety validation"""\n        try:\n            response = await self.llm_client.chat.completions.create(\n                model="gpt-4-turbo",\n                messages=[\n                    {"role": "system", "content": "You are a safety validator for robotic systems."},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,\n                max_tokens=1024,\n                response_format={"type": "json_object"}\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f"Warning: Safety validation LLM call failed: {e}")\n            # Return a safe default response\n            return json.dumps({\n                "actions_safe": False,\n                "safety_issues": ["Could not contact safety validator"],\n                "recommended_modifications": [],\n                "risk_assessment": "high",\n                "validation_reasoning": "Safety validation service unavailable"\n            })\n'})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.h3,{id:"action-client-interface",children:"Action Client Interface"}),"\n",(0,i.jsx)(n.p,{children:"Connect LLM outputs to ROS 2 action execution:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nfrom builtin_interfaces.msg import Duration\n\nfrom nav2_msgs.action import NavigateToPose\nfrom moveit_msgs.action import MoveGroup\nfrom manipulation_msgs.action import PickObject, PlaceObject\n\nclass LLMActionExecutor(Node):\n    """Execute LLM-generated tasks through ROS 2 action interfaces"""\n\n    def __init__(self):\n        super().__init__(\'llm_action_executor\')\n\n        # Action clients for different capabilities\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        self.moveit_client = ActionClient(self, MoveGroup, \'move_group\')\n        self.pick_client = ActionClient(self, PickObject, \'pick_object\')\n        self.place_client = ActionClient(self, PlaceObject, \'place_object\')\n\n        # Publishers for status updates\n        self.status_pub = self.create_publisher(String, \'llm_execution_status\', 10)\n\n        # Task queue for execution\n        self.execution_queue = []\n        self.current_task = None\n\n        # Timer for task execution loop\n        self.timer = self.create_timer(0.1, self._execution_loop)\n\n    def execute_task_list(self, tasks: List[Task]):\n        """Execute a list of tasks generated by LLM"""\n        self.execution_queue.extend(tasks)\n        self.get_logger().info(f"Added {len(tasks)} tasks to execution queue")\n\n    def _execution_loop(self):\n        """Main execution loop for processing tasks"""\n        if self.current_task is None and self.execution_queue:\n            # Start next task\n            self.current_task = self.execution_queue.pop(0)\n            self._execute_current_task()\n        elif self.current_task:\n            # Check if current task is complete\n            if self._is_task_complete():\n                self._complete_current_task()\n\n    def _execute_current_task(self):\n        """Execute the current task based on its type"""\n        if not self.current_task:\n            return\n\n        task = self.current_task\n        self.get_logger().info(f"Executing task: {task.description}")\n\n        if task.action_type == ActionType.NAVIGATION:\n            self._execute_navigation_task(task)\n        elif task.action_type == ActionType.OBJECT_DETECTION:\n            self._execute_detection_task(task)\n        elif task.action_type == ActionType.MANIPULATION:\n            self._execute_manipulation_task(task)\n        elif task.action_type == ActionType.INTERACTION:\n            self._execute_interaction_task(task)\n        elif task.action_type == ActionType.SAFETY_CHECK:\n            self._execute_safety_check_task(task)\n\n    def _execute_navigation_task(self, task: Task):\n        """Execute navigation task"""\n        goal_msg = NavigateToPose.Goal()\n\n        # Set target pose from task parameters\n        pose_param = task.parameters.get(\'target_pose\')\n        if pose_param:\n            goal_msg.pose.header.frame_id = pose_param.get(\'frame_id\', \'map\')\n            goal_msg.pose.pose.position.x = pose_param[\'position\'][\'x\']\n            goal_msg.pose.pose.position.y = pose_param[\'position\'][\'y\']\n            goal_msg.pose.pose.position.z = pose_param[\'position\'][\'z\']\n            goal_msg.pose.pose.orientation.w = pose_param[\'orientation\'][\'w\']\n        else:\n            # Use location name lookup\n            location_name = task.parameters.get(\'target_location\')\n            location_pose = self._lookup_location(location_name)\n            if location_pose:\n                goal_msg.pose = location_pose\n\n        # Send navigation goal\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self._navigation_completed)\n\n    def _execute_manipulation_task(self, task: Task):\n        """Execute manipulation task"""\n        if task.parameters.get(\'action\') == \'pick\':\n            goal_msg = PickObject.Goal()\n            goal_msg.object_name = task.parameters.get(\'object_name\')\n            goal_msg.object_pose = task.parameters.get(\'object_pose\')\n\n            self.pick_client.wait_for_server()\n            future = self.pick_client.send_goal_async(goal_msg)\n            future.add_done_callback(self._pick_completed)\n\n        elif task.parameters.get(\'action\') == \'place\':\n            goal_msg = PlaceObject.Goal()\n            goal_msg.target_pose = task.parameters.get(\'target_pose\')\n\n            self.place_client.wait_for_server()\n            future = self.place_client.send_goal_async(goal_msg)\n            future.add_done_callback(self._place_completed)\n\n    def _is_task_complete(self) -> bool:\n        """Check if current task is complete"""\n        # Implementation depends on task type and feedback\n        # This is a simplified version\n        return True  # Placeholder - implement based on actual task feedback\n\n    def _complete_current_task(self):\n        """Complete current task and move to next"""\n        if self.current_task:\n            self.get_logger().info(f"Completed task: {self.current_task.description}")\n            self.current_task = None\n\n    def _lookup_location(self, location_name: str) -> PoseStamped:\n        """Lookup predefined location by name"""\n        # This would typically come from a map or configuration\n        locations = {\n            \'kitchen\': PoseStamped(),\n            \'living_room\': PoseStamped(),\n            \'bedroom\': PoseStamped(),\n            # ... other locations\n        }\n        return locations.get(location_name)\n\n    def _navigation_completed(self, future):\n        """Handle navigation completion"""\n        goal_handle = future.result()\n        if goal_handle.accepted:\n            self.get_logger().info(\'Navigation goal accepted\')\n\n    def _pick_completed(self, future):\n        """Handle pick completion"""\n        goal_handle = future.result()\n        if goal_handle.accepted:\n            self.get_logger().info(\'Pick goal accepted\')\n\n    def _place_completed(self, future):\n        """Handle place completion"""\n        goal_handle = future.result()\n        if goal_handle.accepted:\n            self.get_logger().info(\'Place goal accepted\')\n'})}),"\n",(0,i.jsx)(n.h3,{id:"main-integration-node",children:"Main Integration Node"}),"\n",(0,i.jsx)(n.p,{children:"Create the main integration node that connects all components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\n\nclass VLAIntegrationNode(Node):\n    \"\"\"Main integration node for Vision-Language-Action system\"\"\"\n\n    def __init__(self):\n        super().__init__('vla_integration_node')\n\n        # Initialize components\n        self.llm_client = self._initialize_llm_client()\n        self.prompt_engineer = RoboticsPromptEngineer()\n        self.task_decomposer = TaskDecompositionEngine(self.llm_client, self.prompt_engineer)\n        self.safety_validator = SafetyValidator(self.llm_client, self.prompt_engineer)\n        self.action_executor = LLMActionExecutor()\n\n        # Subscriptions\n        self.voice_command_sub = self.create_subscription(\n            String, 'voice_command', self.voice_command_callback, 10)\n        self.environment_sub = self.create_subscription(\n            String, 'environment_state', self.environment_callback, 10)\n        self.robot_pose_sub = self.create_subscription(\n            PoseWithCovarianceStamped, 'amcl_pose', self.pose_callback, 10)\n\n        # Publishers\n        self.response_pub = self.create_publisher(String, 'vla_response', 10)\n        self.task_status_pub = self.create_publisher(String, 'task_status', 10)\n\n        # Internal state\n        self.current_environment = {}\n        self.robot_pose = None\n\n        self.get_logger().info(\"VLA Integration Node initialized\")\n\n    def _initialize_llm_client(self):\n        \"\"\"Initialize LLM client based on environment configuration\"\"\"\n        import os\n        from dotenv import load_dotenv\n        load_dotenv()\n\n        provider = os.getenv('LLM_PROVIDER', 'openai')\n\n        if provider == 'openai':\n            import openai\n            openai.api_key = os.getenv('OPENAI_API_KEY')\n            return openai.AsyncOpenAI()\n        elif provider == 'anthropic':\n            import anthropic\n            return anthropic.AsyncAnthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n        else:\n            raise ValueError(f\"Unsupported LLM provider: {provider}\")\n\n    async def process_voice_command(self, command: str):\n        \"\"\"Process a voice command through the full VLA pipeline\"\"\"\n        try:\n            # Get current environment context\n            env_context = await self._get_environment_context()\n\n            # Decompose command into tasks\n            tasks = await self.task_decomposer.decompose_command(command, env_context)\n\n            # Validate safety\n            safety_result = await self.safety_validator.validate_action_plan(tasks, env_context)\n\n            if not safety_result['overall_safe']:\n                # Handle unsafe commands\n                response_msg = String()\n                response_msg.data = f\"Command '{command}' is unsafe to execute: {safety_result['blocking_issues']}\"\n                self.response_pub.publish(response_msg)\n                return\n\n            # Execute safe tasks\n            self.action_executor.execute_task_list(tasks)\n\n            # Publish success response\n            response_msg = String()\n            response_msg.data = f\"Processing command: {command}. Executing {len(tasks)} tasks.\"\n            self.response_pub.publish(response_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing voice command: {e}\")\n            error_msg = String()\n            error_msg.data = f\"Error processing command: {str(e)}\"\n            self.response_pub.publish(error_msg)\n\n    async def _get_environment_context(self) -> dict:\n        \"\"\"Get current environment context for LLM prompting\"\"\"\n        return {\n            'position': self.robot_pose,\n            'objects': self.current_environment.get('objects', []),\n            'capabilities': ['navigation', 'manipulation', 'interaction'],\n            'safety_constraints': ['human_proximity', 'fragile_objects'],\n            'restricted_areas': self.current_environment.get('restricted_areas', [])\n        }\n\n    def voice_command_callback(self, msg: String):\n        \"\"\"Handle incoming voice commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f\"Received voice command: {command}\")\n\n        # Process in separate thread to avoid blocking\n        import threading\n        thread = threading.Thread(target=lambda: rclpy.spin_until_future_complete(\n            self, self.process_voice_command(command)))\n        thread.start()\n\n    def environment_callback(self, msg: String):\n        \"\"\"Handle environment updates\"\"\"\n        try:\n            env_data = json.loads(msg.data)\n            self.current_environment.update(env_data)\n        except json.JSONDecodeError:\n            self.get_logger().warn(\"Invalid environment data received\")\n\n    def pose_callback(self, msg: PoseWithCovarianceStamped):\n        \"\"\"Handle robot pose updates\"\"\"\n        self.robot_pose = {\n            'x': msg.pose.pose.position.x,\n            'y': msg.pose.pose.position.y,\n            'z': msg.pose.pose.position.z,\n            'orientation': {\n                'x': msg.pose.pose.orientation.x,\n                'y': msg.pose.pose.orientation.y,\n                'z': msg.pose.pose.orientation.z,\n                'w': msg.pose.pose.orientation.w\n            }\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAIntegrationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h3,{id:"1-set-up-the-llm-integration-package",children:"1. Set Up the LLM Integration Package"}),"\n",(0,i.jsx)(n.p,{children:"First, create the ROS 2 package structure:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create package\ncd ~/ros2_ws/src\nros2 pkg create --build-type ament_python vla_llm_integration\ncd vla_llm_integration\nmkdir -p vla_llm_integration/config\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-install-dependencies",children:"2. Install Dependencies"}),"\n",(0,i.jsx)(n.p,{children:"Create and populate the requirements file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create requirements.txt\ncat > vla_llm_integration/requirements.txt << EOF\nopenai>=1.0.0\nanthropic>=0.5.0\ntransformers>=4.35.0\ntorch>=2.0.0\npython-dotenv>=1.0.0\nnumpy>=1.21.0\nEOF\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-configure-the-system",children:"3. Configure the System"}),"\n",(0,i.jsx)(n.p,{children:"Create a launch file to bring up the VLA system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- vla_llm_integration/launch/vla_system.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config_dir = os.path.join(get_package_share_directory('vla_llm_integration'), 'config')\n\n    return LaunchDescription([\n        Node(\n            package='vla_llm_integration',\n            executable='vla_integration_node',\n            name='vla_integration_node',\n            parameters=[],\n            output='screen'\n        ),\n        Node(\n            package='voice_recognition',\n            executable='whisper_speech_recognizer',\n            name='whisper_speech_recognizer',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-testing-the-integration",children:"4. Testing the Integration"}),"\n",(0,i.jsx)(n.p,{children:"Create a test script to verify the integration:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# test_vla_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport time\n\nclass VLATestClient(Node):\n    def __init__(self):\n        super().__init__(\'vla_test_client\')\n        self.publisher = self.create_publisher(String, \'voice_command\', 10)\n        self.timer = self.create_timer(5.0, self.send_test_commands)\n        self.command_count = 0\n\n    def send_test_commands(self):\n        """Send test commands to VLA system"""\n        test_commands = [\n            "Navigate to the kitchen",\n            "Find the red cup",\n            "Bring me a glass of water"\n        ]\n\n        if self.command_count < len(test_commands):\n            cmd = String()\n            cmd.data = test_commands[self.command_count]\n            self.publisher.publish(cmd)\n            self.get_logger().info(f"Sent test command: {cmd.data}")\n            self.command_count += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    test_client = VLATestClient()\n\n    try:\n        rclpy.spin(test_client)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        test_client.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-and-considerations",children:"Best Practices and Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-prompt-optimization",children:"1. Prompt Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use few-shot examples to guide LLM behavior"}),"\n",(0,i.jsx)(n.li,{children:"Include explicit safety constraints in prompts"}),"\n",(0,i.jsx)(n.li,{children:"Provide clear output formatting requirements"}),"\n",(0,i.jsx)(n.li,{children:"Test prompts with edge cases and ambiguous commands"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-safety-first-approach",children:"2. Safety First Approach"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Always validate LLM outputs before execution"}),"\n",(0,i.jsx)(n.li,{children:"Implement multiple safety layers (LLM analysis + internal rules)"}),"\n",(0,i.jsx)(n.li,{children:"Maintain emergency stop capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Log all safety decisions for audit purposes"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-performance-optimization",children:"3. Performance Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Cache LLM responses for repeated commands"}),"\n",(0,i.jsx)(n.li,{children:"Use local models for low-latency responses when possible"}),"\n",(0,i.jsx)(n.li,{children:"Implement proper error handling and retry logic"}),"\n",(0,i.jsx)(n.li,{children:"Monitor token usage and costs"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-context-management",children:"4. Context Management"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Maintain conversation history for context awareness"}),"\n",(0,i.jsx)(n.li,{children:"Regularly clean up old context to prevent memory bloat"}),"\n",(0,i.jsx)(n.li,{children:"Synchronize context between different system components"}),"\n",(0,i.jsx)(n.li,{children:"Handle context conflicts gracefully"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Rate Limiting"}),": Implement proper rate limiting and retry logic"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Length Limits"}),": Manage context size to stay within token limits"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety Validation Failures"}),": Fine-tune safety rules based on environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition Errors"}),": Improve prompt engineering for better parsing"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"debugging-tips",children:"Debugging Tips"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Enable detailed logging for LLM interactions"}),"\n",(0,i.jsx)(n.li,{children:"Monitor token usage and response times"}),"\n",(0,i.jsx)(n.li,{children:"Validate JSON parsing of LLM responses"}),"\n",(0,i.jsx)(n.li,{children:"Test with various command complexities"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This LLM integration system provides the cognitive foundation for converting natural language commands into executable robotic actions, forming a crucial bridge between human communication and robot execution in Vision-Language-Action systems."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);