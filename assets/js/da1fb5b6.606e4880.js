"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1468],{8386:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>m,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-ai-robot-brain/ai-integration/isaac-ros-perception","title":"Isaac ROS: Hardware-Accelerated Perception","description":"Overview","source":"@site/docs/module-3-ai-robot-brain/ai-integration/isaac-ros-perception.md","sourceDirName":"module-3-ai-robot-brain/ai-integration","slug":"/module-3-ai-robot-brain/ai-integration/isaac-ros-perception","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/ai-integration/isaac-ros-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Jiyakhan321/hackathon_textbook_ai_robotics/tree/main/docs/module-3-ai-robot-brain/ai-integration/isaac-ros-perception.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Setup for Humanoid Robots","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/isaac-sim/isaac-sim-setup"},"next":{"title":"Nav2: Bipedal Path Planning for Humanoid Robots","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/navigation/nav2-bipedal-navigation"}}');var i=a(4848),r=a(8453);const s={sidebar_position:4},o="Isaac ROS: Hardware-Accelerated Perception",m={},c=[{value:"Overview",id:"overview",level:2},{value:"Isaac ROS Ecosystem Overview",id:"isaac-ros-ecosystem-overview",level:2},{value:"1. Key Isaac ROS Packages",id:"1-key-isaac-ros-packages",level:3},{value:"2. Installation and Setup",id:"2-installation-and-setup",level:3},{value:"3. Verification of Installation",id:"3-verification-of-installation",level:3},{value:"Isaac ROS Perception Pipeline for Humanoids",id:"isaac-ros-perception-pipeline-for-humanoids",level:2},{value:"1. Basic Perception Node Implementation",id:"1-basic-perception-node-implementation",level:3},{value:"2. Isaac ROS NITROS Integration",id:"2-isaac-ros-nitros-integration",level:3},{value:"Isaac ROS VSLAM Implementation",id:"isaac-ros-vslam-implementation",level:2},{value:"1. Visual-Inertial Odometry Setup",id:"1-visual-inertial-odometry-setup",level:3},{value:"2. Isaac ROS Stereo DNN Integration",id:"2-isaac-ros-stereo-dnn-integration",level:3},{value:"Isaac ROS Apriltag Detection",id:"isaac-ros-apriltag-detection",level:2},{value:"1. Apriltag Detection Setup",id:"1-apriltag-detection-setup",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. GPU Resource Management",id:"1-gpu-resource-management",level:3},{value:"Launch Files for Isaac ROS Perception",id:"launch-files-for-isaac-ros-perception",level:2},{value:"1. Complete Perception Pipeline Launch",id:"1-complete-perception-pipeline-launch",level:3},{value:"Next Steps",id:"next-steps",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"isaac-ros-hardware-accelerated-perception",children:"Isaac ROS: Hardware-Accelerated Perception"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS provides hardware-accelerated perception capabilities that are essential for real-time processing in humanoid robotics applications. This section covers the implementation of Isaac ROS packages, including visual-inertial odometry, object detection, and sensor processing, all optimized for NVIDIA GPU acceleration."}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS bridges the gap between high-performance GPU computing and ROS 2, enabling humanoid robots to process sensor data in real-time while maintaining the flexibility of the ROS ecosystem."}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-ecosystem-overview",children:"Isaac ROS Ecosystem Overview"}),"\n",(0,i.jsx)(n.h3,{id:"1-key-isaac-ros-packages",children:"1. Key Isaac ROS Packages"}),"\n",(0,i.jsx)(n.p,{children:"Isaac ROS includes several specialized packages optimized for robotics perception:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": High-performance AprilTag detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Stereo DNN"}),": Deep neural network inference for stereo cameras"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Visual Inertial Odometry (VIO)"}),": Real-time pose estimation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS NITROS"}),": Network Interface for Time-sensitive, Real-time, Operating System agnostic communication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Manipulators"}),": Advanced manipulation algorithms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS Bi-AMP"}),": Bipedal locomotion and navigation"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-installation-and-setup",children:"2. Installation and Setup"}),"\n",(0,i.jsx)(n.p,{children:"Setting up Isaac ROS for humanoid robotics applications:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Update package lists\nsudo apt update\n\n# Install Isaac ROS dependencies\nsudo apt install -y python3-pip python3-dev\n\n# Install Isaac ROS packages for Humble\nsudo apt install -y ros-humble-isaac-ros-perception\nsudo apt install -y ros-humble-isaac-ros-common\nsudo apt install -y ros-humble-isaac-ros-messages\n\n# Install GPU acceleration dependencies\nsudo apt install -y nvidia-jetpack nvidia-jetpack-dev\n\n# Install additional perception packages\nsudo apt install -y ros-humble-isaac-ros-apriltag\nsudo apt install -y ros-humble-isaac-ros-stereo-dnn\nsudo apt install -y ros-humble-isaac-ros-visual-inertial-odometry\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-verification-of-installation",children:"3. Verification of Installation"}),"\n",(0,i.jsx)(n.p,{children:"Verify that Isaac ROS packages are properly installed:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check installed Isaac ROS packages\ndpkg -l | grep isaac-ros\n\n# Verify GPU acceleration\nnvidia-smi\n\n# Test Isaac ROS functionality\nros2 run isaac_ros_apriltag apriltag_node\n"})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-perception-pipeline-for-humanoids",children:"Isaac ROS Perception Pipeline for Humanoids"}),"\n",(0,i.jsx)(n.h3,{id:"1-basic-perception-node-implementation",children:"1. Basic Perception Node Implementation"}),"\n",(0,i.jsx)(n.p,{children:"Creating a basic perception node that leverages Isaac ROS capabilities:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# humanoid_perception_pipeline.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, Imu\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass HumanoidPerceptionPipeline(Node):\n    """\n    Basic perception pipeline for humanoid robots using Isaac ROS\n    """\n    def __init__(self):\n        super().__init__(\'humanoid_perception_pipeline\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers for sensor data\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/head_camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/head_camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Create publishers for processed data\n        self.object_detection_pub = self.create_publisher(\n            Image,\n            \'/perception/object_detection\',\n            10\n        )\n\n        self.pose_pub = self.create_publisher(\n            PoseStamped,\n            \'/perception/pose\',\n            10\n        )\n\n        # Initialize perception parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.latest_image = None\n        self.latest_imu = None\n\n        # Performance tracking\n        self.frame_count = 0\n        self.start_time = self.get_clock().now()\n\n        self.get_logger().info(\'Humanoid Perception Pipeline initialized\')\n\n    def image_callback(self, msg):\n        """\n        Process incoming image data\n        """\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.latest_image = cv_image\n\n            # Process the image using Isaac ROS optimized methods\n            processed_image = self.process_image(cv_image)\n\n            # Publish processed image\n            processed_msg = self.cv_bridge.cv2_to_imgmsg(processed_image, encoding=\'bgr8\')\n            processed_msg.header = msg.header\n            self.object_detection_pub.publish(processed_msg)\n\n            # Update performance metrics\n            self.frame_count += 1\n            current_time = self.get_clock().now()\n            elapsed = (current_time - self.start_time).nanoseconds / 1e9\n            if elapsed > 0:\n                fps = self.frame_count / elapsed\n                if self.frame_count % 30 == 0:  # Log every 30 frames\n                    self.get_logger().info(f\'Processing at {fps:.2f} FPS\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def camera_info_callback(self, msg):\n        """\n        Store camera calibration information\n        """\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def imu_callback(self, msg):\n        """\n        Process IMU data for humanoid stability\n        """\n        self.latest_imu = msg\n\n    def process_image(self, image):\n        """\n        Process image using Isaac ROS optimized methods\n        """\n        # Apply camera calibration if available\n        if self.camera_matrix is not None and self.distortion_coeffs is not None:\n            image = cv2.undistort(\n                image,\n                self.camera_matrix,\n                self.distortion_coeffs,\n                None,\n                self.camera_matrix\n            )\n\n        # Placeholder for Isaac ROS processing\n        # In real implementation, this would use Isaac ROS DNN nodes\n        processed_image = self.apply_object_detection(image)\n\n        return processed_image\n\n    def apply_object_detection(self, image):\n        """\n        Apply object detection using Isaac ROS methods\n        """\n        # This is a placeholder - in real implementation,\n        # Isaac ROS DNN nodes would be used for hardware-accelerated detection\n        height, width = image.shape[:2]\n\n        # Draw placeholder detection results\n        # In Isaac ROS, this would be replaced with actual DNN inference\n        result_image = image.copy()\n        cv2.rectangle(result_image, (width//4, height//4), (3*width//4, 3*height//4), (0, 255, 0), 2)\n        cv2.putText(result_image, \'Object Detected\', (width//4, height//4 - 10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n        return result_image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_pipeline = HumanoidPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_pipeline)\n    except KeyboardInterrupt:\n        perception_pipeline.get_logger().info(\'Shutting down perception pipeline\')\n    finally:\n        perception_pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-isaac-ros-nitros-integration",children:"2. Isaac ROS NITROS Integration"}),"\n",(0,i.jsx)(n.p,{children:"Implementing NITROS (Network Interface for Time-sensitive, Real-time, Operating System agnostic) for optimized data transport:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# nitros_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, QoSDurabilityPolicy\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom isaac_ros_nitros_camera_info_type.srv import NitrosCameraInfo\nfrom isaac_ros_nitros_image_type.srv import NitrosImage\nimport numpy as np\n\nclass NitrosPerceptionNode(Node):\n    """\n    Perception node using Isaac ROS NITROS for optimized data transport\n    """\n    def __init__(self):\n        super().__init__(\'nitros_perception_node\')\n\n        # Create QoS profile optimized for perception\n        qos_profile = QoSProfile(\n            depth=1,\n            durability=QoSDurabilityPolicy.VOLATILE\n        )\n\n        # Create subscribers with NITROS optimization\n        self.image_sub = self.create_subscription(\n            Image,\n            \'image_raw\',\n            self.nitros_image_callback,\n            qos_profile\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'camera_info\',\n            self.nitros_camera_info_callback,\n            qos_profile\n        )\n\n        # Create publisher for processed data\n        self.processed_image_pub = self.create_publisher(\n            Image,\n            \'processed_image\',\n            qos_profile\n        )\n\n        # Initialize NITROS components\n        self.setup_nitros_transports()\n\n        self.get_logger().info(\'NITROS Perception Node initialized\')\n\n    def setup_nitros_transports(self):\n        """\n        Setup NITROS transport configurations for optimal performance\n        """\n        # Configure NITROS transport settings\n        # This would typically involve setting up transport adapters\n        # and optimizing for the specific hardware configuration\n        self.get_logger().info(\'NITROS transports configured\')\n\n    def nitros_image_callback(self, msg):\n        """\n        Process image data with NITROS optimization\n        """\n        # Process image using hardware acceleration\n        processed_image = self.accelerated_image_processing(msg)\n\n        # Publish processed image\n        self.processed_image_pub.publish(processed_image)\n\n    def nitros_camera_info_callback(self, msg):\n        """\n        Process camera info with NITROS optimization\n        """\n        # Store camera parameters for image processing\n        self.camera_params = {\n            \'k\': np.array(msg.k).reshape(3, 3),\n            \'d\': np.array(msg.d),\n            \'width\': msg.width,\n            \'height\': msg.height\n        }\n\n    def accelerated_image_processing(self, image_msg):\n        """\n        Perform hardware-accelerated image processing using Isaac ROS\n        """\n        # This method would interface with Isaac ROS hardware acceleration\n        # In a real implementation, this would use CUDA/DLA acceleration\n        # through Isaac ROS extension packages\n\n        # Placeholder implementation\n        return image_msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nitros_node = NitrosPerceptionNode()\n\n    try:\n        rclpy.spin(nitros_node)\n    except KeyboardInterrupt:\n        nitros_node.get_logger().info(\'Shutting down NITROS perception node\')\n    finally:\n        nitros_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-vslam-implementation",children:"Isaac ROS VSLAM Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"1-visual-inertial-odometry-setup",children:"1. Visual-Inertial Odometry Setup"}),"\n",(0,i.jsx)(n.p,{children:"Setting up hardware-accelerated VSLAM for humanoid robots:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vio_setup.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, CameraInfo\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped, TwistStamped\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacROSVisualInertialOdometry(Node):\n    """\n    Visual-Inertial Odometry implementation using Isaac ROS\n    """\n    def __init__(self):\n        super().__init__(\'isaac_ros_vio\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers for stereo camera and IMU\n        self.left_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_raw\',\n            self.left_image_callback,\n            5\n        )\n\n        self.right_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/right/image_raw\',\n            self.right_image_callback,\n            5\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.left_camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/stereo/left/camera_info\',\n            self.left_camera_info_callback,\n            10\n        )\n\n        self.right_camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/stereo/right/camera_info\',\n            self.right_camera_info_callback,\n            10\n        )\n\n        # Create publisher for odometry\n        self.odom_pub = self.create_publisher(\n            Odometry,\n            \'/visual_odometry/odom\',\n            10\n        )\n\n        # Create publisher for pose\n        self.pose_pub = self.create_publisher(\n            PoseStamped,\n            \'/visual_odometry/pose\',\n            10\n        )\n\n        # Initialize VIO parameters\n        self.left_image = None\n        self.right_image = None\n        self.imu_data = None\n        self.left_camera_info = None\n        self.right_camera_info = None\n\n        # Store previous pose for integration\n        self.previous_pose = np.eye(4)\n        self.current_pose = np.eye(4)\n\n        # TF broadcaster for robot pose\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Performance metrics\n        self.processed_frames = 0\n        self.last_process_time = self.get_clock().now()\n\n        self.get_logger().info(\'Isaac ROS Visual-Inertial Odometry initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left camera image"""\n        self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n\n    def right_image_callback(self, msg):\n        """Process right camera image"""\n        self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for VIO"""\n        self.imu_data = msg\n\n    def left_camera_info_callback(self, msg):\n        """Store left camera calibration"""\n        self.left_camera_info = msg\n\n    def right_camera_info_callback(self, msg):\n        """Store right camera calibration"""\n        self.right_camera_info = msg\n\n    def process_stereo_pair(self):\n        """\n        Process stereo images using Isaac ROS VIO\n        """\n        if (self.left_image is not None and\n            self.right_image is not None and\n            self.left_camera_info is not None and\n            self.right_camera_info is not None):\n\n            # In Isaac ROS, this would call the actual VIO algorithm\n            # which uses GPU acceleration for feature extraction and matching\n            pose_update = self.accelerated_vio_processing()\n\n            if pose_update is not None:\n                # Update current pose\n                self.current_pose = self.current_pose @ pose_update\n\n                # Publish odometry\n                self.publish_odometry()\n\n                # Update frame counter\n                self.processed_frames += 1\n\n                # Log performance\n                current_time = self.get_clock().now()\n                if (current_time - self.last_process_time).nanoseconds > 1e9:  # 1 second\n                    fps = self.processed_frames / ((current_time - self.last_process_time).nanoseconds / 1e9)\n                    self.get_logger().info(f\'VIO processing at {fps:.2f} FPS\')\n                    self.processed_frames = 0\n                    self.last_process_time = current_time\n\n    def accelerated_vio_processing(self):\n        """\n        Hardware-accelerated VIO processing using Isaac ROS\n        """\n        # This is a placeholder - in real implementation, this would use\n        # Isaac ROS Visual Inertial Odometry package with GPU acceleration\n        # The actual implementation would involve:\n        # 1. Feature extraction using GPU\n        # 2. Stereo matching on GPU\n        # 3. Pose estimation with IMU fusion\n\n        # Simulate pose update (in real implementation, this comes from Isaac ROS VIO)\n        dt = 0.033  # 30 FPS\n        # Simulate small movement\n        pose_update = np.eye(4)\n        pose_update[0, 3] = 0.01  # Move forward 1cm\n        pose_update[2, 3] = 0.001  # Move up 1mm\n\n        return pose_update\n\n    def publish_odometry(self):\n        """\n        Publish odometry data\n        """\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'odom\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set pose from current transformation matrix\n        pos = self.current_pose[:3, 3]\n        odom_msg.pose.pose.position.x = float(pos[0])\n        odom_msg.pose.pose.position.y = float(pos[1])\n        odom_msg.pose.pose.position.z = float(pos[2])\n\n        # Convert rotation matrix to quaternion\n        quat = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\n        odom_msg.pose.pose.orientation.x = quat[0]\n        odom_msg.pose.pose.orientation.y = quat[1]\n        odom_msg.pose.pose.orientation.z = quat[2]\n        odom_msg.pose.pose.orientation.w = quat[3]\n\n        # Publish odometry\n        self.odom_pub.publish(odom_msg)\n\n        # Publish pose\n        pose_msg = PoseStamped()\n        pose_msg.header = odom_msg.header\n        pose_msg.pose = odom_msg.pose.pose\n        self.pose_pub.publish(pose_msg)\n\n        # Broadcast transform\n        self.broadcast_transform(odom_msg)\n\n    def rotation_matrix_to_quaternion(self, rotation_matrix):\n        """\n        Convert rotation matrix to quaternion\n        """\n        trace = np.trace(rotation_matrix)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            w = 0.25 * s\n            x = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n            y = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n            z = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n        else:\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\n                s = np.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\n                w = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n                x = 0.25 * s\n                y = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                z = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\n                s = np.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\n                w = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n                x = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                y = 0.25 * s\n                z = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\n                w = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n                x = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n                y = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n                z = 0.25 * s\n\n        return [x, y, z, w]\n\n    def broadcast_transform(self, odom_msg):\n        """\n        Broadcast transform for visualization\n        """\n        from geometry_msgs.msg import TransformStamped\n\n        t = TransformStamped()\n        t.header.stamp = odom_msg.header.stamp\n        t.header.frame_id = \'odom\'\n        t.child_frame_id = \'base_link\'\n\n        t.transform.translation.x = odom_msg.pose.pose.position.x\n        t.transform.translation.y = odom_msg.pose.pose.position.y\n        t.transform.translation.z = odom_msg.pose.pose.position.z\n        t.transform.rotation = odom_msg.pose.pose.orientation\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vio_node = IsaacROSVisualInertialOdometry()\n\n    # Create timer to process stereo pairs at regular intervals\n    vio_node.create_timer(0.033, vio_node.process_stereo_pair)  # ~30 FPS\n\n    try:\n        rclpy.spin(vio_node)\n    except KeyboardInterrupt:\n        vio_node.get_logger().info(\'Shutting down VIO node\')\n    finally:\n        vio_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-isaac-ros-stereo-dnn-integration",children:"2. Isaac ROS Stereo DNN Integration"}),"\n",(0,i.jsx)(n.p,{children:"Implementing deep neural network processing for stereo vision:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# stereo_dnn_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom stereo_msgs.msg import DisparityImage\nfrom geometry_msgs.msg import PointStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass IsaacROSDNNProcessor(Node):\n    """\n    DNN processor using Isaac ROS Stereo DNN package\n    """\n    def __init__(self):\n        super().__init__(\'isaac_ros_dnn_processor\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers for stereo images\n        self.left_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_rect\',\n            self.left_image_callback,\n            5\n        )\n\n        self.right_image_sub = self.create_subscription(\n            Image,\n            \'/stereo/right/image_rect\',\n            self.right_image_callback,\n            5\n        )\n\n        # Create publisher for disparity map\n        self.disparity_pub = self.create_publisher(\n            DisparityImage,\n            \'/stereo/disparity\',\n            5\n        )\n\n        # Create publisher for object detections\n        self.detection_pub = self.create_publisher(\n            Image,\n            \'/dnn_detection\',\n            5\n        )\n\n        # Initialize stereo processing parameters\n        self.left_image = None\n        self.right_image = None\n\n        # Performance tracking\n        self.frame_count = 0\n        self.start_time = self.get_clock().now()\n\n        self.get_logger().info(\'Isaac ROS DNN Processor initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left image"""\n        try:\n            self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n            if self.right_image is not None:\n                self.process_stereo_pair()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing left image: {e}\')\n\n    def right_image_callback(self, msg):\n        """Process right image"""\n        try:\n            self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n            if self.left_image is not None:\n                self.process_stereo_pair()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing right image: {e}\')\n\n    def process_stereo_pair(self):\n        """\n        Process stereo pair using Isaac ROS DNN acceleration\n        """\n        if self.left_image is None or self.right_image is None:\n            return\n\n        # In Isaac ROS, this would use hardware-accelerated stereo matching\n        # For this example, we\'ll simulate the process\n        disparity_map = self.accelerated_stereo_matching()\n\n        # Publish disparity map\n        if disparity_map is not None:\n            self.publish_disparity(disparity_map)\n\n        # Perform object detection using DNN\n        detection_result = self.accelerated_object_detection(self.left_image)\n\n        # Publish detection result\n        if detection_result is not None:\n            detection_msg = self.cv_bridge.cv2_to_imgmsg(detection_result, encoding=\'bgr8\')\n            detection_msg.header.stamp = self.get_clock().now().to_msg()\n            detection_msg.header.frame_id = \'camera_link\'\n            self.detection_pub.publish(detection_msg)\n\n        # Update performance metrics\n        self.frame_count += 1\n        current_time = self.get_clock().now()\n        elapsed = (current_time - self.start_time).nanoseconds / 1e9\n        if elapsed > 0 and self.frame_count % 30 == 0:\n            fps = self.frame_count / elapsed\n            self.get_logger().info(f\'DNN processing at {fps:.2f} FPS\')\n\n    def accelerated_stereo_matching(self):\n        """\n        Hardware-accelerated stereo matching using Isaac ROS\n        """\n        # This is a placeholder - in real implementation, this would use\n        # Isaac ROS Stereo DNN package with GPU acceleration\n        # The actual implementation would involve:\n        # 1. GPU-accelerated stereo matching\n        # 2. Subpixel refinement\n        # 3. Disparity filtering\n\n        # For simulation, create a simple disparity map\n        if self.left_image is not None:\n            # Simulate disparity based on features in the image\n            gray = self.left_image.astype(np.float32)\n            # Add some simulated depth variation\n            disparity = np.random.rand(*gray.shape) * 64  # Max disparity of 64\n            return disparity.astype(np.float32)\n\n        return None\n\n    def accelerated_object_detection(self, image):\n        """\n        Hardware-accelerated object detection using Isaac ROS DNN\n        """\n        # This is a placeholder - in real implementation, this would use\n        # Isaac ROS DNN packages with TensorRT acceleration\n        # The actual implementation would involve:\n        # 1. TensorRT-optimized neural networks\n        # 2. Hardware-accelerated inference\n        # 3. Post-processing of detections\n\n        # For simulation, draw detection boxes\n        result_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n\n        # Simulate some detections\n        h, w = image.shape\n        for i in range(3):  # Simulate 3 detections\n            x = np.random.randint(0, w - 100)\n            y = np.random.randint(0, h - 100)\n            width = np.random.randint(50, 100)\n            height = np.random.randint(50, 100)\n\n            cv2.rectangle(result_image, (x, y), (x + width, y + height), (0, 255, 0), 2)\n            cv2.putText(result_image, f\'Object {i+1}\', (x, y - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n        return result_image\n\n    def publish_disparity(self, disparity_map):\n        """\n        Publish disparity map\n        """\n        # Convert disparity map to DisparityImage message\n        disp_msg = DisparityImage()\n        disp_msg.header.stamp = self.get_clock().now().to_msg()\n        disp_msg.header.frame_id = \'camera_link\'\n\n        # Set disparity image parameters\n        disp_msg.image = self.cv_bridge.cv2_to_imgmsg(disparity_map, encoding=\'32FC1\')\n        disp_msg.f = 320.0  # Focal length (example value)\n        disp_msg.T = 0.12  # Baseline (example value)\n        disp_msg.min_disparity = 0.0\n        disp_msg.max_disparity = 64.0\n        disp_msg.delta_d = 0.125\n\n        self.disparity_pub.publish(disp_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    dnn_node = IsaacROSDNNProcessor()\n\n    try:\n        rclpy.spin(dnn_node)\n    except KeyboardInterrupt:\n        dnn_node.get_logger().info(\'Shutting down DNN processor\')\n    finally:\n        dnn_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-ros-apriltag-detection",children:"Isaac ROS Apriltag Detection"}),"\n",(0,i.jsx)(n.h3,{id:"1-apriltag-detection-setup",children:"1. Apriltag Detection Setup"}),"\n",(0,i.jsx)(n.p,{children:"Implementing hardware-accelerated Apriltag detection for humanoid navigation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# apriltag_detection.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, PointStamped\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacROSApriltagDetector(Node):\n    \"\"\"\n    Apriltag detector using Isaac ROS hardware acceleration\n    \"\"\"\n    def __init__(self):\n        super().__init__('isaac_ros_apriltag_detector')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/head_camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/head_camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publishers\n        self.tag_pose_pub = self.create_publisher(\n            PoseStamped,\n            '/apriltag/pose',\n            10\n        )\n\n        self.tag_marker_pub = self.create_publisher(\n            MarkerArray,\n            '/apriltag/markers',\n            10\n        )\n\n        # Initialize Apriltag parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.tag_size = 0.16  # 16cm tag size (adjust as needed)\n\n        # Tag dictionary (36h11 is commonly used)\n        self.aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_APRILTAG_36h11)\n        self.parameters = cv2.aruco.DetectorParameters_create()\n\n        self.get_logger().info('Isaac ROS Apriltag Detector initialized')\n\n    def image_callback(self, msg):\n        \"\"\"\n        Process image for Apriltag detection\n        \"\"\"\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Detect Apriltags\n            tag_poses = self.detect_apriltags(cv_image)\n\n            # Publish tag poses and markers\n            if tag_poses:\n                for pose in tag_poses:\n                    self.publish_tag_pose(pose, msg.header)\n                    self.publish_tag_marker(pose)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image for Apriltag detection: {e}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"\n        Store camera calibration information\n        \"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def detect_apriltags(self, image):\n        \"\"\"\n        Detect Apriltags in the image using Isaac ROS acceleration\n        \"\"\"\n        if self.camera_matrix is None:\n            return []\n\n        # In Isaac ROS, this would use hardware-accelerated Apriltag detection\n        # For this example, we'll use OpenCV as a placeholder\n        # The actual Isaac ROS implementation would be much faster\n\n        # Detect markers\n        corners, ids, rejected_img_points = cv2.aruco.detectMarkers(\n            image, self.aruco_dict, parameters=self.parameters,\n            cameraMatrix=self.camera_matrix, distCoeff=self.distortion_coeffs\n        )\n\n        tag_poses = []\n\n        if ids is not None:\n            # Estimate pose of each marker\n            rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(\n                corners, self.tag_size, self.camera_matrix, self.distortion_coeffs\n            )\n\n            for i in range(len(ids)):\n                # Convert rotation vector to rotation matrix\n                rmat, _ = cv2.Rodrigues(rvecs[i])\n\n                # Create transformation matrix\n                transform = np.eye(4)\n                transform[:3, :3] = rmat\n                transform[:3, 3] = tvecs[i].flatten()\n\n                tag_poses.append({\n                    'id': int(ids[i][0]),\n                    'transform': transform,\n                    'corner_points': corners[i][0]\n                })\n\n        return tag_poses\n\n    def publish_tag_pose(self, tag_pose, header):\n        \"\"\"\n        Publish the pose of a detected tag\n        \"\"\"\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = 'camera_link'\n\n        # Extract position\n        pos = tag_pose['transform'][:3, 3]\n        pose_msg.pose.position.x = float(pos[0])\n        pose_msg.pose.position.y = float(pos[1])\n        pose_msg.pose.position.z = float(pos[2])\n\n        # Convert rotation matrix to quaternion\n        quat = self.rotation_matrix_to_quaternion(tag_pose['transform'][:3, :3])\n        pose_msg.pose.orientation.x = quat[0]\n        pose_msg.pose.orientation.y = quat[1]\n        pose_msg.pose.orientation.z = quat[2]\n        pose_msg.pose.orientation.w = quat[3]\n\n        self.tag_pose_pub.publish(pose_msg)\n\n    def publish_tag_marker(self, tag_pose):\n        \"\"\"\n        Publish visualization marker for the tag\n        \"\"\"\n        marker = Marker()\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.header.frame_id = 'camera_link'\n        marker.ns = 'apriltags'\n        marker.id = tag_pose['id']\n        marker.type = Marker.CUBE\n        marker.action = Marker.ADD\n\n        # Position from transform\n        pos = tag_pose['transform'][:3, 3]\n        marker.pose.position.x = float(pos[0])\n        marker.pose.position.y = float(pos[1])\n        marker.pose.position.z = float(pos[2])\n\n        # Orientation from transform\n        quat = self.rotation_matrix_to_quaternion(tag_pose['transform'][:3, :3])\n        marker.pose.orientation.x = quat[0]\n        marker.pose.orientation.y = quat[1]\n        marker.pose.orientation.z = quat[2]\n        marker.pose.orientation.w = quat[3]\n\n        # Scale (tag size)\n        marker.scale.x = self.tag_size\n        marker.scale.y = self.tag_size\n        marker.scale.z = 0.01  # Thin cube\n\n        # Color (based on tag ID)\n        marker.color.r = float(hash(f\"tag_{tag_pose['id']}\") % 256) / 255.0\n        marker.color.g = float(hash(f\"tag_{tag_pose['id']}_g\") % 256) / 255.0\n        marker.color.b = float(hash(f\"tag_{tag_pose['id']}_b\") % 256) / 255.0\n        marker.color.a = 0.8\n\n        # Create marker array and publish\n        marker_array = MarkerArray()\n        marker_array.markers.append(marker)\n        self.tag_marker_pub.publish(marker_array)\n\n    def rotation_matrix_to_quaternion(self, rotation_matrix):\n        \"\"\"\n        Convert rotation matrix to quaternion\n        \"\"\"\n        trace = np.trace(rotation_matrix)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            w = 0.25 * s\n            x = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n            y = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n            z = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n        else:\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\n                s = np.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\n                w = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n                x = 0.25 * s\n                y = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                z = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\n                s = np.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\n                w = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n                x = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                y = 0.25 * s\n                z = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\n                w = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n                x = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n                y = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n                z = 0.25 * s\n\n        return [x, y, z, w]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    apriltag_node = IsaacROSApriltagDetector()\n\n    try:\n        rclpy.spin(apriltag_node)\n    except KeyboardInterrupt:\n        apriltag_node.get_logger().info('Shutting down Apriltag detector')\n    finally:\n        apriltag_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-gpu-resource-management",children:"1. GPU Resource Management"}),"\n",(0,i.jsx)(n.p,{children:"Optimizing GPU usage for multiple perception tasks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# gpu_resource_manager.py\nimport rclpy\nfrom rclpy.node import Node\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\nimport numpy as np\n\nclass GPUResourceManager:\n    \"\"\"\n    Manager for GPU resources in Isaac ROS perception pipeline\n    \"\"\"\n    def __init__(self):\n        # Initialize CUDA context\n        self.device = cuda.Device(0)  # Use first GPU\n        self.context = self.device.make_context()\n\n        # Track GPU memory usage\n        self.max_memory = self.device.total_memory()\n        self.current_memory_usage = 0\n\n        # GPU memory pools for different tasks\n        self.memory_pools = {\n            'detection': {'size': 1024*1024*512, 'used': 0},  # 512MB\n            'tracking': {'size': 1024*1024*256, 'used': 0},   # 256MB\n            'mapping': {'size': 1024*1024*1024, 'used': 0},   # 1GB\n        }\n\n    def allocate_memory(self, task_type, size_bytes):\n        \"\"\"\n        Allocate GPU memory for a specific task\n        \"\"\"\n        if task_type in self.memory_pools:\n            if self.memory_pools[task_type]['used'] + size_bytes <= self.memory_pools[task_type]['size']:\n                self.memory_pools[task_type]['used'] += size_bytes\n                self.current_memory_usage += size_bytes\n                return True\n        return False\n\n    def release_memory(self, task_type, size_bytes):\n        \"\"\"\n        Release GPU memory for a specific task\n        \"\"\"\n        if task_type in self.memory_pools:\n            self.memory_pools[task_type]['used'] = max(0, self.memory_pools[task_type]['used'] - size_bytes)\n            self.current_memory_usage = max(0, self.current_memory_usage - size_bytes)\n\n    def get_gpu_status(self):\n        \"\"\"\n        Get current GPU status\n        \"\"\"\n        free_memory = self.max_memory - self.current_memory_usage\n        usage_percent = (self.current_memory_usage / self.max_memory) * 100\n\n        return {\n            'total_memory': self.max_memory,\n            'used_memory': self.current_memory_usage,\n            'free_memory': free_memory,\n            'usage_percent': usage_percent,\n            'memory_pools': self.memory_pools\n        }\n\nclass IsaacROSPerceptionOptimizer(Node):\n    \"\"\"\n    Node that optimizes Isaac ROS perception using GPU resource management\n    \"\"\"\n    def __init__(self):\n        super().__init__('isaac_ros_perception_optimizer')\n\n        # Initialize GPU resource manager\n        self.gpu_manager = GPUResourceManager()\n\n        # Create timer for periodic optimization\n        self.optimization_timer = self.create_timer(1.0, self.optimize_resources)\n\n        self.get_logger().info('Isaac ROS Perception Optimizer initialized')\n\n    def optimize_resources(self):\n        \"\"\"\n        Periodically optimize GPU resource allocation\n        \"\"\"\n        status = self.gpu_manager.get_gpu_status()\n\n        self.get_logger().info(\n            f'GPU Status - Used: {status[\"used_memory\"]/1024/1024:.1f}MB, '\n            f'Free: {status[\"free_memory\"]/1024/1024:.1f}MB, '\n            f'Usage: {status[\"usage_percent\"]:.1f}%'\n        )\n\n        # Adjust processing rates based on GPU usage\n        if status['usage_percent'] > 80:\n            self.get_logger().warn('High GPU usage detected, consider reducing processing rate')\n        elif status['usage_percent'] < 30:\n            self.get_logger().info('GPU resources available, could increase processing rate')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    optimizer = IsaacROSPerceptionOptimizer()\n\n    try:\n        rclpy.spin(optimizer)\n    except KeyboardInterrupt:\n        optimizer.get_logger().info('Shutting down perception optimizer')\n    finally:\n        optimizer.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"launch-files-for-isaac-ros-perception",children:"Launch Files for Isaac ROS Perception"}),"\n",(0,i.jsx)(n.h3,{id:"1-complete-perception-pipeline-launch",children:"1. Complete Perception Pipeline Launch"}),"\n",(0,i.jsx)(n.p,{children:"Creating launch files to bring up the complete Isaac ROS perception system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/humanoid_perception_pipeline.launch.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    camera_namespace = LaunchConfiguration('camera_namespace')\n\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation time'\n    )\n\n    declare_camera_namespace = DeclareLaunchArgument(\n        'camera_namespace',\n        default_value='/head_camera',\n        description='Namespace for camera topics'\n    )\n\n    # Isaac ROS VIO node\n    vio_node = Node(\n        package='isaac_ros_visual_inertial_odometry',\n        executable='visual_inertial_odometry_node',\n        name='visual_inertial_odometry',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'enable_debug_mode': False,\n            'publish_tf': True,\n            'world_frame': 'odom',\n            'base_frame': 'base_link'\n        }],\n        remappings=[\n            ('left/image_rect', [camera_namespace, '/left/image_rect']),\n            ('left/camera_info', [camera_namespace, '/left/camera_info']),\n            ('right/image_rect', [camera_namespace, '/right/image_rect']),\n            ('right/camera_info', [camera_namespace, '/right/camera_info']),\n            ('imu', '/imu/data'),\n            ('visual_odometry', '/visual_odometry'),\n        ]\n    )\n\n    # Isaac ROS Apriltag node\n    apriltag_node = Node(\n        package='isaac_ros_apriltag',\n        executable='apriltag_node',\n        name='apriltag',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'family': 'tag36h11',\n            'max_tags': 64,\n            'tag36h11_size': 0.16  # 16cm tags\n        }],\n        remappings=[\n            ('image', [camera_namespace, '/image_raw']),\n            ('camera_info', [camera_namespace, '/camera_info']),\n            ('detections', '/apriltag/detections'),\n        ]\n    )\n\n    # Isaac ROS Stereo DNN node (for object detection)\n    stereo_dnn_node = Node(\n        package='isaac_ros_stereo_dnn',\n        executable='stereo_dnn_node',\n        name='stereo_dnn',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'network_type': 'coco_tensorrt',\n            'input_tensor_names': ['input'],\n            'input_binding_names': ['input'],\n            'output_tensor_names': ['output'],\n            'output_binding_names': ['output'],\n            'threshold': 0.5\n        }],\n        remappings=[\n            ('left_image', [camera_namespace, '/left/image_rect']),\n            ('right_image', [camera_namespace, '/right/image_rect']),\n            ('detections', '/dnn_detections'),\n        ]\n    )\n\n    # Humanoid-specific perception processing node\n    humanoid_perception = Node(\n        package='humanoid_perception',\n        executable='humanoid_perception_pipeline',\n        name='humanoid_perception',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        remappings=[\n            ('/head_camera/image_raw', [camera_namespace, '/image_raw']),\n            ('/head_camera/camera_info', [camera_namespace, '/camera_info']),\n            ('/imu/data', '/imu/data'),\n        ]\n    )\n\n    return LaunchDescription([\n        declare_use_sim_time,\n        declare_camera_namespace,\n        vio_node,\n        apriltag_node,\n        stereo_dnn_node,\n        humanoid_perception,\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"With Isaac ROS perception properly implemented, you're ready to move on to implementing Visual-Inertial SLAM (VSLAM) for humanoid robots. The next section will cover creating a complete VSLAM system that leverages the Isaac ROS perception pipeline you've built for real-time localization and mapping in humanoid robotics applications."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var t=a(6540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);